{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine learning for computational materials science and chemistry with MALA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What is MALA?\n",
    "\n",
    "A short summary about MALA and how it works is given in `mala_background.pdf`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up MALA\n",
    "\n",
    "For this tutorial, a Google Collab enviroment will be provided that includes all necessary packages. Generally, MALA is an open source framework that can be obtained [here](https://github.com/mala-project/mala). Detailled (installation) instructions can be found [here](https://mala-project.github.io/mala/).\n",
    "\n",
    "A few examples at the end of the notebook tackle advanced applications. The necessary backends are, for the ease of installation, not bundled with the Google Collab environment. Interested readers may install them themselves on their machines, for in presence workshops, they will be demonstrated by the host, as are sosme aspects of data generation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the modules\n",
    "\n",
    "These modules will be necessary for the tutorials discussed here."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# MALA itself.\n",
    "\n",
    "import mala\n",
    "\n",
    "# We would like to visualize simple plots.\n",
    "# The font size can sometimes be a bit small for Jupyter Notebooks.\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "font = {'size'   : 22}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "# For the data paths."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data I: Performing simulations (presentation only)\n",
    "\n",
    "Data is the backbone of each ML application. For MALA, this includes target data (some electronic structure quantity to be learned, usually the local density of states, LDOS) and some descriptor data (a vectorial field that encodes the atomic density at each point in space, usually via so called bispectrum components).\n",
    "\n",
    "MALA data generation can be performed with the Quantum ESPRESSO package. Some changes to this open source package were necessary to enable the correct sampling of the LDOS. The current development branch of Quantum ESPRESSO includes those - beginning with Quantum ESPRESSO version 7.2 (to be released in ~June 2023) users can simply download the latest QE version and perform data generation.\n",
    "\n",
    "Data generation is two-fold: First, one creates a set of atomic position via a regular DFT-MD simulation at the conditions of interest. This can be done with any suitable code, such as VASP, QE, etc. Secondly, one performs DFT simulations to access the LDOS.\n",
    "\n",
    "The test system for our investigation here will be a simply beryllium system at room temperature consisting of 2 beryllium atoms. Atomic configurations have been sampled beforehand. We will start with the DFT simulation. Note: For actual data generation, the simulation output needs to be saved (usually as \".out\" file) and is of course done on HPC infrastructure."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "MALA data generation thus comes down to a simply DFT + Postprocessing calculation. There is one drawback though: The LDOS has to be sampled with a very high fidelity in the k-space (phase space, i.e., how the Fourier components of the basis set are sampled). The fidelity of the MALA calculation has to be higher then for standard DFT calculations, which has to be kept in mind. A good way to visualize the problem is through the density of states (LDOS integrated on real space grid) which shows unphysical oscillations for low fidelity calculations. These oscillations vanish as one moves to higher fidelity calculations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![kpoint_comparison](./figures/DOSwdifferentdeltasandks.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After we have performed the actual simulation, we are halfway done. We now have simulation outputs, both the DFT output as well as the LDOS. From this we need to do two things:\n",
    "\n",
    "1. We need to convert the LDOS into a format we actually want to work with. Cube files are unnecessarily huge, complicated both disk usage as well as speed.\n",
    "2. We need to calculate the atomic density descriptors from the atomic positions.\n",
    "\n",
    "So it's finally time to fire up MALA."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The MALA interface (hands-on)\n",
    "\n",
    "Within extended ML frameworks, a big problem is reproducibility. Models often depend on a lot of so called hyperparameters, that characterize model behavior. This may include, but in no way be limited to, neural network layer sizes and number, training procedures, description of data specifics (e.g.: how is the local density of states sampled?), etc.\n",
    "There is a number of ways to efficiently handle this problem. A lot of frameworks rely on command line arguments to deal with this, i.e., the user provides a, potentially extensive, list of command line arguments upon runtime. Another good way to handle all this is the usage of input files. This is consistent with the way standard computational science simulation codes work.\n",
    "An obvious downside to this is that one has to have a framework at hand to prepare these input files.\n",
    "\n",
    "MALA follows a route sort of in between the two approaches. The central quantity is the `Parameters()` object. It holds ALL (hyper)parameters one could use in the course of a MALA run. It is structured by the subtasks of MALA."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<mala.common.parameters.ParametersHyperparameterOptimization at 0x7fab7d654280>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = mala.Parameters()\n",
    "\n",
    "# All parameters related to how data is handled in general.\n",
    "parameters.data\n",
    "\n",
    "# \"Targets\" always refers to the quantity being learned.\n",
    "parameters.targets\n",
    "\n",
    "# \"Descriptors\" always refers to the quantity from which we learn.\n",
    "parameters.descriptors\n",
    "\n",
    "# \"Data generation\" refers to useful routines for creating training data.\n",
    "# These routines are mostly experimental at the moment and will not be discussed here in detail\n",
    "parameters.datageneration\n",
    "\n",
    "# \"Network\" refers to everything related to neural network creation and training.\n",
    "# In the future, support for more models is planned, and this collection of parameters\n",
    "# will be updated to reflect this.\n",
    "parameters.network\n",
    "\n",
    "# \"Hyperparameters\" means hyperparameter optimization. This is the process of finding the optimal\n",
    "# hyperparameters for MALA model training, and we will come back to this process later.\n",
    "parameters.hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Individual parameter objects can be printed to see what's hidden inside. This also works on the main object as well."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snapshot_directories_list: []\n",
      "data_splitting_type: by_snapshot\n",
      "input_rescaling_type: None\n",
      "output_rescaling_type: None\n",
      "use_lazy_loading: False\n",
      "use_clustering : False\n",
      "number_of_clusters: 40\n",
      "train_ratio    : 0.1\n",
      "sample_ratio   : 0.5\n",
      "use_fast_tensor_data_set: False\n",
      "shuffling_seed : None\n"
     ]
    }
   ],
   "source": [
    "parameters.data.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, there are some high-level parameters one needs when performing ML at scale."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "''"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Whether or not to use a GPU for model training and inference.\n",
    "parameters.use_gpu\n",
    "\n",
    "# Whether or not to use MPI parallel CPU inference (no training supported).\n",
    "# This option is either for pre-processing or production runs of trained models.\n",
    "# More on that later.\n",
    "parameters.use_mpi\n",
    "\n",
    "# Manual seeds can be used to fix the Pseudo RNG to re-create models with the exact\n",
    "# same model weights.\n",
    "parameters.manual_seed\n",
    "\n",
    "# A comment may be useful to distinguish between sets of parameters.\n",
    "parameters.comment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All of this does not explain how MALA handles reproducibility. Write hundreds of lines of parameter statement in each python script is not exactly maintainable.\n",
    "Therefore MALA provides a .json interface."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "parameters.save(\"mala_parameters_01.json\")\n",
    "new_parameters = mala.Parameters.load_from_file(\"mala_parameters_01.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Have a look at the .json file that was just created. You will see that it is structured in the same ways as the python object, allowing fast access. You will see some parameters that are not dicussed here since they exceed the scope of this tutorial. For a first excercise, try to modify parameters both in python and in json and see whether loading will recover those changes. The comment and manual seed are good first examples for this."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Set a comment\n",
    "parameters.comment = \"My first parameters.\"\n",
    "\n",
    "# Save.\n",
    "parameters.save(\"mala_parameters_01.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---     All parameter MALA needs to perform its various tasks. ---\n",
      "comment        : My first parameters.\n",
      "manual_seed    : 1234\n",
      "use_gpu        : False\n",
      "device         : cpu\n",
      "use_horovod    : False\n",
      "use_mpi        : False\n",
      "verbosity      : 1\n",
      "openpmd_configuration: {}\n",
      "openpmd_granularity: 1\n",
      "---     Parameters necessary for constructing a neural network. ---\n",
      "\tnn_type        : feed-forward\n",
      "\tlayer_sizes    : [10, 10, 10]\n",
      "\tlayer_activations: ['Sigmoid']\n",
      "\tloss_function_type: mse\n",
      "\tnum_hidden_layers: 1\n",
      "\tno_hidden_state: False\n",
      "\tbidirection    : False\n",
      "\tdropout        : 0.1\n",
      "\tnum_heads      : 10\n",
      "---     Parameters necessary for calculating/parsing input descriptors. ---\n",
      "\tdescriptor_type: Bispectrum\n",
      "\tlammps_compute_file: \n",
      "\tdescriptors_contain_xyz: True\n",
      "\tuse_z_splitting: True\n",
      "\tnumber_y_planes: 0\n",
      "\tbispectrum_twojmax: 10\n",
      "\trcutfac        : 4.67637\n",
      "\tatomic_density_cutoff: 4.67637\n",
      "\tsnap_switchflag: 1\n",
      "\tuse_atomic_density_energy_formula: False\n",
      "\tatomic_density_sigma: None\n",
      "---     Parameters necessary for calculating/parsing output quantites. ---\n",
      "\ttarget_type    : LDOS\n",
      "\tldos_gridsize  : 0\n",
      "\tldos_gridspacing_ev: 0\n",
      "\tldos_gridoffset_ev: 0\n",
      "\trestrict_targets: zero_out_negative\n",
      "\tpseudopotential_path: None\n",
      "\trdf_parameters : {'number_of_bins': 500, 'rMax': 'mic'}\n",
      "\ttpcf_parameters: {'number_of_bins': 20, 'rMax': 'mic'}\n",
      "\tssf_parameters : {'number_of_bins': 100, 'kMax': 12.0}\n",
      "---     Parameters necessary for loading and preprocessing data. ---\n",
      "\tsnapshot_directories_list: []\n",
      "\tdata_splitting_type: by_snapshot\n",
      "\tinput_rescaling_type: None\n",
      "\toutput_rescaling_type: None\n",
      "\tuse_lazy_loading: False\n",
      "\tuse_clustering : False\n",
      "\tnumber_of_clusters: 40\n",
      "\ttrain_ratio    : 0.1\n",
      "\tsample_ratio   : 0.5\n",
      "\tuse_fast_tensor_data_set: False\n",
      "\tshuffling_seed : None\n",
      "---     Parameters needed for network runs (train, test or inference). ---\n",
      "\ttrainingtype   : SGD\n",
      "\tlearning_rate  : 0.5\n",
      "\tmax_number_epochs: 100\n",
      "\tverbosity      : True\n",
      "\tmini_batch_size: 10\n",
      "\tweight_decay   : 0\n",
      "\tearly_stopping_epochs: 0\n",
      "\tearly_stopping_threshold: 0\n",
      "\tlearning_rate_scheduler: None\n",
      "\tlearning_rate_decay: 0.1\n",
      "\tlearning_rate_patience: 0\n",
      "\tuse_compression: False\n",
      "\tnum_workers    : 0\n",
      "\tuse_shuffling_for_samplers: True\n",
      "\tcheckpoints_each_epoch: 0\n",
      "\tcheckpoint_name: checkpoint_mala\n",
      "\tvisualisation  : 0\n",
      "\tvisualisation_dir: ./mala_logging\n",
      "\tvisualisation_dir_append_date: True\n",
      "\tduring_training_metric: ldos\n",
      "\tafter_before_training_metric: ldos\n",
      "\tinference_data_grid: [0, 0, 0]\n",
      "\tuse_mixed_precision: False\n",
      "\tuse_graphs     : False\n",
      "\ttraining_report_frequency: 1000\n",
      "\tprofiler_range : [1000, 2000]\n",
      "---     Hyperparameter optimization parameters. ---\n",
      "\tdirection      : minimize\n",
      "\tn_trials       : 100\n",
      "\thyper_opt_method: optuna\n",
      "\tcheckpoints_each_trial: 0\n",
      "\tcheckpoint_name: checkpoint_mala_ho\n",
      "\tstudy_name     : None\n",
      "\trdb_storage    : None\n",
      "\trdb_storage_heartbeat: None\n",
      "\tnumber_training_per_trial: 1\n",
      "\ttrial_ensemble_evaluation: mean\n",
      "\tuse_multivariate: True\n",
      "\tnaswot_pruner_cutoff: 0\n",
      "\tpruner         : None\n",
      "\tnaswot_pruner_batch_size: 0\n",
      "\tnumber_bad_trials_before_stopping: None\n",
      "\tsqlite_timeout : 600\n",
      "\tacsd_points    : 100\n",
      "---     All parameters to help with data generation. ---\n",
      "\ttrajectory_analysis_denoising_width: 100\n",
      "\ttrajectory_analysis_below_average_counter: 50\n",
      "\ttrajectory_analysis_estimated_equilibrium: 0.1\n",
      "\tlocal_psp_path : None\n",
      "\tlocal_psp_name : None\n",
      "\tofdft_timestep : 0\n",
      "\tofdft_number_of_timesteps: 0\n",
      "\tofdft_temperature: 0\n",
      "\tofdft_kedf     : WT\n",
      "\tofdft_friction : 0.1\n"
     ]
    }
   ],
   "source": [
    "# Edit something in the file (e.g. the manual_seed) and reload.\n",
    "parameters = mala.Parameters.load_from_file(\"mala_parameters_01.json\")\n",
    "parameters.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For extended experiments, it is very useful to operate with such input files and only use the in-python parameter editing when absolutely necessary. Further, concluded experiments can be saved in this way for future reference.\n",
    "In the following, we will use a combination of both approaches for the sake of transparency."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data II: Data preprocessing (presentation-only)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now start using MALA to prepare our data. MALA directly takes in calculation outputs and transforms it into formats with which we can easily work.\n",
    "First, we have to decide which descriptors to calculate and how to correctly process the LDOS."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "parameters = mala.Parameters()\n",
    "\n",
    "# These values we will take for granted now. In the hyperparameter section we will find out\n",
    "# how they are determined.\n",
    "parameters.descriptors.descriptor_type = \"Bispectrum\"\n",
    "parameters.descriptors.bispectrum_twojmax = 10\n",
    "parameters.descriptors.bispectrum_cutoff = 4.67637\n",
    "parameters.descriptors.descriptors_contain_xyz = True\n",
    "\n",
    "# These values need to correspond to the ones used in the DFT simulation.\n",
    "parameters.targets.target_type = \"LDOS\"\n",
    "parameters.targets.ldos_gridsize = 11\n",
    "parameters.targets.ldos_gridspacing_ev = 2.5\n",
    "parameters.targets.ldos_gridoffset_ev = -5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can use the `DataConverter` class to convert simulation outputs. File format labels follow the ASE package wherever possible!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disabling z-splitting for preprocessing.\n",
      "Calculating descriptors from ./data_generation/dft.out\n",
      "Reading 11 LDOS files from./data_generation/tmp.pp0*Be_ldos.cube.\n"
     ]
    }
   ],
   "source": [
    "data_converter = mala.DataConverter(parameters)\n",
    "data_converter.add_snapshot(descriptor_input_type=\"espresso-out\",\n",
    "                            descriptor_input_path=\"./data_generation/dft.out\",\n",
    "                            target_input_type=\".cube\",\n",
    "                            target_input_path=\"./data_generation/tmp.pp0*Be_ldos.cube\")\n",
    "data_converter.convert_snapshots(\"./data_generation/\", naming_scheme=\"Be_snapshot*.npy\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example we will use the [MALA test data set](https://github.com/mala-project/test-data), which contains 4 atomic configurations, including simulation output, bispectrum components and LDOS - i.e., this preprocessing has already been done for all of them."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "data_path = \"/home/fiedlerl/data/mala_data_repo/Be2\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}