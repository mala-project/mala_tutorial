{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine learning for computational materials science and chemistry with MALA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What is MALA?\n",
    "\n",
    "A short summary about MALA and how it works is given in `mala_background.pdf`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up MALA\n",
    "\n",
    "For this tutorial, a Google Collab enviroment will be provided that includes all necessary packages. Generally, MALA is an open source framework that can be obtained [here](https://github.com/mala-project/mala). Detailled (installation) instructions can be found [here](https://mala-project.github.io/mala/).\n",
    "\n",
    "A few examples at the end of the notebook tackle advanced applications. The necessary backends are, for the ease of installation, not bundled with the Google Collab environment. Interested readers may install them themselves on their machines, for in presence workshops, they will be demonstrated by the host."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the modules\n",
    "\n",
    "These modules will be necessary for the tutorials discussed here."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# MALA itself.\n",
    "\n",
    "import mala\n",
    "\n",
    "# We would like to visualize simple plots.\n",
    "# The font size can sometimes be a bit small for Jupyter Notebooks.\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "font = {'size'   : 22}\n",
    "matplotlib.rc('font', **font)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "\n",
    "Data is the backbone of each ML application."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The MALA interface\n",
    "\n",
    "Within extended ML frameworks, a big problem is reproducibility. Models often depend on a lot of so called hyperparameters, that characterize model behavior. This may include, but in no way be limited to, neural network layer sizes and number, training procedures, description of data specifics (e.g.: how is the local density of states sampled?), etc.\n",
    "There is a number of ways to efficiently handle this problem. A lot of frameworks rely on command line arguments to deal with this, i.e., the user provides a, potentially extensive, list of command line arguments upon runtime. Another good way to handle all this is the usage of input files. This is consistent with the way standard computational science simulation codes work.\n",
    "An obvious downside to this is that one has to have a framework at hand to prepare these input files.\n",
    "\n",
    "MALA follows a route sort of in between the two approaches. The central quantity is the `Parameters()` object. It holds ALL (hyper)parameters one could use in the course of a MALA run. It is structured by the subtasks of MALA."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<mala.common.parameters.ParametersHyperparameterOptimization at 0x7fab7d654280>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = mala.Parameters()\n",
    "\n",
    "# All parameters related to how data is handled in general.\n",
    "parameters.data\n",
    "\n",
    "# \"Targets\" always refers to the quantity being learned.\n",
    "parameters.targets\n",
    "\n",
    "# \"Descriptors\" always refers to the quantity from which we learn.\n",
    "parameters.descriptors\n",
    "\n",
    "# \"Data generation\" refers to useful routines for creating training data.\n",
    "# These routines are mostly experimental at the moment and will not be discussed here in detail\n",
    "parameters.datageneration\n",
    "\n",
    "# \"Network\" refers to everything related to neural network creation and training.\n",
    "# In the future, support for more models is planned, and this collection of parameters\n",
    "# will be updated to reflect this.\n",
    "parameters.network\n",
    "\n",
    "# \"Hyperparameters\" means hyperparameter optimization. This is the process of finding the optimal\n",
    "# hyperparameters for MALA model training, and we will come back to this process later.\n",
    "parameters.hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Individual parameter objects can be printed to see what's hidden inside. This also works on the main object as well."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snapshot_directories_list: []\n",
      "data_splitting_type: by_snapshot\n",
      "input_rescaling_type: None\n",
      "output_rescaling_type: None\n",
      "use_lazy_loading: False\n",
      "use_clustering : False\n",
      "number_of_clusters: 40\n",
      "train_ratio    : 0.1\n",
      "sample_ratio   : 0.5\n",
      "use_fast_tensor_data_set: False\n",
      "shuffling_seed : None\n"
     ]
    }
   ],
   "source": [
    "parameters.data.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, there are some high-level parameters one needs when performing ML at scale."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "''"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Whether or not to use a GPU for model training and inference.\n",
    "parameters.use_gpu\n",
    "\n",
    "# Whether or not to use MPI parallel CPU inference (no training supported).\n",
    "# This option is either for pre-processing or production runs of trained models.\n",
    "# More on that later.\n",
    "parameters.use_mpi\n",
    "\n",
    "# Manual seeds can be used to fix the Pseudo RNG to re-create models with the exact\n",
    "# same model weights.\n",
    "parameters.manual_seed\n",
    "\n",
    "# A comment may be useful to distinguish between sets of parameters.\n",
    "parameters.comment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All of this does not explain how MALA handles reproducibility. Write hundreds of lines of parameter statement in each python script is not exactly maintainable.\n",
    "Therefore MALA provides a .json interface."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "parameters.save(\"mala_parameters_01.json\")\n",
    "new_parameters = mala.Parameters.load_from_file(\"mala_parameters_01.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Have a look at the .json file that was just created. You will see that it is structured in the same ways as the python object, allowing fast access. You will see some parameters that are not dicussed here since they exceed the scope of this tutorial. For a first excercise, try to modify parameters both in python and in json and see whether loading will recover those changes. The comment and manual seed are good first examples for this."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Set a comment\n",
    "parameters.comment = \"My first parameters.\"\n",
    "\n",
    "# Save.\n",
    "parameters.save(\"mala_parameters_01.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---     All parameter MALA needs to perform its various tasks. ---\n",
      "comment        : My first parameters.\n",
      "manual_seed    : 1234\n",
      "use_gpu        : False\n",
      "device         : cpu\n",
      "use_horovod    : False\n",
      "use_mpi        : False\n",
      "verbosity      : 1\n",
      "openpmd_configuration: {}\n",
      "openpmd_granularity: 1\n",
      "---     Parameters necessary for constructing a neural network. ---\n",
      "\tnn_type        : feed-forward\n",
      "\tlayer_sizes    : [10, 10, 10]\n",
      "\tlayer_activations: ['Sigmoid']\n",
      "\tloss_function_type: mse\n",
      "\tnum_hidden_layers: 1\n",
      "\tno_hidden_state: False\n",
      "\tbidirection    : False\n",
      "\tdropout        : 0.1\n",
      "\tnum_heads      : 10\n",
      "---     Parameters necessary for calculating/parsing input descriptors. ---\n",
      "\tdescriptor_type: Bispectrum\n",
      "\tlammps_compute_file: \n",
      "\tdescriptors_contain_xyz: True\n",
      "\tuse_z_splitting: True\n",
      "\tnumber_y_planes: 0\n",
      "\tbispectrum_twojmax: 10\n",
      "\trcutfac        : 4.67637\n",
      "\tatomic_density_cutoff: 4.67637\n",
      "\tsnap_switchflag: 1\n",
      "\tuse_atomic_density_energy_formula: False\n",
      "\tatomic_density_sigma: None\n",
      "---     Parameters necessary for calculating/parsing output quantites. ---\n",
      "\ttarget_type    : LDOS\n",
      "\tldos_gridsize  : 0\n",
      "\tldos_gridspacing_ev: 0\n",
      "\tldos_gridoffset_ev: 0\n",
      "\trestrict_targets: zero_out_negative\n",
      "\tpseudopotential_path: None\n",
      "\trdf_parameters : {'number_of_bins': 500, 'rMax': 'mic'}\n",
      "\ttpcf_parameters: {'number_of_bins': 20, 'rMax': 'mic'}\n",
      "\tssf_parameters : {'number_of_bins': 100, 'kMax': 12.0}\n",
      "---     Parameters necessary for loading and preprocessing data. ---\n",
      "\tsnapshot_directories_list: []\n",
      "\tdata_splitting_type: by_snapshot\n",
      "\tinput_rescaling_type: None\n",
      "\toutput_rescaling_type: None\n",
      "\tuse_lazy_loading: False\n",
      "\tuse_clustering : False\n",
      "\tnumber_of_clusters: 40\n",
      "\ttrain_ratio    : 0.1\n",
      "\tsample_ratio   : 0.5\n",
      "\tuse_fast_tensor_data_set: False\n",
      "\tshuffling_seed : None\n",
      "---     Parameters needed for network runs (train, test or inference). ---\n",
      "\ttrainingtype   : SGD\n",
      "\tlearning_rate  : 0.5\n",
      "\tmax_number_epochs: 100\n",
      "\tverbosity      : True\n",
      "\tmini_batch_size: 10\n",
      "\tweight_decay   : 0\n",
      "\tearly_stopping_epochs: 0\n",
      "\tearly_stopping_threshold: 0\n",
      "\tlearning_rate_scheduler: None\n",
      "\tlearning_rate_decay: 0.1\n",
      "\tlearning_rate_patience: 0\n",
      "\tuse_compression: False\n",
      "\tnum_workers    : 0\n",
      "\tuse_shuffling_for_samplers: True\n",
      "\tcheckpoints_each_epoch: 0\n",
      "\tcheckpoint_name: checkpoint_mala\n",
      "\tvisualisation  : 0\n",
      "\tvisualisation_dir: ./mala_logging\n",
      "\tvisualisation_dir_append_date: True\n",
      "\tduring_training_metric: ldos\n",
      "\tafter_before_training_metric: ldos\n",
      "\tinference_data_grid: [0, 0, 0]\n",
      "\tuse_mixed_precision: False\n",
      "\tuse_graphs     : False\n",
      "\ttraining_report_frequency: 1000\n",
      "\tprofiler_range : [1000, 2000]\n",
      "---     Hyperparameter optimization parameters. ---\n",
      "\tdirection      : minimize\n",
      "\tn_trials       : 100\n",
      "\thyper_opt_method: optuna\n",
      "\tcheckpoints_each_trial: 0\n",
      "\tcheckpoint_name: checkpoint_mala_ho\n",
      "\tstudy_name     : None\n",
      "\trdb_storage    : None\n",
      "\trdb_storage_heartbeat: None\n",
      "\tnumber_training_per_trial: 1\n",
      "\ttrial_ensemble_evaluation: mean\n",
      "\tuse_multivariate: True\n",
      "\tnaswot_pruner_cutoff: 0\n",
      "\tpruner         : None\n",
      "\tnaswot_pruner_batch_size: 0\n",
      "\tnumber_bad_trials_before_stopping: None\n",
      "\tsqlite_timeout : 600\n",
      "\tacsd_points    : 100\n",
      "---     All parameters to help with data generation. ---\n",
      "\ttrajectory_analysis_denoising_width: 100\n",
      "\ttrajectory_analysis_below_average_counter: 50\n",
      "\ttrajectory_analysis_estimated_equilibrium: 0.1\n",
      "\tlocal_psp_path : None\n",
      "\tlocal_psp_name : None\n",
      "\tofdft_timestep : 0\n",
      "\tofdft_number_of_timesteps: 0\n",
      "\tofdft_temperature: 0\n",
      "\tofdft_kedf     : WT\n",
      "\tofdft_friction : 0.1\n"
     ]
    }
   ],
   "source": [
    "# Edit something in the file (e.g. the manual_seed) and reload.\n",
    "parameters = mala.Parameters.load_from_file(\"mala_parameters_01.json\")\n",
    "parameters.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For extended experiments, it is very useful to operate with such input files and only use the in-python parameter editing when absolutely necessary. Further, concluded experiments can be saved in this way for future reference.\n",
    "In the following, we will use a combination of both approaches for the sake of transparency."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}