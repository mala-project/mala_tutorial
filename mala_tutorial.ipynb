{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine learning for computational materials science and chemistry with MALA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What is MALA?\n",
    "\n",
    "A short summary about MALA and how it works is given in `mala_background.pdf`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up MALA\n",
    "\n",
    "For this tutorial, a Google Collab enviroment will be provided that includes all necessary packages. Generally, MALA is an open source framework that can be obtained [here](https://github.com/mala-project/mala). Detailled (installation) instructions can be found [here](https://mala-project.github.io/mala/).\n",
    "\n",
    "A few examples at the end of the notebook tackle advanced applications. The necessary backends are, for the ease of installation, not bundled with the Google Collab environment. Interested readers may install them themselves on their machines, for in presence workshops, they will be demonstrated by the host, as are sosme aspects of data generation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the modules\n",
    "\n",
    "These modules will be necessary for the tutorials discussed here."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "# MALA itself.\n",
    "\n",
    "import mala\n",
    "\n",
    "# We would like to visualize simple plots.\n",
    "# The font size can sometimes be a bit small for Jupyter Notebooks.\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "font = {'size'   : 22}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "# For the data paths.\n",
    "from os.path import join as pj\n",
    "\n",
    "# Only for the prediction down below.\n",
    "from ase.io import read\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data I: Performing simulations (presentation only)\n",
    "\n",
    "Data is the backbone of each ML application. For MALA, this includes target data (some electronic structure quantity to be learned, usually the local density of states, LDOS) and some descriptor data (a vectorial field that encodes the atomic density at each point in space, usually via so called bispectrum components).\n",
    "\n",
    "MALA data generation can be performed with the Quantum ESPRESSO package. Some changes to this open source package were necessary to enable the correct sampling of the LDOS. The current development branch of Quantum ESPRESSO includes those - beginning with Quantum ESPRESSO version 7.2 (to be released in ~June 2023) users can simply download the latest QE version and perform data generation.\n",
    "\n",
    "Data generation is two-fold: First, one creates a set of atomic position via a regular DFT-MD simulation at the conditions of interest. This can be done with any suitable code, such as VASP, QE, etc. Secondly, one performs DFT simulations to access the LDOS.\n",
    "\n",
    "The test system for our investigation here will be a simply beryllium system at room temperature consisting of 2 beryllium atoms. Atomic configurations have been sampled beforehand. We will start with the DFT simulation. Note: For actual data generation, the simulation output needs to be saved (usually as \".out\" file) and is of course done on HPC infrastructure."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "MALA data generation thus comes down to a simply DFT + Postprocessing calculation. There is one drawback though: The LDOS has to be sampled with a very high fidelity in the k-space (phase space, i.e., how the Fourier components of the basis set are sampled). The fidelity of the MALA calculation has to be higher then for standard DFT calculations, which has to be kept in mind. A good way to visualize the problem is through the density of states (LDOS integrated on real space grid) which shows unphysical oscillations for low fidelity calculations. These oscillations vanish as one moves to higher fidelity calculations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![kpoint_comparison](./figures/DOSwdifferentdeltasandks.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After we have performed the actual simulation, we are halfway done. We now have simulation outputs, both the DFT output as well as the LDOS. From this we need to do two things:\n",
    "\n",
    "1. We need to convert the LDOS into a format we actually want to work with. Cube files are unnecessarily huge, complicated both disk usage as well as speed.\n",
    "2. We need to calculate the atomic density descriptors from the atomic positions.\n",
    "\n",
    "So it's finally time to fire up MALA."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The MALA interface (hands-on)\n",
    "\n",
    "Within extended ML frameworks, a big problem is reproducibility. Models often depend on a lot of so called hyperparameters, that characterize model behavior. This may include, but in no way be limited to, neural network layer sizes and number, training procedures, description of data specifics (e.g.: how is the local density of states sampled?), etc.\n",
    "There is a number of ways to efficiently handle this problem. A lot of frameworks rely on command line arguments to deal with this, i.e., the user provides a, potentially extensive, list of command line arguments upon runtime. Another good way to handle all this is the usage of input files. This is consistent with the way standard computational science simulation codes work.\n",
    "An obvious downside to this is that one has to have a framework at hand to prepare these input files.\n",
    "\n",
    "MALA follows a route sort of in between the two approaches. The central quantity is the `Parameters()` object. It holds ALL (hyper)parameters one could use in the course of a MALA run. It is structured by the subtasks of MALA."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<mala.common.parameters.ParametersHyperparameterOptimization at 0x7f5c63aac760>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = mala.Parameters()\n",
    "\n",
    "# All parameters related to how data is handled in general.\n",
    "parameters.data\n",
    "\n",
    "# \"Targets\" always refers to the quantity being learned.\n",
    "parameters.targets\n",
    "\n",
    "# \"Descriptors\" always refers to the quantity from which we learn.\n",
    "parameters.descriptors\n",
    "\n",
    "# \"Data generation\" refers to useful routines for creating training data.\n",
    "# These routines are mostly experimental at the moment and will not be discussed here in detail\n",
    "parameters.datageneration\n",
    "\n",
    "# \"Network\" refers to everything related to neural network creation and training.\n",
    "# In the future, support for more models is planned, and this collection of parameters\n",
    "# will be updated to reflect this.\n",
    "parameters.network\n",
    "\n",
    "# \"Hyperparameters\" means hyperparameter optimization. This is the process of finding the optimal\n",
    "# hyperparameters for MALA model training, and we will come back to this process later.\n",
    "parameters.hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Individual parameter objects can be printed to see what's hidden inside. This also works on the main object as well."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snapshot_directories_list: []\n",
      "data_splitting_type: by_snapshot\n",
      "input_rescaling_type: None\n",
      "output_rescaling_type: None\n",
      "use_lazy_loading: False\n",
      "use_clustering : False\n",
      "number_of_clusters: 40\n",
      "train_ratio    : 0.1\n",
      "sample_ratio   : 0.5\n",
      "use_fast_tensor_data_set: False\n",
      "shuffling_seed : None\n"
     ]
    }
   ],
   "source": [
    "parameters.data.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, there are some high-level parameters one needs when performing ML at scale."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Whether or not to use a GPU for model training and inference.\n",
    "parameters.use_gpu\n",
    "\n",
    "# Whether or not to use MPI parallel CPU inference (no training supported).\n",
    "# This option is either for pre-processing or production runs of trained models.\n",
    "# More on that later.\n",
    "parameters.use_mpi\n",
    "\n",
    "# Manual seeds can be used to fix the Pseudo RNG to re-create models with the exact\n",
    "# same model weights.\n",
    "parameters.manual_seed\n",
    "\n",
    "# A comment may be useful to distinguish between sets of parameters.\n",
    "parameters.comment\n",
    "\n",
    "# This is useful for adjusting the output level of MALA.\n",
    "parameters.verbosity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All of this does not explain how MALA handles reproducibility. Write hundreds of lines of parameter statement in each python script is not exactly maintainable.\n",
    "Therefore MALA provides a .json interface."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "parameters.save(\"mala_parameters_01.json\")\n",
    "new_parameters = mala.Parameters.load_from_file(\"mala_parameters_01.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Have a look at the .json file that was just created. You will see that it is structured in the same ways as the python object, allowing fast access. You will see some parameters that are not dicussed here since they exceed the scope of this tutorial. For a first excercise, try to modify parameters both in python and in json and see whether loading will recover those changes. The comment and manual seed are good first examples for this."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Set a comment\n",
    "parameters.comment = \"My first parameters.\"\n",
    "\n",
    "# Save.\n",
    "parameters.save(\"mala_parameters_01.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---     All parameter MALA needs to perform its various tasks. ---\n",
      "comment        : My first parameters.\n",
      "manual_seed    : None\n",
      "use_gpu        : False\n",
      "device         : cpu\n",
      "use_horovod    : False\n",
      "use_mpi        : False\n",
      "verbosity      : 1\n",
      "openpmd_configuration: {}\n",
      "openpmd_granularity: 1\n",
      "---     Parameters necessary for constructing a neural network. ---\n",
      "\tnn_type        : feed-forward\n",
      "\tlayer_sizes    : [10, 10, 10]\n",
      "\tlayer_activations: ['Sigmoid']\n",
      "\tloss_function_type: mse\n",
      "\tnum_hidden_layers: 1\n",
      "\tno_hidden_state: False\n",
      "\tbidirection    : False\n",
      "\tdropout        : 0.1\n",
      "\tnum_heads      : 10\n",
      "---     Parameters necessary for calculating/parsing input descriptors. ---\n",
      "\tdescriptor_type: Bispectrum\n",
      "\tlammps_compute_file: \n",
      "\tdescriptors_contain_xyz: True\n",
      "\tuse_z_splitting: True\n",
      "\tnumber_y_planes: 0\n",
      "\tbispectrum_twojmax: 10\n",
      "\trcutfac        : 4.67637\n",
      "\tatomic_density_cutoff: 4.67637\n",
      "\tsnap_switchflag: 1\n",
      "\tuse_atomic_density_energy_formula: False\n",
      "\tatomic_density_sigma: None\n",
      "---     Parameters necessary for calculating/parsing output quantites. ---\n",
      "\ttarget_type    : LDOS\n",
      "\tldos_gridsize  : 0\n",
      "\tldos_gridspacing_ev: 0\n",
      "\tldos_gridoffset_ev: 0\n",
      "\trestrict_targets: zero_out_negative\n",
      "\tpseudopotential_path: None\n",
      "\trdf_parameters : {'number_of_bins': 500, 'rMax': 'mic'}\n",
      "\ttpcf_parameters: {'number_of_bins': 20, 'rMax': 'mic'}\n",
      "\tssf_parameters : {'number_of_bins': 100, 'kMax': 12.0}\n",
      "---     Parameters necessary for loading and preprocessing data. ---\n",
      "\tsnapshot_directories_list: []\n",
      "\tdata_splitting_type: by_snapshot\n",
      "\tinput_rescaling_type: None\n",
      "\toutput_rescaling_type: None\n",
      "\tuse_lazy_loading: False\n",
      "\tuse_clustering : False\n",
      "\tnumber_of_clusters: 40\n",
      "\ttrain_ratio    : 0.1\n",
      "\tsample_ratio   : 0.5\n",
      "\tuse_fast_tensor_data_set: False\n",
      "\tshuffling_seed : None\n",
      "---     Parameters needed for network runs (train, test or inference). ---\n",
      "\ttrainingtype   : SGD\n",
      "\tlearning_rate  : 0.5\n",
      "\tmax_number_epochs: 100\n",
      "\tverbosity      : True\n",
      "\tmini_batch_size: 10\n",
      "\tweight_decay   : 0\n",
      "\tearly_stopping_epochs: 0\n",
      "\tearly_stopping_threshold: 0\n",
      "\tlearning_rate_scheduler: None\n",
      "\tlearning_rate_decay: 0.1\n",
      "\tlearning_rate_patience: 0\n",
      "\tuse_compression: False\n",
      "\tnum_workers    : 0\n",
      "\tuse_shuffling_for_samplers: True\n",
      "\tcheckpoints_each_epoch: 0\n",
      "\tcheckpoint_name: checkpoint_mala\n",
      "\tvisualisation  : 0\n",
      "\tvisualisation_dir: ./mala_logging\n",
      "\tvisualisation_dir_append_date: True\n",
      "\tduring_training_metric: ldos\n",
      "\tafter_before_training_metric: ldos\n",
      "\tinference_data_grid: [0, 0, 0]\n",
      "\tuse_mixed_precision: False\n",
      "\tuse_graphs     : False\n",
      "\ttraining_report_frequency: 1000\n",
      "\tprofiler_range : [1000, 2000]\n",
      "---     Hyperparameter optimization parameters. ---\n",
      "\tdirection      : minimize\n",
      "\tn_trials       : 100\n",
      "\thyper_opt_method: optuna\n",
      "\tcheckpoints_each_trial: 0\n",
      "\tcheckpoint_name: checkpoint_mala_ho\n",
      "\tstudy_name     : None\n",
      "\trdb_storage    : None\n",
      "\trdb_storage_heartbeat: None\n",
      "\tnumber_training_per_trial: 1\n",
      "\ttrial_ensemble_evaluation: mean\n",
      "\tuse_multivariate: True\n",
      "\tnaswot_pruner_cutoff: 0\n",
      "\tpruner         : None\n",
      "\tnaswot_pruner_batch_size: 0\n",
      "\tnumber_bad_trials_before_stopping: None\n",
      "\tsqlite_timeout : 600\n",
      "\tacsd_points    : 100\n",
      "---     All parameters to help with data generation. ---\n",
      "\ttrajectory_analysis_denoising_width: 100\n",
      "\ttrajectory_analysis_below_average_counter: 50\n",
      "\ttrajectory_analysis_estimated_equilibrium: 0.1\n",
      "\tlocal_psp_path : None\n",
      "\tlocal_psp_name : None\n",
      "\tofdft_timestep : 0\n",
      "\tofdft_number_of_timesteps: 0\n",
      "\tofdft_temperature: 0\n",
      "\tofdft_kedf     : WT\n",
      "\tofdft_friction : 0.1\n"
     ]
    }
   ],
   "source": [
    "# Edit something in the file (e.g. the manual_seed) and reload.\n",
    "parameters = mala.Parameters.load_from_file(\"mala_parameters_01.json\")\n",
    "parameters.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For extended experiments, it is very useful to operate with such input files and only use the in-python parameter editing when absolutely necessary. Further, concluded experiments can be saved in this way for future reference.\n",
    "In the following, we will use a combination of both approaches for the sake of transparency."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data II: Data preprocessing (presentation-only)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now start using MALA to prepare our data. MALA directly takes in calculation outputs and transforms it into formats with which we can easily work.\n",
    "First, we have to decide which descriptors to calculate and how to correctly process the LDOS."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "parameters = mala.Parameters()\n",
    "\n",
    "# These values we will take for granted now. In the hyperparameter section we will find out\n",
    "# how they are determined.\n",
    "parameters.descriptors.descriptor_type = \"Bispectrum\"\n",
    "parameters.descriptors.bispectrum_twojmax = 10\n",
    "parameters.descriptors.bispectrum_cutoff = 4.67637\n",
    "parameters.descriptors.descriptors_contain_xyz = True\n",
    "\n",
    "# These values need to correspond to the ones used in the DFT simulation.\n",
    "parameters.targets.target_type = \"LDOS\"\n",
    "parameters.targets.ldos_gridsize = 11\n",
    "parameters.targets.ldos_gridspacing_ev = 2.5\n",
    "parameters.targets.ldos_gridoffset_ev = -5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can use the `DataConverter` class to convert simulation outputs. File format labels follow the ASE package wherever possible!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disabling z-splitting for preprocessing.\n",
      "Calculating descriptors from ./data_generation/dft.out\n"
     ]
    }
   ],
   "source": [
    "data_converter = mala.DataConverter(parameters)\n",
    "data_converter.add_snapshot(descriptor_input_type=\"espresso-out\",\n",
    "                            descriptor_input_path=\"./data_generation/dft.out\",\n",
    "                            target_input_type=\".cube\",\n",
    "                            target_input_path=\"./data_generation/tmp.pp0*Be_ldos.cube\")\n",
    "data_converter.convert_snapshots(\"./data_generation/\", naming_scheme=\"Be_snapshot*.npy\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example we will use the [MALA test data set](https://github.com/mala-project/test-data), which contains 4 atomic configurations, including simulation output, bispectrum components and LDOS - i.e., this preprocessing has already been done for all of them."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data_path = \"/home/fiedlerl/data/mala_data_repo/Be2\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualizing and reproducing output data (hands-on)\n",
    "\n",
    "Before we train a model, it is a good idea to think about which metric is important, i.e., how do we test if a model is good?\n",
    "\n",
    "In essence, the advantage of MALA is the access to multiple observables. Two easily accesible metrics are the density of states (DOS) and the band energy. We will now learn how to calculate them from the LDOS (the actual DFT LDOS in this case) so we can do the same after model training to test our models.\n",
    "\n",
    "For this, we first have to make sure the correct LDOS parameters are used."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parameters.targets.target_type = \"LDOS\"\n",
    "parameters.targets.ldos_gridsize = 11\n",
    "parameters.targets.ldos_gridspacing_ev = 2.5\n",
    "parameters.targets.ldos_gridoffset_ev = -5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can create an LDOS calculator and directly populate it with the LDOS data from the data set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ldos_calculator = mala.LDOS.from_numpy_file(parameters, pj(data_path, \"Be_snapshot0.out.npy\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Afterwards, we have to read in some additional information from the simulation data (size of the real space grid, temperature, etc.)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ldos_calculator.read_additional_calculation_data(pj(data_path, \"Be_snapshot0.out\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can access the DOS and the band energy as properties of the calculator object."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(ldos_calculator.energy_grid, ldos_calculator.density_of_states)\n",
    "\n",
    "ax.set_xlabel(\"Energy [eV]\")\n",
    "ax.set_ylabel(\"Density of States [1/eV]\")\n",
    "\n",
    "print(ldos_calculator.band_energy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training a model (hands-on)\n",
    "\n",
    "Finally, we can train a neural network based model for the electronic structure. First, let us review which parameters we need for this. In the following we will slowly adapt the parameters until we get a good model out of it.\n",
    "\n",
    "Since we want to learn about the inner workings of MALA, we want full output. We will also fix the manual seed, so that all the models are comparable."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "parameters = mala.Parameters()\n",
    "parameters.verbosity = 2\n",
    "parameters.manual_seed = 2023"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since MALA provides quite a few reasonable default values, in the simplest case the only thing we have to decide upon is the data we want to learn on and the architecture of the neural network.\n",
    "\n",
    "For each training we have to specify training (`\"tr\"`) and validation (`\"va\"`) snapshots. The former are used to actually tune the network weights, the latter monitor model performance during training. They will become very relevant as we optimize the training process.\n",
    "\n",
    "Deciding on the layer sizes is usually done AFTER the data is loaded, since the first and last layer need to match up with the data provided."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data rescaling will be performed.\n",
      "No data rescaling will be performed.\n",
      "Checking the snapshots and your inputs for consistency.\n",
      "Checking descriptor file  Be_snapshot0.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot0.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking descriptor file  Be_snapshot1.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot1.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Consistency check successful.\n",
      "Initializing the data scalers.\n",
      "Input scaler parametrized.\n",
      "Output scaler parametrized.\n",
      "Data scalers initialized.\n",
      "Build datasets.\n",
      "Build dataset: Done.\n"
     ]
    }
   ],
   "source": [
    "data_handler = mala.DataHandler(parameters)\n",
    "data_handler.add_snapshot(\"Be_snapshot0.in.npy\", data_path,\n",
    "                          \"Be_snapshot0.out.npy\", data_path, \"tr\")\n",
    "data_handler.add_snapshot(\"Be_snapshot1.in.npy\", data_path,\n",
    "                          \"Be_snapshot1.out.npy\", data_path, \"va\")\n",
    "\n",
    "# This already loads data into RAM!\n",
    "data_handler.prepare_data()\n",
    "parameters.network.layer_sizes = [data_handler.input_dimension,\n",
    "                                  100,\n",
    "                                  data_handler.output_dimension]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can actually train a network."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Guess - validation data loss:  0.23660719517299106\n",
      "Epoch 0: validation data loss: 2.0642450877598352e-05, training data loss: 0.0010316168240138463\n",
      "Time for epoch[s]: 14.075532674789429\n",
      "Epoch 1: validation data loss: 8.426698190825327e-06, training data loss: 9.5022799713271e-06\n",
      "Time for epoch[s]: 0.5715315341949463\n",
      "Epoch 2: validation data loss: 6.573324224778584e-06, training data loss: 4.276818728872708e-06\n",
      "Time for epoch[s]: 0.49625110626220703\n",
      "Epoch 3: validation data loss: 6.066970527172088e-06, training data loss: 3.2637411994593483e-06\n",
      "Time for epoch[s]: 0.5719480514526367\n",
      "Epoch 4: validation data loss: 5.8819215212549484e-06, training data loss: 2.9526403439896447e-06\n",
      "Time for epoch[s]: 0.614145040512085\n",
      "Epoch 5: validation data loss: 5.756421280758722e-06, training data loss: 2.780921491129058e-06\n",
      "Time for epoch[s]: 0.8275718688964844\n",
      "Epoch 6: validation data loss: 5.729225065026964e-06, training data loss: 2.724048016326768e-06\n",
      "Time for epoch[s]: 0.9969642162322998\n",
      "Epoch 7: validation data loss: 5.697706980364663e-06, training data loss: 2.702274758900915e-06\n",
      "Time for epoch[s]: 0.8270742893218994\n",
      "Epoch 8: validation data loss: 5.712806646312986e-06, training data loss: 2.691476472786495e-06\n",
      "Time for epoch[s]: 0.7120273113250732\n",
      "Epoch 9: validation data loss: 5.707444357020515e-06, training data loss: 2.685247255223138e-06\n",
      "Time for epoch[s]: 0.7106037139892578\n",
      "Epoch 10: validation data loss: 5.702229482786996e-06, training data loss: 2.682395013315337e-06\n",
      "Time for epoch[s]: 0.6155221462249756\n",
      "Epoch 11: validation data loss: 5.692719348839351e-06, training data loss: 2.6807026671511785e-06\n",
      "Time for epoch[s]: 0.8146119117736816\n",
      "Epoch 12: validation data loss: 5.695252014057977e-06, training data loss: 2.679776932512011e-06\n",
      "Time for epoch[s]: 0.6083807945251465\n",
      "Epoch 13: validation data loss: 5.691708730799811e-06, training data loss: 2.6787942541497095e-06\n",
      "Time for epoch[s]: 0.5773789882659912\n",
      "Epoch 14: validation data loss: 5.692836429391588e-06, training data loss: 2.6785630200590405e-06\n",
      "Time for epoch[s]: 0.7173018455505371\n",
      "Epoch 15: validation data loss: 5.673026932137353e-06, training data loss: 2.677937437381063e-06\n",
      "Time for epoch[s]: 0.789555549621582\n",
      "Epoch 16: validation data loss: 5.694691091775894e-06, training data loss: 2.678379150373595e-06\n",
      "Time for epoch[s]: 0.6404266357421875\n",
      "Epoch 17: validation data loss: 5.685808935335704e-06, training data loss: 2.6772831167493547e-06\n",
      "Time for epoch[s]: 0.5561385154724121\n",
      "Epoch 18: validation data loss: 5.695537797042301e-06, training data loss: 2.677000526871e-06\n",
      "Time for epoch[s]: 0.7231574058532715\n",
      "Epoch 19: validation data loss: 5.695699048893792e-06, training data loss: 2.6767663657665252e-06\n",
      "Time for epoch[s]: 0.774376630783081\n",
      "Epoch 20: validation data loss: 5.697107740810939e-06, training data loss: 2.6764550379344395e-06\n",
      "Time for epoch[s]: 0.6875059604644775\n",
      "Epoch 21: validation data loss: 5.683457744973047e-06, training data loss: 2.6765835604497364e-06\n",
      "Time for epoch[s]: 0.8308591842651367\n",
      "Epoch 22: validation data loss: 5.688743931906564e-06, training data loss: 2.675719027008329e-06\n",
      "Time for epoch[s]: 0.7643272876739502\n",
      "Epoch 23: validation data loss: 5.687439548117774e-06, training data loss: 2.6755500584840774e-06\n",
      "Time for epoch[s]: 0.9397306442260742\n",
      "Epoch 24: validation data loss: 5.696567041533334e-06, training data loss: 2.6751743363482613e-06\n",
      "Time for epoch[s]: 1.0406899452209473\n",
      "Epoch 25: validation data loss: 5.686759948730469e-06, training data loss: 2.674911969474384e-06\n",
      "Time for epoch[s]: 0.6029198169708252\n",
      "Epoch 26: validation data loss: 5.6885060455117905e-06, training data loss: 2.6745322559561047e-06\n",
      "Time for epoch[s]: 0.583899974822998\n",
      "Epoch 27: validation data loss: 5.679631871836526e-06, training data loss: 2.6747254388672967e-06\n",
      "Time for epoch[s]: 0.9348618984222412\n",
      "Epoch 28: validation data loss: 5.678408380065645e-06, training data loss: 2.674342000058719e-06\n",
      "Time for epoch[s]: 0.5834403038024902\n",
      "Epoch 29: validation data loss: 5.6765563786029815e-06, training data loss: 2.6734617671796254e-06\n",
      "Time for epoch[s]: 0.523076057434082\n",
      "Epoch 30: validation data loss: 5.689655031476702e-06, training data loss: 2.6729524667773927e-06\n",
      "Time for epoch[s]: 0.6305310726165771\n",
      "Epoch 31: validation data loss: 5.668357546840395e-06, training data loss: 2.67244635948113e-06\n",
      "Time for epoch[s]: 0.8350734710693359\n",
      "Epoch 32: validation data loss: 5.673676729202271e-06, training data loss: 2.671939453908375e-06\n",
      "Time for epoch[s]: 0.7913932800292969\n",
      "Epoch 33: validation data loss: 5.662577492850168e-06, training data loss: 2.6723119829382214e-06\n",
      "Time for epoch[s]: 0.682532787322998\n",
      "Epoch 34: validation data loss: 5.666138870375497e-06, training data loss: 2.670639327594212e-06\n",
      "Time for epoch[s]: 0.6397969722747803\n",
      "Epoch 35: validation data loss: 5.6451053491660525e-06, training data loss: 2.669292901243482e-06\n",
      "Time for epoch[s]: 0.6119339466094971\n",
      "Epoch 36: validation data loss: 5.596482860190528e-06, training data loss: 2.6646525199924197e-06\n",
      "Time for epoch[s]: 0.5647501945495605\n",
      "Epoch 37: validation data loss: 5.599578044244221e-06, training data loss: 2.6184325771672383e-06\n",
      "Time for epoch[s]: 0.5498390197753906\n",
      "Epoch 38: validation data loss: 5.596215171473367e-06, training data loss: 2.6153421827725e-06\n",
      "Time for epoch[s]: 0.5725610256195068\n",
      "Epoch 39: validation data loss: 5.597453032221113e-06, training data loss: 2.6145593396254948e-06\n",
      "Time for epoch[s]: 0.4898519515991211\n",
      "Epoch 40: validation data loss: 5.600179412535258e-06, training data loss: 2.613751749907221e-06\n",
      "Time for epoch[s]: 0.589958906173706\n",
      "Epoch 41: validation data loss: 5.597214613642011e-06, training data loss: 2.6128121784755164e-06\n",
      "Time for epoch[s]: 0.7074122428894043\n",
      "Epoch 42: validation data loss: 5.597128399780818e-06, training data loss: 2.6135175888027466e-06\n",
      "Time for epoch[s]: 0.6498312950134277\n",
      "Epoch 43: validation data loss: 5.5973817195211136e-06, training data loss: 2.612104107226644e-06\n",
      "Time for epoch[s]: 0.49660253524780273\n",
      "Epoch 44: validation data loss: 5.596153438091278e-06, training data loss: 2.6118976197072436e-06\n",
      "Time for epoch[s]: 0.6109602451324463\n",
      "Epoch 45: validation data loss: 5.5981714810643874e-06, training data loss: 2.6113949716091155e-06\n",
      "Time for epoch[s]: 0.5843162536621094\n",
      "Epoch 46: validation data loss: 5.596881466252463e-06, training data loss: 2.6079706315483367e-06\n",
      "Time for epoch[s]: 0.6611764430999756\n",
      "Epoch 47: validation data loss: 5.593972014529365e-06, training data loss: 2.60619047497e-06\n",
      "Time for epoch[s]: 0.7518255710601807\n",
      "Epoch 48: validation data loss: 5.599977714674813e-06, training data loss: 2.603631200534957e-06\n",
      "Time for epoch[s]: 0.6135280132293701\n",
      "Epoch 49: validation data loss: 5.5985237870897565e-06, training data loss: 2.6030471282345906e-06\n",
      "Time for epoch[s]: 0.535102128982544\n",
      "Epoch 50: validation data loss: 5.598042160272598e-06, training data loss: 2.6024886007819857e-06\n",
      "Time for epoch[s]: 0.915785551071167\n",
      "Epoch 51: validation data loss: 5.594375410250255e-06, training data loss: 2.6023324046816145e-06\n",
      "Time for epoch[s]: 0.9687020778656006\n",
      "Epoch 52: validation data loss: 5.595672343458448e-06, training data loss: 2.6014354079961777e-06\n",
      "Time for epoch[s]: 0.7520744800567627\n",
      "Epoch 53: validation data loss: 5.5941359273024966e-06, training data loss: 2.601393365434238e-06\n",
      "Time for epoch[s]: 0.8056824207305908\n",
      "Epoch 54: validation data loss: 5.59027014034135e-06, training data loss: 2.600994227187974e-06\n",
      "Time for epoch[s]: 0.5770814418792725\n",
      "Epoch 55: validation data loss: 5.588012614420482e-06, training data loss: 2.6007366499730518e-06\n",
      "Time for epoch[s]: 0.5289945602416992\n",
      "Epoch 56: validation data loss: 5.5851233857018605e-06, training data loss: 2.6008641081196923e-06\n",
      "Time for epoch[s]: 0.504213809967041\n",
      "Epoch 57: validation data loss: 5.582323563950402e-06, training data loss: 2.600413347993578e-06\n",
      "Time for epoch[s]: 0.5193929672241211\n",
      "Epoch 58: validation data loss: 5.578838820968356e-06, training data loss: 2.599148345845086e-06\n",
      "Time for epoch[s]: 0.5525875091552734\n",
      "Epoch 59: validation data loss: 5.572438240051269e-06, training data loss: 2.5972436581339156e-06\n",
      "Time for epoch[s]: 0.5304019451141357\n",
      "Epoch 60: validation data loss: 5.571066801037107e-06, training data loss: 2.5952979922294615e-06\n",
      "Time for epoch[s]: 0.610764741897583\n",
      "Epoch 61: validation data loss: 5.5686293968132565e-06, training data loss: 2.5935274149690357e-06\n",
      "Time for epoch[s]: 0.9232916831970215\n",
      "Epoch 62: validation data loss: 5.569770932197571e-06, training data loss: 2.5923800255571095e-06\n",
      "Time for epoch[s]: 0.6547732353210449\n",
      "Epoch 63: validation data loss: 5.569536771093096e-06, training data loss: 2.590401896408626e-06\n",
      "Time for epoch[s]: 0.5143871307373047\n",
      "Epoch 64: validation data loss: 5.5699955139841356e-06, training data loss: 2.587507079754557e-06\n",
      "Time for epoch[s]: 0.4905548095703125\n",
      "Epoch 65: validation data loss: 5.571945969547544e-06, training data loss: 2.585990886603083e-06\n",
      "Time for epoch[s]: 0.46637797355651855\n",
      "Epoch 66: validation data loss: 5.570605929408755e-06, training data loss: 2.5850747312818256e-06\n",
      "Time for epoch[s]: 0.6118371486663818\n",
      "Epoch 67: validation data loss: 5.5687097566468375e-06, training data loss: 2.5844089686870573e-06\n",
      "Time for epoch[s]: 0.7926161289215088\n",
      "Epoch 68: validation data loss: 5.5684234414781845e-06, training data loss: 2.583767952663558e-06\n",
      "Time for epoch[s]: 0.7259180545806885\n",
      "Epoch 69: validation data loss: 5.568031221628189e-06, training data loss: 2.583315862076623e-06\n",
      "Time for epoch[s]: 0.7184686660766602\n",
      "Epoch 70: validation data loss: 5.568735833678927e-06, training data loss: 2.5833770632743836e-06\n",
      "Time for epoch[s]: 0.6295461654663086\n",
      "Epoch 71: validation data loss: 5.564784897225244e-06, training data loss: 2.583258120076997e-06\n",
      "Time for epoch[s]: 0.5676460266113281\n",
      "Epoch 72: validation data loss: 5.56598390851702e-06, training data loss: 2.581950543182237e-06\n",
      "Time for epoch[s]: 0.5125396251678467\n",
      "Epoch 73: validation data loss: 5.565719945090158e-06, training data loss: 2.5817565619945525e-06\n",
      "Time for epoch[s]: 0.5333960056304932\n",
      "Epoch 74: validation data loss: 5.565832768167768e-06, training data loss: 2.581356625471796e-06\n",
      "Time for epoch[s]: 0.5480928421020508\n",
      "Epoch 75: validation data loss: 5.565087177923748e-06, training data loss: 2.580558881163597e-06\n",
      "Time for epoch[s]: 0.5401206016540527\n",
      "Epoch 76: validation data loss: 5.562589636870793e-06, training data loss: 2.5804849075419563e-06\n",
      "Time for epoch[s]: 0.5585849285125732\n",
      "Epoch 77: validation data loss: 5.565952509641647e-06, training data loss: 2.5803497327225548e-06\n",
      "Time for epoch[s]: 0.7106566429138184\n",
      "Epoch 78: validation data loss: 5.563034010784967e-06, training data loss: 2.580023503729275e-06\n",
      "Time for epoch[s]: 0.5654194355010986\n",
      "Epoch 79: validation data loss: 5.562925445181983e-06, training data loss: 2.579802115048681e-06\n",
      "Time for epoch[s]: 0.5879652500152588\n",
      "Epoch 80: validation data loss: 5.562736519745418e-06, training data loss: 2.5802270642348698e-06\n",
      "Time for epoch[s]: 0.6572055816650391\n",
      "Epoch 81: validation data loss: 5.562308643545423e-06, training data loss: 2.579363062977791e-06\n",
      "Time for epoch[s]: 0.5458753108978271\n",
      "Epoch 82: validation data loss: 5.561845643179758e-06, training data loss: 2.5790315121412277e-06\n",
      "Time for epoch[s]: 0.5072858333587646\n",
      "Epoch 83: validation data loss: 5.563337888036455e-06, training data loss: 2.578034996986389e-06\n",
      "Time for epoch[s]: 0.5083973407745361\n",
      "Epoch 84: validation data loss: 5.56429528764316e-06, training data loss: 2.5765475417886463e-06\n",
      "Time for epoch[s]: 0.5179479122161865\n",
      "Epoch 85: validation data loss: 5.562386874641691e-06, training data loss: 2.5758596935442517e-06\n",
      "Time for epoch[s]: 0.4535949230194092\n",
      "Epoch 86: validation data loss: 5.56017884186336e-06, training data loss: 2.5747971875326973e-06\n",
      "Time for epoch[s]: 0.5580353736877441\n",
      "Epoch 87: validation data loss: 5.5602288671902245e-06, training data loss: 2.575417182275227e-06\n",
      "Time for epoch[s]: 0.5249660015106201\n",
      "Epoch 88: validation data loss: 5.557117185422352e-06, training data loss: 2.574138343334198e-06\n",
      "Time for epoch[s]: 0.47477126121520996\n",
      "Epoch 89: validation data loss: 5.5592177169663566e-06, training data loss: 2.5747716426849364e-06\n",
      "Time for epoch[s]: 0.5890536308288574\n",
      "Epoch 90: validation data loss: 5.56168332695961e-06, training data loss: 2.5739302592618126e-06\n",
      "Time for epoch[s]: 0.5673470497131348\n",
      "Epoch 91: validation data loss: 5.556233759437289e-06, training data loss: 2.574151914034571e-06\n",
      "Time for epoch[s]: 0.5707414150238037\n",
      "Epoch 92: validation data loss: 5.5582448840141295e-06, training data loss: 2.5735893951995033e-06\n",
      "Time for epoch[s]: 0.46752429008483887\n",
      "Epoch 93: validation data loss: 5.5569506117275784e-06, training data loss: 2.573606957282339e-06\n",
      "Time for epoch[s]: 0.48187875747680664\n",
      "Epoch 94: validation data loss: 5.556435457297734e-06, training data loss: 2.573434263467789e-06\n",
      "Time for epoch[s]: 0.566683292388916\n",
      "Epoch 95: validation data loss: 5.555758518832071e-06, training data loss: 2.573469919817788e-06\n",
      "Time for epoch[s]: 0.7254226207733154\n",
      "Epoch 96: validation data loss: 5.555643034832818e-06, training data loss: 2.572829702070781e-06\n",
      "Time for epoch[s]: 0.7218220233917236\n",
      "Epoch 97: validation data loss: 5.554889461823872e-06, training data loss: 2.572573721408844e-06\n",
      "Time for epoch[s]: 0.5735270977020264\n",
      "Epoch 98: validation data loss: 5.557044808353697e-06, training data loss: 2.5722972516502653e-06\n",
      "Time for epoch[s]: 0.5974586009979248\n",
      "Epoch 99: validation data loss: 5.555403019700731e-06, training data loss: 2.572127484849521e-06\n",
      "Time for epoch[s]: 0.7279651165008545\n",
      "Final validation data loss:  5.555403019700731e-06\n"
     ]
    }
   ],
   "source": [
    "network = mala.Network(parameters)\n",
    "trainer = mala.Trainer(parameters, network, data_handler)\n",
    "trainer.train_network()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Well, how was that? Do we have a good model now, can we predict the LDOS with this? That is not an easy question to answer from this output alone. First of all, we see loss values being printed, and those look all nice, but they are not trivially related to physical/chemical accuracy, which we are actually looking for.\n",
    "\n",
    "We can test this by using the `Tester` class. The class works similar to the Trainer class. We add data, push them through the model, and then use the results to perform calculations.\n",
    "\n",
    "We just have to make sure that the LDOS is correctly integrated by setting the appropriate parameters. Then we can add data to test. We should always test on data different from the one we trained on. Also, we now have to specify the corresponding calculation output, since we may need this for integration."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the snapshots and your inputs for consistency.\n",
      "Checking descriptor file  Be_snapshot2.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot2.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking descriptor file  Be_snapshot3.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot3.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "DataHandler prepared for inference. No training possible with this setup. If this is not what you wanted, please revise the input script. Validation snapshots you may have entered willbe ignored.\n",
      "Consistency check successful.\n",
      "Build datasets.\n",
      "Build dataset: Done.\n"
     ]
    }
   ],
   "source": [
    "parameters.targets.ldos_gridsize = 11\n",
    "parameters.targets.ldos_gridspacing_ev = 2.5\n",
    "parameters.targets.ldos_gridoffset_ev = -5\n",
    "\n",
    "data_handler.clear_data()\n",
    "data_handler.add_snapshot(\"Be_snapshot2.in.npy\", data_path,\n",
    "                          \"Be_snapshot2.out.npy\", data_path, \"te\",\n",
    "                          calculation_output_file=pj(data_path, \"Be_snapshot2.out\"))\n",
    "data_handler.add_snapshot(\"Be_snapshot3.in.npy\", data_path,\n",
    "                          \"Be_snapshot3.out.npy\", data_path, \"te\",\n",
    "                          calculation_output_file=pj(data_path, \"Be_snapshot3.out\"))\n",
    "data_handler.prepare_data(reparametrize_scaler=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now comes the actual object with which to test. We simply tell it which observables to test for and off we go."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "tester = mala.Tester(parameters, network, data_handler, observables_to_test=[\"band_energy\"])\n",
    "results = tester.test_all_snapshots()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The results are given as a dictionary and in the units of meV/atom."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "{'band_energy': [-0.0352251206432328, -0.02902837244603873]}"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That already looks pretty solid. On the machine this notebook was tested on, errors below 0.1 meV/atom were reported, i.e., the network reproduces the band energy excellently. We may also visualize the DOS, and will do so later, but for now let's focus on the band energy. Can we get even better results? Of course, for simple data like this, we already seem to be doing a good job. But larger and more diverse data sets can be tricky.\n",
    "\n",
    "So let us see if we can improve the training routine even further. To do so, first let's rewrite the code above into functions so that we can toy around with them."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def training(parameters):\n",
    "    data_handler = mala.DataHandler(parameters)\n",
    "    data_handler.clear_data()\n",
    "\n",
    "    data_handler.add_snapshot(\"Be_snapshot0.in.npy\", data_path,\n",
    "                              \"Be_snapshot0.out.npy\", data_path, \"tr\")\n",
    "    data_handler.add_snapshot(\"Be_snapshot1.in.npy\", data_path,\n",
    "                              \"Be_snapshot1.out.npy\", data_path, \"va\")\n",
    "\n",
    "    # This already loads data into RAM!\n",
    "    data_handler.prepare_data()\n",
    "    parameters.network.layer_sizes.insert(0, data_handler.input_dimension)\n",
    "    parameters.network.layer_sizes.append(data_handler.output_dimension)\n",
    "    network = mala.Network(parameters)\n",
    "    trainer = mala.Trainer(parameters, network, data_handler)\n",
    "    trainer.train_network()\n",
    "\n",
    "\n",
    "    return parameters, data_handler, network\n",
    "\n",
    "def testing(parameters, data_handler, network):\n",
    "\n",
    "    parameters.targets.ldos_gridsize = 11\n",
    "    parameters.targets.ldos_gridspacing_ev = 2.5\n",
    "    parameters.targets.ldos_gridoffset_ev = -5\n",
    "\n",
    "    data_handler.clear_data()\n",
    "    data_handler.add_snapshot(\"Be_snapshot2.in.npy\", data_path,\n",
    "                              \"Be_snapshot2.out.npy\", data_path, \"te\",\n",
    "                              calculation_output_file=pj(data_path, \"Be_snapshot2.out\"))\n",
    "    data_handler.add_snapshot(\"Be_snapshot3.in.npy\", data_path,\n",
    "                              \"Be_snapshot3.out.npy\", data_path, \"te\",\n",
    "                              calculation_output_file=pj(data_path, \"Be_snapshot3.out\"))\n",
    "    data_handler.prepare_data(reparametrize_scaler=False)\n",
    "\n",
    "    tester = mala.Tester(parameters, network, data_handler, observables_to_test=[\"band_energy\"])\n",
    "    results = tester.test_all_snapshots()\n",
    "    return results\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's see what happens if we start changing up the parameters. Let's start with those that influence the network architecture first. If we define the functions in an appropriate way, we can easily manipulate layer sizes of the neural networks for instance. What happens if we choose a ridiculously small number of neurons for the hidden layer? What happens if we choose a large number? Try e.g. 2 or 1000 as number for the hidden layer (the one in between)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data rescaling will be performed.\n",
      "No data rescaling will be performed.\n",
      "Checking the snapshots and your inputs for consistency.\n",
      "Checking descriptor file  Be_snapshot0.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot0.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking descriptor file  Be_snapshot1.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot1.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Consistency check successful.\n",
      "Initializing the data scalers.\n",
      "Input scaler parametrized.\n",
      "Output scaler parametrized.\n",
      "Data scalers initialized.\n",
      "Build datasets.\n",
      "Build dataset: Done.\n",
      "Initial Guess - validation data loss:  0.244789306640625\n",
      "Epoch 0: validation data loss: 5.834028124809265e-06, training data loss: 0.00029535348074776786\n",
      "Time for epoch[s]: 1.2008910179138184\n",
      "Epoch 1: validation data loss: 5.244482308626175e-06, training data loss: 2.741139116031783e-06\n",
      "Time for epoch[s]: 1.0245182514190674\n",
      "Epoch 2: validation data loss: 4.929769784212113e-06, training data loss: 2.128387801349163e-06\n",
      "Time for epoch[s]: 0.8836274147033691\n",
      "Epoch 3: validation data loss: 4.762781517846244e-06, training data loss: 1.8161613760249955e-06\n",
      "Time for epoch[s]: 0.9657278060913086\n",
      "Epoch 4: validation data loss: 4.624924489430019e-06, training data loss: 1.6222886208977017e-06\n",
      "Time for epoch[s]: 1.0848958492279053\n",
      "Epoch 5: validation data loss: 4.525044134684971e-06, training data loss: 1.4753945704017366e-06\n",
      "Time for epoch[s]: 0.9729197025299072\n",
      "Epoch 6: validation data loss: 4.425929061004093e-06, training data loss: 1.3596758778606142e-06\n",
      "Time for epoch[s]: 0.8690645694732666\n",
      "Epoch 7: validation data loss: 4.361816282783236e-06, training data loss: 1.2635821476578712e-06\n",
      "Time for epoch[s]: 0.8533074855804443\n",
      "Epoch 8: validation data loss: 4.3037962168455125e-06, training data loss: 1.1865913069673946e-06\n",
      "Time for epoch[s]: 0.8642852306365967\n",
      "Epoch 9: validation data loss: 4.2643618902989796e-06, training data loss: 1.125034343983446e-06\n",
      "Time for epoch[s]: 0.8091838359832764\n",
      "Epoch 10: validation data loss: 4.2058969182627546e-06, training data loss: 1.071995190743889e-06\n",
      "Time for epoch[s]: 0.8750100135803223\n",
      "Epoch 11: validation data loss: 4.193282819219998e-06, training data loss: 1.0275359132460184e-06\n",
      "Time for epoch[s]: 1.1300954818725586\n",
      "Epoch 12: validation data loss: 4.143711179494858e-06, training data loss: 9.87964349665812e-07\n",
      "Time for epoch[s]: 1.4861729145050049\n",
      "Epoch 13: validation data loss: 4.124534715499197e-06, training data loss: 9.520175600690501e-07\n",
      "Time for epoch[s]: 1.267737627029419\n",
      "Epoch 14: validation data loss: 4.111648138080325e-06, training data loss: 9.204593620129994e-07\n",
      "Time for epoch[s]: 1.1939799785614014\n",
      "Epoch 15: validation data loss: 4.078522590654237e-06, training data loss: 8.922993604625974e-07\n",
      "Time for epoch[s]: 1.0839383602142334\n",
      "Epoch 16: validation data loss: 4.048402022038188e-06, training data loss: 8.674472836511475e-07\n",
      "Time for epoch[s]: 1.1358551979064941\n",
      "Epoch 17: validation data loss: 4.04401496052742e-06, training data loss: 8.44614247658423e-07\n",
      "Time for epoch[s]: 1.3044228553771973\n",
      "Epoch 18: validation data loss: 4.025137051939964e-06, training data loss: 8.236555648701531e-07\n",
      "Time for epoch[s]: 1.4294588565826416\n",
      "Epoch 19: validation data loss: 4.013977146574429e-06, training data loss: 8.03465023636818e-07\n",
      "Time for epoch[s]: 1.287536859512329\n",
      "Epoch 20: validation data loss: 4.013364602412496e-06, training data loss: 7.841698825359344e-07\n",
      "Time for epoch[s]: 1.097550630569458\n",
      "Epoch 21: validation data loss: 3.995969625455992e-06, training data loss: 7.659875388656344e-07\n",
      "Time for epoch[s]: 1.0526623725891113\n",
      "Epoch 22: validation data loss: 3.981759239520346e-06, training data loss: 7.481735332735947e-07\n",
      "Time for epoch[s]: 1.5173144340515137\n",
      "Epoch 23: validation data loss: 3.974564905677523e-06, training data loss: 7.307136963520731e-07\n",
      "Time for epoch[s]: 1.4187610149383545\n",
      "Epoch 24: validation data loss: 3.948054143360683e-06, training data loss: 7.137264391141279e-07\n",
      "Time for epoch[s]: 1.1770122051239014\n",
      "Epoch 25: validation data loss: 3.9370589490447725e-06, training data loss: 6.984172921095576e-07\n",
      "Time for epoch[s]: 1.3630285263061523\n",
      "Epoch 26: validation data loss: 3.934834950736591e-06, training data loss: 6.839311016457421e-07\n",
      "Time for epoch[s]: 1.112184762954712\n",
      "Epoch 27: validation data loss: 3.909551139388766e-06, training data loss: 6.700302474200725e-07\n",
      "Time for epoch[s]: 1.2613344192504883\n",
      "Epoch 28: validation data loss: 3.895905667117664e-06, training data loss: 6.546833818512303e-07\n",
      "Time for epoch[s]: 1.130436658859253\n",
      "Epoch 29: validation data loss: 3.869765571185521e-06, training data loss: 6.408440614385265e-07\n",
      "Time for epoch[s]: 1.177762746810913\n",
      "Epoch 30: validation data loss: 3.865611074226243e-06, training data loss: 6.294063558535917e-07\n",
      "Time for epoch[s]: 1.1164555549621582\n",
      "Epoch 31: validation data loss: 3.879226744174957e-06, training data loss: 6.193071603775024e-07\n",
      "Time for epoch[s]: 1.2834997177124023\n",
      "Epoch 32: validation data loss: 3.871617306556021e-06, training data loss: 6.091702463371413e-07\n",
      "Time for epoch[s]: 1.305861234664917\n",
      "Epoch 33: validation data loss: 3.859372011252812e-06, training data loss: 5.991860026759761e-07\n",
      "Time for epoch[s]: 1.1212878227233887\n",
      "Epoch 34: validation data loss: 3.842448017426899e-06, training data loss: 5.898332622434412e-07\n",
      "Time for epoch[s]: 1.1110165119171143\n",
      "Epoch 35: validation data loss: 3.837319357054574e-06, training data loss: 5.810892741594996e-07\n",
      "Time for epoch[s]: 1.1413836479187012\n",
      "Epoch 36: validation data loss: 3.8042677832501274e-06, training data loss: 5.726385861635208e-07\n",
      "Time for epoch[s]: 1.250610113143921\n",
      "Epoch 37: validation data loss: 3.808795873607908e-06, training data loss: 5.6473258882761e-07\n",
      "Time for epoch[s]: 1.367995023727417\n",
      "Epoch 38: validation data loss: 3.8112801100526536e-06, training data loss: 5.570818404001849e-07\n",
      "Time for epoch[s]: 1.1147549152374268\n",
      "Epoch 39: validation data loss: 3.809166806084769e-06, training data loss: 5.496346857398748e-07\n",
      "Time for epoch[s]: 1.2703826427459717\n",
      "Epoch 40: validation data loss: 3.7976261228322983e-06, training data loss: 5.42526033573917e-07\n",
      "Time for epoch[s]: 1.1407980918884277\n",
      "Epoch 41: validation data loss: 3.804136599813189e-06, training data loss: 5.357798321970871e-07\n",
      "Time for epoch[s]: 1.1094927787780762\n",
      "Epoch 42: validation data loss: 3.7748076553855624e-06, training data loss: 5.289212401424136e-07\n",
      "Time for epoch[s]: 1.3416838645935059\n",
      "Epoch 43: validation data loss: 3.7694341902221954e-06, training data loss: 5.226787512323687e-07\n",
      "Time for epoch[s]: 1.0877509117126465\n",
      "Epoch 44: validation data loss: 3.7639361939259938e-06, training data loss: 5.164961330592632e-07\n",
      "Time for epoch[s]: 1.1828503608703613\n",
      "Epoch 45: validation data loss: 3.757831241403307e-06, training data loss: 5.103229611579861e-07\n",
      "Time for epoch[s]: 1.1612474918365479\n",
      "Epoch 46: validation data loss: 3.726411876933915e-06, training data loss: 5.04297370623265e-07\n",
      "Time for epoch[s]: 1.1881487369537354\n",
      "Epoch 47: validation data loss: 3.7583075463771822e-06, training data loss: 4.987207773540701e-07\n",
      "Time for epoch[s]: 1.531139850616455\n",
      "Epoch 48: validation data loss: 3.730817564896175e-06, training data loss: 4.931946085499866e-07\n",
      "Time for epoch[s]: 1.0928459167480469\n",
      "Epoch 49: validation data loss: 3.7243666925600596e-06, training data loss: 4.879633696483715e-07\n",
      "Time for epoch[s]: 1.1925368309020996\n",
      "Epoch 50: validation data loss: 3.732029614703996e-06, training data loss: 4.828534687736204e-07\n",
      "Time for epoch[s]: 1.3527753353118896\n",
      "Epoch 51: validation data loss: 3.7347754197461264e-06, training data loss: 4.779226811868804e-07\n",
      "Time for epoch[s]: 1.2664515972137451\n",
      "Epoch 52: validation data loss: 3.7011091730424335e-06, training data loss: 4.7281194877411636e-07\n",
      "Time for epoch[s]: 1.4006521701812744\n",
      "Epoch 53: validation data loss: 3.738165700009891e-06, training data loss: 4.6758393623999185e-07\n",
      "Time for epoch[s]: 1.1717250347137451\n",
      "Epoch 54: validation data loss: 3.7457112755094254e-06, training data loss: 4.6261163827564035e-07\n",
      "Time for epoch[s]: 1.0644948482513428\n",
      "Epoch 55: validation data loss: 3.7369890404599055e-06, training data loss: 4.580716070319925e-07\n",
      "Time for epoch[s]: 1.0847985744476318\n",
      "Epoch 56: validation data loss: 3.7175725613321577e-06, training data loss: 4.536323581955263e-07\n",
      "Time for epoch[s]: 1.0882060527801514\n",
      "Epoch 57: validation data loss: 3.730743591274534e-06, training data loss: 4.494400096258947e-07\n",
      "Time for epoch[s]: 1.4872260093688965\n",
      "Epoch 58: validation data loss: 3.7324904863323485e-06, training data loss: 4.453628124403102e-07\n",
      "Time for epoch[s]: 1.3501582145690918\n",
      "Epoch 59: validation data loss: 3.707300073334149e-06, training data loss: 4.413437896541187e-07\n",
      "Time for epoch[s]: 1.1024315357208252\n",
      "Epoch 60: validation data loss: 3.6912150681018828e-06, training data loss: 4.3746100605598517e-07\n",
      "Time for epoch[s]: 1.1085553169250488\n",
      "Epoch 61: validation data loss: 3.6833501820053372e-06, training data loss: 4.3377300192202843e-07\n",
      "Time for epoch[s]: 1.090376615524292\n",
      "Epoch 62: validation data loss: 3.7265667425734657e-06, training data loss: 4.3018960527011326e-07\n",
      "Time for epoch[s]: 1.2793598175048828\n",
      "Epoch 63: validation data loss: 3.7289283105305264e-06, training data loss: 4.2662713011460644e-07\n",
      "Time for epoch[s]: 1.2316722869873047\n",
      "Epoch 64: validation data loss: 3.705254090683801e-06, training data loss: 4.230653201895101e-07\n",
      "Time for epoch[s]: 1.4130175113677979\n",
      "Epoch 65: validation data loss: 3.700103344661849e-06, training data loss: 4.195959772914648e-07\n",
      "Time for epoch[s]: 1.3436882495880127\n",
      "Epoch 66: validation data loss: 3.7127118557691575e-06, training data loss: 4.161677123712642e-07\n",
      "Time for epoch[s]: 1.1827938556671143\n",
      "Epoch 67: validation data loss: 3.6803696836744036e-06, training data loss: 4.1277314137135234e-07\n",
      "Time for epoch[s]: 1.3676321506500244\n",
      "Epoch 68: validation data loss: 3.6609566637447904e-06, training data loss: 4.0926993824541567e-07\n",
      "Time for epoch[s]: 1.167663812637329\n",
      "Epoch 69: validation data loss: 3.667564530457769e-06, training data loss: 4.056044854223728e-07\n",
      "Time for epoch[s]: 1.079380750656128\n",
      "Epoch 70: validation data loss: 3.6425372319562095e-06, training data loss: 4.0205451659858225e-07\n",
      "Time for epoch[s]: 1.0789766311645508\n",
      "Epoch 71: validation data loss: 3.641250410250255e-06, training data loss: 3.9856372001980033e-07\n",
      "Time for epoch[s]: 1.080477237701416\n",
      "Epoch 72: validation data loss: 3.65725186254297e-06, training data loss: 3.950945101678371e-07\n",
      "Time for epoch[s]: 1.1329407691955566\n",
      "Epoch 73: validation data loss: 3.6613097680466515e-06, training data loss: 3.9173057302832604e-07\n",
      "Time for epoch[s]: 1.5341432094573975\n",
      "Epoch 74: validation data loss: 3.6289628062929424e-06, training data loss: 3.883328089224441e-07\n",
      "Time for epoch[s]: 1.4934556484222412\n",
      "Epoch 75: validation data loss: 3.612676901476724e-06, training data loss: 3.853552376053163e-07\n",
      "Time for epoch[s]: 1.269176959991455\n",
      "Epoch 76: validation data loss: 3.6322192421981268e-06, training data loss: 3.822708302842719e-07\n",
      "Time for epoch[s]: 1.1240031719207764\n",
      "Epoch 77: validation data loss: 3.584957814642361e-06, training data loss: 3.7902274302073887e-07\n",
      "Time for epoch[s]: 1.3822729587554932\n",
      "Epoch 78: validation data loss: 3.603496721812657e-06, training data loss: 3.762632674936737e-07\n",
      "Time for epoch[s]: 1.1974272727966309\n",
      "Epoch 79: validation data loss: 3.597346799714225e-06, training data loss: 3.7338238741670337e-07\n",
      "Time for epoch[s]: 1.239661455154419\n",
      "Epoch 80: validation data loss: 3.5805281783853257e-06, training data loss: 3.707667347043753e-07\n",
      "Time for epoch[s]: 1.120849847793579\n",
      "Epoch 81: validation data loss: 3.5859812051057817e-06, training data loss: 3.6806992388197356e-07\n",
      "Time for epoch[s]: 1.6426100730895996\n",
      "Epoch 82: validation data loss: 3.5816130361386707e-06, training data loss: 3.653071222028562e-07\n",
      "Time for epoch[s]: 2.4996538162231445\n",
      "Epoch 83: validation data loss: 3.5773145833185743e-06, training data loss: 3.6277941295078824e-07\n",
      "Time for epoch[s]: 2.0191657543182373\n",
      "Epoch 84: validation data loss: 3.58041056564876e-06, training data loss: 3.6026989775044576e-07\n",
      "Time for epoch[s]: 1.6329553127288818\n",
      "Epoch 85: validation data loss: 3.5864806600979395e-06, training data loss: 3.577428204672677e-07\n",
      "Time for epoch[s]: 1.5321693420410156\n",
      "Epoch 86: validation data loss: 3.5725754818746024e-06, training data loss: 3.5550747998058797e-07\n",
      "Time for epoch[s]: 1.6321845054626465\n",
      "Epoch 87: validation data loss: 3.569589661700385e-06, training data loss: 3.53209042389478e-07\n",
      "Time for epoch[s]: 1.1782019138336182\n",
      "Epoch 88: validation data loss: 3.5829943205629076e-06, training data loss: 3.5080167331865853e-07\n",
      "Time for epoch[s]: 1.2938637733459473\n",
      "Epoch 89: validation data loss: 3.539341368845531e-06, training data loss: 3.4875825180539065e-07\n",
      "Time for epoch[s]: 1.2029798030853271\n",
      "Epoch 90: validation data loss: 3.5582448222807476e-06, training data loss: 3.466461119907243e-07\n",
      "Time for epoch[s]: 1.9121990203857422\n",
      "Epoch 91: validation data loss: 3.549937158823013e-06, training data loss: 3.4448657450931413e-07\n",
      "Time for epoch[s]: 1.197505235671997\n",
      "Epoch 92: validation data loss: 3.543146486793246e-06, training data loss: 3.425062833619969e-07\n",
      "Time for epoch[s]: 1.1312694549560547\n",
      "Epoch 93: validation data loss: 3.5263411700725556e-06, training data loss: 3.403990329908473e-07\n",
      "Time for epoch[s]: 1.244464635848999\n",
      "Epoch 94: validation data loss: 3.517695835658482e-06, training data loss: 3.384517372718879e-07\n",
      "Time for epoch[s]: 1.1586968898773193\n",
      "Epoch 95: validation data loss: 3.5281255841255186e-06, training data loss: 3.366077518356698e-07\n",
      "Time for epoch[s]: 1.9060869216918945\n",
      "Epoch 96: validation data loss: 3.5274970744337354e-06, training data loss: 3.3476266876927443e-07\n",
      "Time for epoch[s]: 1.479750156402588\n",
      "Epoch 97: validation data loss: 3.5471320152282717e-06, training data loss: 3.3283662716192856e-07\n",
      "Time for epoch[s]: 1.7484803199768066\n",
      "Epoch 98: validation data loss: 3.515759749071939e-06, training data loss: 3.311389258929661e-07\n",
      "Time for epoch[s]: 1.4727373123168945\n",
      "Epoch 99: validation data loss: 3.4991691687277384e-06, training data loss: 3.29408362241728e-07\n",
      "Time for epoch[s]: 1.5963051319122314\n",
      "Final validation data loss:  3.4991691687277384e-06\n",
      "Checking the snapshots and your inputs for consistency.\n",
      "Checking descriptor file  Be_snapshot2.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot2.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking descriptor file  Be_snapshot3.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot3.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "DataHandler prepared for inference. No training possible with this setup. If this is not what you wanted, please revise the input script. Validation snapshots you may have entered willbe ignored.\n",
      "Consistency check successful.\n",
      "Build datasets.\n",
      "Build dataset: Done.\n",
      "Had to readjust batch size from 10 to 12\n",
      "{'band_energy': [0.10552580387314592, 0.040525814617691225]}\n"
     ]
    }
   ],
   "source": [
    "parameters = mala.Parameters()\n",
    "parameters.verbosity = 2\n",
    "parameters.manual_seed = 2023\n",
    "parameters.network.layer_sizes = [1000]\n",
    "\n",
    "parameters, data_handler, network = training(parameters)\n",
    "print(testing(parameters, data_handler, network))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, so this clearly has an effect. We can build better and worse models by simply adjusting this number. Models that are too small don't have enough information capacity - they simply cannot capture the information present in the data. This is called **underfitting**.\n",
    "\n",
    "But also big models (if you haven't tried - try e.g. 1000 neurons!) do not yield immediate improvement. In my case, 1000 neurons in the middle gave noticeably worse performance.\n",
    "\n",
    "Why is that? Large models possess a large capacity to store information. If they are not carefully fitted, they may very soon begin to **overfit**, i.e. learn to predict the training data - and only the training data. This is obviously not what is supposed to happen.\n",
    "\n",
    "This is what the aforementioned validation data is for. Validation data is data unseen by the model during the weight optimization, but checked after each epoch. The idea is simple: Since the model is not directly fitted on the validation data, we can use it to monitor fitting progress. If validation accuracy is drastically different from training accuracy, something is off.\n",
    "\n",
    "If you check your outputs, you will realize that both losses behave differently for different models. In the 1000 neuron case at the end you will encounter a factor of 10 between both of them! For a smaller model, it was only 2.\n",
    "\n",
    "There is one way to alleviate this: Early stopping. The idea is to end training prematurely if we see validation accuracy stagnating for a certain amount of time, i.e., the model starting to overfit. MALA supports this with a simple command. Let's try it out with 1000 neurons."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data rescaling will be performed.\n",
      "No data rescaling will be performed.\n",
      "Checking the snapshots and your inputs for consistency.\n",
      "Checking descriptor file  Be_snapshot0.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot0.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking descriptor file  Be_snapshot1.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot1.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Consistency check successful.\n",
      "Initializing the data scalers.\n",
      "Input scaler parametrized.\n",
      "Output scaler parametrized.\n",
      "Data scalers initialized.\n",
      "Build datasets.\n",
      "Build dataset: Done.\n",
      "Initial Guess - validation data loss:  0.244789306640625\n",
      "Epoch 0: validation data loss: 5.834028124809265e-06, training data loss: 0.00029535348074776786\n",
      "Time for epoch[s]: 1.1085457801818848\n",
      "Epoch 1: validation data loss: 5.244482308626175e-06, training data loss: 2.741139116031783e-06\n",
      "Time for epoch[s]: 0.797311544418335\n",
      "Epoch 2: validation data loss: 4.929769784212113e-06, training data loss: 2.128387801349163e-06\n",
      "Time for epoch[s]: 0.7820658683776855\n",
      "Epoch 3: validation data loss: 4.762781517846244e-06, training data loss: 1.8161613760249955e-06\n",
      "Time for epoch[s]: 0.8111674785614014\n",
      "Epoch 4: validation data loss: 4.624924489430019e-06, training data loss: 1.6222886208977017e-06\n",
      "Time for epoch[s]: 0.7758948802947998\n",
      "Epoch 5: validation data loss: 4.525044134684971e-06, training data loss: 1.4753945704017366e-06\n",
      "Time for epoch[s]: 0.8326370716094971\n",
      "Epoch 6: validation data loss: 4.425929061004093e-06, training data loss: 1.3596758778606142e-06\n",
      "Time for epoch[s]: 0.8134944438934326\n",
      "Epoch 7: validation data loss: 4.361816282783236e-06, training data loss: 1.2635821476578712e-06\n",
      "Time for epoch[s]: 0.8544294834136963\n",
      "Epoch 8: validation data loss: 4.3037962168455125e-06, training data loss: 1.1865913069673946e-06\n",
      "Time for epoch[s]: 0.773249626159668\n",
      "Epoch 9: validation data loss: 4.2643618902989796e-06, training data loss: 1.125034343983446e-06\n",
      "Time for epoch[s]: 0.7942981719970703\n",
      "Epoch 10: validation data loss: 4.2058969182627546e-06, training data loss: 1.071995190743889e-06\n",
      "Time for epoch[s]: 0.8837954998016357\n",
      "Epoch 11: validation data loss: 4.193282819219998e-06, training data loss: 1.0275359132460184e-06\n",
      "Time for epoch[s]: 0.7747278213500977\n",
      "Epoch 12: validation data loss: 4.143711179494858e-06, training data loss: 9.87964349665812e-07\n",
      "Time for epoch[s]: 0.8345868587493896\n",
      "Epoch 13: validation data loss: 4.124534715499197e-06, training data loss: 9.520175600690501e-07\n",
      "Time for epoch[s]: 0.8770403861999512\n",
      "Epoch 14: validation data loss: 4.111648138080325e-06, training data loss: 9.204593620129994e-07\n",
      "Time for epoch[s]: 0.8011624813079834\n",
      "Epoch 15: validation data loss: 4.078522590654237e-06, training data loss: 8.922993604625974e-07\n",
      "Time for epoch[s]: 0.7775888442993164\n",
      "Epoch 16: validation data loss: 4.048402022038188e-06, training data loss: 8.674472836511475e-07\n",
      "Time for epoch[s]: 0.8402798175811768\n",
      "Epoch 17: validation data loss: 4.04401496052742e-06, training data loss: 8.44614247658423e-07\n",
      "Time for epoch[s]: 0.8939766883850098\n",
      "Epoch 18: validation data loss: 4.025137051939964e-06, training data loss: 8.236555648701531e-07\n",
      "Time for epoch[s]: 0.8361666202545166\n",
      "Epoch 19: validation data loss: 4.013977146574429e-06, training data loss: 8.03465023636818e-07\n",
      "Time for epoch[s]: 0.8099820613861084\n",
      "Epoch 20: validation data loss: 4.013364602412496e-06, training data loss: 7.841698825359344e-07\n",
      "Time for epoch[s]: 0.8112621307373047\n",
      "Epoch 21: validation data loss: 3.995969625455992e-06, training data loss: 7.659875388656344e-07\n",
      "Time for epoch[s]: 0.8608779907226562\n",
      "Epoch 22: validation data loss: 3.981759239520346e-06, training data loss: 7.481735332735947e-07\n",
      "Time for epoch[s]: 0.8576219081878662\n",
      "Epoch 23: validation data loss: 3.974564905677523e-06, training data loss: 7.307136963520731e-07\n",
      "Time for epoch[s]: 0.8678781986236572\n",
      "Epoch 24: validation data loss: 3.948054143360683e-06, training data loss: 7.137264391141279e-07\n",
      "Time for epoch[s]: 0.8173172473907471\n",
      "Epoch 25: validation data loss: 3.9370589490447725e-06, training data loss: 6.984172921095576e-07\n",
      "Time for epoch[s]: 0.8500890731811523\n",
      "Epoch 26: validation data loss: 3.934834950736591e-06, training data loss: 6.839311016457421e-07\n",
      "Time for epoch[s]: 0.816124677658081\n",
      "Epoch 27: validation data loss: 3.909551139388766e-06, training data loss: 6.700302474200725e-07\n",
      "Time for epoch[s]: 0.8470661640167236\n",
      "Epoch 28: validation data loss: 3.895905667117664e-06, training data loss: 6.546833818512303e-07\n",
      "Time for epoch[s]: 0.8547914028167725\n",
      "Epoch 29: validation data loss: 3.869765571185521e-06, training data loss: 6.408440614385265e-07\n",
      "Time for epoch[s]: 0.8209099769592285\n",
      "Epoch 30: validation data loss: 3.865611074226243e-06, training data loss: 6.294063558535917e-07\n",
      "Time for epoch[s]: 0.7826840877532959\n",
      "Epoch 31: validation data loss: 3.879226744174957e-06, training data loss: 6.193071603775024e-07\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.8448941707611084\n",
      "Epoch 32: validation data loss: 3.871617306556021e-06, training data loss: 6.091702463371413e-07\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.8826487064361572\n",
      "Epoch 33: validation data loss: 3.859372011252812e-06, training data loss: 5.991860026759761e-07\n",
      "Time for epoch[s]: 0.8082561492919922\n",
      "Epoch 34: validation data loss: 3.842448017426899e-06, training data loss: 5.898332622434412e-07\n",
      "Time for epoch[s]: 1.0332646369934082\n",
      "Epoch 35: validation data loss: 3.837319357054574e-06, training data loss: 5.810892741594996e-07\n",
      "Time for epoch[s]: 0.8398745059967041\n",
      "Epoch 36: validation data loss: 3.8042677832501274e-06, training data loss: 5.726385861635208e-07\n",
      "Time for epoch[s]: 0.9606943130493164\n",
      "Epoch 37: validation data loss: 3.808795873607908e-06, training data loss: 5.6473258882761e-07\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.8533716201782227\n",
      "Epoch 38: validation data loss: 3.8112801100526536e-06, training data loss: 5.570818404001849e-07\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.8318045139312744\n",
      "Epoch 39: validation data loss: 3.809166806084769e-06, training data loss: 5.496346857398748e-07\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.8120400905609131\n",
      "Epoch 40: validation data loss: 3.7976261228322983e-06, training data loss: 5.42526033573917e-07\n",
      "Time for epoch[s]: 0.8270988464355469\n",
      "Epoch 41: validation data loss: 3.804136599813189e-06, training data loss: 5.357798321970871e-07\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.9854562282562256\n",
      "Epoch 42: validation data loss: 3.7748076553855624e-06, training data loss: 5.289212401424136e-07\n",
      "Time for epoch[s]: 0.9825267791748047\n",
      "Epoch 43: validation data loss: 3.7694341902221954e-06, training data loss: 5.226787512323687e-07\n",
      "Time for epoch[s]: 0.8013603687286377\n",
      "Epoch 44: validation data loss: 3.7639361939259938e-06, training data loss: 5.164961330592632e-07\n",
      "Time for epoch[s]: 0.8041088581085205\n",
      "Epoch 45: validation data loss: 3.757831241403307e-06, training data loss: 5.103229611579861e-07\n",
      "Time for epoch[s]: 0.8282039165496826\n",
      "Epoch 46: validation data loss: 3.726411876933915e-06, training data loss: 5.04297370623265e-07\n",
      "Time for epoch[s]: 0.8713486194610596\n",
      "Epoch 47: validation data loss: 3.7583075463771822e-06, training data loss: 4.987207773540701e-07\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.9910926818847656\n",
      "Epoch 48: validation data loss: 3.730817564896175e-06, training data loss: 4.931946085499866e-07\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.8344118595123291\n",
      "Epoch 49: validation data loss: 3.7243666925600596e-06, training data loss: 4.879633696483715e-07\n",
      "Time for epoch[s]: 0.8983516693115234\n",
      "Epoch 50: validation data loss: 3.732029614703996e-06, training data loss: 4.828534687736204e-07\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.8570077419281006\n",
      "Epoch 51: validation data loss: 3.7347754197461264e-06, training data loss: 4.779226811868804e-07\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.8251509666442871\n",
      "Epoch 52: validation data loss: 3.7011091730424335e-06, training data loss: 4.7281194877411636e-07\n",
      "Time for epoch[s]: 0.8324196338653564\n",
      "Epoch 53: validation data loss: 3.738165700009891e-06, training data loss: 4.6758393623999185e-07\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.8212869167327881\n",
      "Epoch 54: validation data loss: 3.7457112755094254e-06, training data loss: 4.6261163827564035e-07\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.926537275314331\n",
      "Epoch 55: validation data loss: 3.7369890404599055e-06, training data loss: 4.580716070319925e-07\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.8338160514831543\n",
      "Epoch 56: validation data loss: 3.7175725613321577e-06, training data loss: 4.536323581955263e-07\n",
      "Validation accuracy has not improved enough.\n",
      "Stopping the training, validation accuracy has not improved for 4 epochs.\n",
      "Final validation data loss:  3.7175725613321577e-06\n",
      "Checking the snapshots and your inputs for consistency.\n",
      "Checking descriptor file  Be_snapshot2.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot2.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking descriptor file  Be_snapshot3.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot3.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "DataHandler prepared for inference. No training possible with this setup. If this is not what you wanted, please revise the input script. Validation snapshots you may have entered willbe ignored.\n",
      "Consistency check successful.\n",
      "Build datasets.\n",
      "Build dataset: Done.\n",
      "Had to readjust batch size from 10 to 12\n",
      "{'band_energy': [0.10128238388313093, 0.04330199676283186]}\n"
     ]
    }
   ],
   "source": [
    "parameters = mala.Parameters()\n",
    "parameters.verbosity = 2\n",
    "parameters.manual_seed = 2023\n",
    "parameters.network.layer_sizes = [1000]\n",
    "parameters.running.early_stopping_epochs = 4\n",
    "\n",
    "parameters, data_handler, network = training(parameters)\n",
    "print(testing(parameters, data_handler, network))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In my case, that helped a tiny bit with the accuracy of one snapshot. So we're getting there. Before we overthinking this specific aspect, let's introduce some more hyperparameters and test them out. We can toy with the length of the training, the learning rate (i.e. the aggressivness of the gradient updates of the neural networks, large learning rates mean larger updates per epoch, which can lead to oscillations, small learning rates can lead to stagnation) and the optimizer being used."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parameters = mala.Parameters()\n",
    "parameters.verbosity = 2\n",
    "parameters.manual_seed = 2023\n",
    "parameters.network.layer_sizes = [100]\n",
    "parameters.running.early_stopping_epochs = 4\n",
    "# Default value is 100 - try out longer trainings as well. Or shorter!\n",
    "parameters.running.max_number_epochs = 100\n",
    "# Default value is 0.5 - try out a few!\n",
    "parameters.running.learning_rate = 0.5\n",
    "# Default value is SGD - try out Adam instead!\n",
    "parameters.running.trainingtype = \"SGD\"\n",
    "\n",
    "parameters, data_handler, network = training(parameters)\n",
    "print(testing(parameters, data_handler, network))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You may beginning to feel a bit overwhelmed by all the choices one can make here. We'll address this very real struggle in a second, but we'll just add a bit more gasoline to the fire while were at it.\n",
    "\n",
    "There are two aspects of ML that can be very important (and that MALA supports) that we have not yet talked about.\n",
    "\n",
    "The first is data scaling. The idea behind data scaling is to make data more uniform - vectorial fields as in our case may contain larger and smaller components per data point and it can be hard for a network to correctly learn this. By normalizing or standardizing data, performance is often improved. MALA supports normalization feature-wise or per entire dataset. Empirically, we have made good experiences with the following. (Be aware: if you change the scaling, the absolute loss values will inevitably change - this is why we monitor the band energy.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the snapshots and your inputs for consistency.\n",
      "Checking descriptor file  Be_snapshot0.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot0.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking descriptor file  Be_snapshot1.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot1.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Consistency check successful.\n",
      "Initializing the data scalers.\n",
      "Input scaler parametrized.\n",
      "Output scaler parametrized.\n",
      "Data scalers initialized.\n",
      "Build datasets.\n",
      "Build dataset: Done.\n",
      "Initial Guess - validation data loss:  0.054730381556919644\n",
      "Epoch 0: validation data loss: 0.002600432804652623, training data loss: 0.0017240849903651645\n",
      "Time for epoch[s]: 0.48135828971862793\n",
      "Epoch 1: validation data loss: 0.002115297998700823, training data loss: 0.0007406753131321499\n",
      "Time for epoch[s]: 0.48793530464172363\n",
      "Epoch 2: validation data loss: 0.001884244918823242, training data loss: 0.0005785423687526158\n",
      "Time for epoch[s]: 0.45787525177001953\n",
      "Epoch 3: validation data loss: 0.0016306495666503905, training data loss: 0.0004649646622794015\n",
      "Time for epoch[s]: 0.4137861728668213\n",
      "Epoch 4: validation data loss: 0.0014923850468226841, training data loss: 0.0003904345716748919\n",
      "Time for epoch[s]: 0.40866875648498535\n",
      "Epoch 5: validation data loss: 0.001329531124659947, training data loss: 0.0003293121542249407\n",
      "Time for epoch[s]: 0.4427037239074707\n",
      "Epoch 6: validation data loss: 0.001223013060433524, training data loss: 0.0002836345263889858\n",
      "Time for epoch[s]: 0.4142918586730957\n",
      "Epoch 7: validation data loss: 0.0010912742614746095, training data loss: 0.00025134381226130895\n",
      "Time for epoch[s]: 0.44902920722961426\n",
      "Epoch 8: validation data loss: 0.0010236147471836634, training data loss: 0.00022441310541970389\n",
      "Time for epoch[s]: 0.46536707878112793\n",
      "Epoch 9: validation data loss: 0.0009318426677158901, training data loss: 0.00020276956898825508\n",
      "Time for epoch[s]: 0.42858195304870605\n",
      "Epoch 10: validation data loss: 0.0008938068662370954, training data loss: 0.00018409161908285957\n",
      "Time for epoch[s]: 0.4156308174133301\n",
      "Epoch 11: validation data loss: 0.0008327815192086356, training data loss: 0.0001698838642665318\n",
      "Time for epoch[s]: 0.4152047634124756\n",
      "Epoch 12: validation data loss: 0.0008069548606872559, training data loss: 0.00015681793008531843\n",
      "Time for epoch[s]: 0.40167856216430664\n",
      "Epoch 13: validation data loss: 0.000765747002192906, training data loss: 0.00014542531967163085\n",
      "Time for epoch[s]: 0.4172813892364502\n",
      "Epoch 14: validation data loss: 0.0007488994598388672, training data loss: 0.0001363550169127328\n",
      "Time for epoch[s]: 0.4175243377685547\n",
      "Epoch 15: validation data loss: 0.0007141206605093819, training data loss: 0.00012856424706322807\n",
      "Time for epoch[s]: 0.4933128356933594\n",
      "Epoch 16: validation data loss: 0.0007131549971444266, training data loss: 0.00012078116621289934\n",
      "Time for epoch[s]: 0.3999912738800049\n",
      "Epoch 17: validation data loss: 0.0006994085311889649, training data loss: 0.00011448664324624197\n",
      "Time for epoch[s]: 0.3988614082336426\n",
      "Epoch 18: validation data loss: 0.0006789231981549944, training data loss: 0.00010829601969037737\n",
      "Time for epoch[s]: 0.40033936500549316\n",
      "Epoch 19: validation data loss: 0.0006676930018833705, training data loss: 0.00010353785753250122\n",
      "Time for epoch[s]: 0.39569759368896484\n",
      "Epoch 20: validation data loss: 0.000667377199445452, training data loss: 9.87018346786499e-05\n",
      "Time for epoch[s]: 0.43834495544433594\n",
      "Epoch 21: validation data loss: 0.0006493257113865444, training data loss: 9.499633312225341e-05\n",
      "Time for epoch[s]: 0.4118785858154297\n",
      "Epoch 22: validation data loss: 0.0006327464921133859, training data loss: 9.001866408756801e-05\n",
      "Time for epoch[s]: 0.4030489921569824\n",
      "Epoch 23: validation data loss: 0.0006302296774727958, training data loss: 8.685459409441267e-05\n",
      "Time for epoch[s]: 0.40514349937438965\n",
      "Epoch 24: validation data loss: 0.0006333806855337961, training data loss: 8.35321205002921e-05\n",
      "Time for epoch[s]: 0.39614272117614746\n",
      "Epoch 25: validation data loss: 0.0006246117864336286, training data loss: 8.108027492250715e-05\n",
      "Time for epoch[s]: 0.4590122699737549\n",
      "Epoch 26: validation data loss: 0.0006284912654331752, training data loss: 7.772974457059588e-05\n",
      "Time for epoch[s]: 0.43070411682128906\n",
      "Epoch 27: validation data loss: 0.0006212819644383022, training data loss: 7.482621499470302e-05\n",
      "Time for epoch[s]: 0.4280390739440918\n",
      "Epoch 28: validation data loss: 0.000615316527230399, training data loss: 7.262876204081944e-05\n",
      "Time for epoch[s]: 0.43807387351989746\n",
      "Epoch 29: validation data loss: 0.0006032741410391671, training data loss: 7.00118328843798e-05\n",
      "Time for epoch[s]: 0.4823791980743408\n",
      "Epoch 30: validation data loss: 0.0006080447605678013, training data loss: 6.755230256489345e-05\n",
      "Time for epoch[s]: 0.39343833923339844\n",
      "Epoch 31: validation data loss: 0.000602400575365339, training data loss: 6.572359800338746e-05\n",
      "Time for epoch[s]: 0.45281529426574707\n",
      "Epoch 32: validation data loss: 0.0005884628977094378, training data loss: 6.276902556419373e-05\n",
      "Time for epoch[s]: 0.41665196418762207\n",
      "Epoch 33: validation data loss: 0.000590014934539795, training data loss: 6.147450208663941e-05\n",
      "Time for epoch[s]: 0.39993953704833984\n",
      "Epoch 34: validation data loss: 0.0005837083544049944, training data loss: 5.9207792793001445e-05\n",
      "Time for epoch[s]: 0.4116823673248291\n",
      "Epoch 35: validation data loss: 0.0005874847003391811, training data loss: 5.7681236948285786e-05\n",
      "Time for epoch[s]: 0.4149646759033203\n",
      "Epoch 36: validation data loss: 0.0005781801768711635, training data loss: 5.545513119016375e-05\n",
      "Time for epoch[s]: 0.4447340965270996\n",
      "Epoch 37: validation data loss: 0.0005786760193961007, training data loss: 5.4123695407594956e-05\n",
      "Time for epoch[s]: 0.4044356346130371\n",
      "Epoch 38: validation data loss: 0.0005807206290108817, training data loss: 5.256196430751256e-05\n",
      "Time for epoch[s]: 0.39835476875305176\n",
      "Epoch 39: validation data loss: 0.0005809912681579589, training data loss: 5.078781502587455e-05\n",
      "Time for epoch[s]: 0.4023168087005615\n",
      "Epoch 40: validation data loss: 0.0005674213341304234, training data loss: 4.953251566205706e-05\n",
      "Time for epoch[s]: 0.4045584201812744\n",
      "Epoch 41: validation data loss: 0.0005593626499176026, training data loss: 4.817848971911839e-05\n",
      "Time for epoch[s]: 0.40263962745666504\n",
      "Epoch 42: validation data loss: 0.0005668220520019531, training data loss: 4.673965062413897e-05\n",
      "Time for epoch[s]: 0.4377472400665283\n",
      "Epoch 43: validation data loss: 0.0005543017727988107, training data loss: 4.545625192778451e-05\n",
      "Time for epoch[s]: 0.4104299545288086\n",
      "Epoch 44: validation data loss: 0.0005595346518925258, training data loss: 4.4564170496804374e-05\n",
      "Time for epoch[s]: 0.40295910835266113\n",
      "Epoch 45: validation data loss: 0.0005571234226226806, training data loss: 4.2930462530681064e-05\n",
      "Time for epoch[s]: 0.41787004470825195\n",
      "Epoch 46: validation data loss: 0.0005624114104679652, training data loss: 4.2695931025913784e-05\n",
      "Time for epoch[s]: 0.40624046325683594\n",
      "Epoch 47: validation data loss: 0.0005470786094665527, training data loss: 4.153754455702646e-05\n",
      "Time for epoch[s]: 0.393169641494751\n",
      "Epoch 48: validation data loss: 0.0005466411794934954, training data loss: 4.049028669084821e-05\n",
      "Time for epoch[s]: 0.43039679527282715\n",
      "Epoch 49: validation data loss: 0.0005446892125265939, training data loss: 3.9641810314995905e-05\n",
      "Time for epoch[s]: 0.41271042823791504\n",
      "Epoch 50: validation data loss: 0.0005347145966121128, training data loss: 3.866049221583775e-05\n",
      "Time for epoch[s]: 0.40039992332458496\n",
      "Epoch 51: validation data loss: 0.0005413805076054164, training data loss: 3.771242925098964e-05\n",
      "Time for epoch[s]: 0.42459583282470703\n",
      "Epoch 52: validation data loss: 0.0005414377961839948, training data loss: 3.7013964993613107e-05\n",
      "Time for epoch[s]: 0.39498019218444824\n",
      "Epoch 53: validation data loss: 0.000545579058783395, training data loss: 3.612976840564183e-05\n",
      "Time for epoch[s]: 0.4250483512878418\n",
      "Epoch 54: validation data loss: 0.0005338455608912876, training data loss: 3.541549827371325e-05\n",
      "Time for epoch[s]: 0.4079291820526123\n",
      "Epoch 55: validation data loss: 0.0005342479092734201, training data loss: 3.44326879296984e-05\n",
      "Time for epoch[s]: 0.414029598236084\n",
      "Epoch 56: validation data loss: 0.0005250646727425712, training data loss: 3.4115974392209735e-05\n",
      "Time for epoch[s]: 0.4224400520324707\n",
      "Epoch 57: validation data loss: 0.0005219217027936663, training data loss: 3.3425056508609226e-05\n",
      "Time for epoch[s]: 0.39292430877685547\n",
      "Epoch 58: validation data loss: 0.0005215444564819336, training data loss: 3.302478151662009e-05\n",
      "Time for epoch[s]: 0.4071063995361328\n",
      "Epoch 59: validation data loss: 0.0005273356778281075, training data loss: 3.238218597003392e-05\n",
      "Time for epoch[s]: 0.39296436309814453\n",
      "Epoch 60: validation data loss: 0.0005186018603188651, training data loss: 3.178612674985613e-05\n",
      "Time for epoch[s]: 0.3817884922027588\n",
      "Epoch 61: validation data loss: 0.0005202345166887555, training data loss: 3.1297458069665094e-05\n",
      "Time for epoch[s]: 0.4288177490234375\n",
      "Epoch 62: validation data loss: 0.0005166962146759033, training data loss: 3.070584791047232e-05\n",
      "Time for epoch[s]: 0.4632086753845215\n",
      "Epoch 63: validation data loss: 0.0005153846740722656, training data loss: 3.012762750898089e-05\n",
      "Time for epoch[s]: 0.41257429122924805\n",
      "Epoch 64: validation data loss: 0.0005131649289812361, training data loss: 2.9732423169272287e-05\n",
      "Time for epoch[s]: 0.4086298942565918\n",
      "Epoch 65: validation data loss: 0.000506195068359375, training data loss: 2.9069014957972936e-05\n",
      "Time for epoch[s]: 0.42926955223083496\n",
      "Epoch 66: validation data loss: 0.0005189828191484723, training data loss: 2.8815695217677526e-05\n",
      "Time for epoch[s]: 0.40641069412231445\n",
      "Epoch 67: validation data loss: 0.0005031188556126186, training data loss: 2.809599893433707e-05\n",
      "Time for epoch[s]: 0.42398500442504883\n",
      "Epoch 68: validation data loss: 0.0005108623504638672, training data loss: 2.7967993702207293e-05\n",
      "Time for epoch[s]: 0.47764086723327637\n",
      "Epoch 69: validation data loss: 0.0005055053915296282, training data loss: 2.7454748749732973e-05\n",
      "Time for epoch[s]: 0.43854427337646484\n",
      "Epoch 70: validation data loss: 0.0005115983486175537, training data loss: 2.711637318134308e-05\n",
      "Time for epoch[s]: 0.48928380012512207\n",
      "Epoch 71: validation data loss: 0.0005096389225551061, training data loss: 2.6754494224275862e-05\n",
      "Time for epoch[s]: 0.4472017288208008\n",
      "Epoch 72: validation data loss: 0.0005093855857849121, training data loss: 2.6223557335989815e-05\n",
      "Time for epoch[s]: 0.4193546772003174\n",
      "Epoch 73: validation data loss: 0.0005052408490862165, training data loss: 2.5989830493927e-05\n",
      "Time for epoch[s]: 0.4227771759033203\n",
      "Epoch 74: validation data loss: 0.0005037470204489572, training data loss: 2.562944165297917e-05\n",
      "Time for epoch[s]: 0.4294397830963135\n",
      "Epoch 75: validation data loss: 0.00050250244140625, training data loss: 2.519410422870091e-05\n",
      "Time for epoch[s]: 0.48288893699645996\n",
      "Epoch 76: validation data loss: 0.0005033470221928188, training data loss: 2.4882659316062927e-05\n",
      "Time for epoch[s]: 0.5522851943969727\n",
      "Epoch 77: validation data loss: 0.0005009912763323102, training data loss: 2.4513363838195802e-05\n",
      "Time for epoch[s]: 0.8707096576690674\n",
      "Epoch 78: validation data loss: 0.0004957279137202672, training data loss: 2.430455812386104e-05\n",
      "Time for epoch[s]: 0.6641280651092529\n",
      "Epoch 79: validation data loss: 0.0004987611089433943, training data loss: 2.3945942521095274e-05\n",
      "Time for epoch[s]: 0.5275826454162598\n",
      "Epoch 80: validation data loss: 0.0004953630651746478, training data loss: 2.3637186203684125e-05\n",
      "Time for epoch[s]: 0.5195660591125488\n",
      "Epoch 81: validation data loss: 0.0004950695378439767, training data loss: 2.3381158709526063e-05\n",
      "Time for epoch[s]: 0.5988326072692871\n",
      "Epoch 82: validation data loss: 0.0004957619394574846, training data loss: 2.3090317845344544e-05\n",
      "Time for epoch[s]: 0.5291998386383057\n",
      "Epoch 83: validation data loss: 0.0004911063058035714, training data loss: 2.2719227841922216e-05\n",
      "Time for epoch[s]: 0.49251365661621094\n",
      "Epoch 84: validation data loss: 0.0004861319065093994, training data loss: 2.248134996209826e-05\n",
      "Time for epoch[s]: 0.5949254035949707\n",
      "Epoch 85: validation data loss: 0.0004929366111755371, training data loss: 2.2246831229754858e-05\n",
      "Time for epoch[s]: 0.5280051231384277\n",
      "Epoch 86: validation data loss: 0.0004929019042423793, training data loss: 2.1937653422355652e-05\n",
      "Time for epoch[s]: 0.462155818939209\n",
      "Epoch 87: validation data loss: 0.0004898176193237305, training data loss: 2.1616484437670027e-05\n",
      "Time for epoch[s]: 0.4728231430053711\n",
      "Epoch 88: validation data loss: 0.0004914012977055141, training data loss: 2.1547830530575342e-05\n",
      "Time for epoch[s]: 0.5588593482971191\n",
      "Epoch 89: validation data loss: 0.000490537234715053, training data loss: 2.13002690247127e-05\n",
      "Time for epoch[s]: 0.6260426044464111\n",
      "Epoch 90: validation data loss: 0.000491154534476144, training data loss: 2.102278811591012e-05\n",
      "Time for epoch[s]: 0.5726041793823242\n",
      "Epoch 91: validation data loss: 0.00047773408889770506, training data loss: 2.0896658301353456e-05\n",
      "Time for epoch[s]: 0.5163888931274414\n",
      "Epoch 92: validation data loss: 0.00048268815449305946, training data loss: 2.068880413259779e-05\n",
      "Time for epoch[s]: 0.5148086547851562\n",
      "Epoch 93: validation data loss: 0.0004881516184125628, training data loss: 2.0330284323011126e-05\n",
      "Time for epoch[s]: 0.49361085891723633\n",
      "Epoch 94: validation data loss: 0.0004814927237374442, training data loss: 2.0216831139155798e-05\n",
      "Time for epoch[s]: 0.4487917423248291\n",
      "Epoch 95: validation data loss: 0.00048323913982936315, training data loss: 1.993641257286072e-05\n",
      "Time for epoch[s]: 0.5320241451263428\n",
      "Epoch 96: validation data loss: 0.0004765101841517857, training data loss: 1.9764144505773273e-05\n",
      "Time for epoch[s]: 0.46277403831481934\n",
      "Epoch 97: validation data loss: 0.0004792407240186419, training data loss: 1.9568298544202532e-05\n",
      "Time for epoch[s]: 0.39625000953674316\n",
      "Epoch 98: validation data loss: 0.00048382721628461564, training data loss: 1.9390725663730076e-05\n",
      "Time for epoch[s]: 0.4296109676361084\n",
      "Epoch 99: validation data loss: 0.00048050570487976073, training data loss: 1.9126251339912414e-05\n",
      "Time for epoch[s]: 0.4068167209625244\n",
      "Final validation data loss:  0.00048050570487976073\n",
      "Checking the snapshots and your inputs for consistency.\n",
      "Checking descriptor file  Be_snapshot2.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot2.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking descriptor file  Be_snapshot3.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot3.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "DataHandler prepared for inference. No training possible with this setup. If this is not what you wanted, please revise the input script. Validation snapshots you may have entered willbe ignored.\n",
      "Consistency check successful.\n",
      "Build datasets.\n",
      "Build dataset: Done.\n",
      "Had to readjust batch size from 10 to 12\n",
      "{'band_energy': [0.007741880422457825, 0.019942810278099188]}\n"
     ]
    }
   ],
   "source": [
    "parameters = mala.Parameters()\n",
    "parameters.verbosity = 2\n",
    "parameters.manual_seed = 2023\n",
    "parameters.network.layer_sizes = [100]\n",
    "parameters.data.input_rescaling_type = \"feature-wise-standard\"\n",
    "parameters.data.output_rescaling_type = \"normal\"\n",
    "\n",
    "\n",
    "parameters, data_handler, network = training(parameters)\n",
    "print(testing(parameters, data_handler, network))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In my case, adding the data scaling improved performance drastically. Feel free to try out other forms of scaling (for no scaling, just specify `\"None\"`).\n",
    "\n",
    "Secondly, we can adapt the activation functions of the neural net, i.e., the linear transformation at the end of each layer. In MALA, this is realized by a list as well. We can specify individual layers, or simply give only one type to be used at each point."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the snapshots and your inputs for consistency.\n",
      "Checking descriptor file  Be_snapshot0.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot0.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking descriptor file  Be_snapshot1.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot1.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Consistency check successful.\n",
      "Initializing the data scalers.\n",
      "Input scaler parametrized.\n",
      "Output scaler parametrized.\n",
      "Data scalers initialized.\n",
      "Build datasets.\n",
      "Build dataset: Done.\n",
      "Initial Guess - validation data loss:  0.1100400652204241\n",
      "Epoch 0: validation data loss: 0.005833011082240514, training data loss: 0.0018355793271745955\n",
      "Time for epoch[s]: 0.4657137393951416\n",
      "Epoch 1: validation data loss: 0.005376566750662668, training data loss: 0.00023572427885872976\n",
      "Time for epoch[s]: 0.4514944553375244\n",
      "Epoch 2: validation data loss: 0.004776518140520368, training data loss: 0.000136334342615945\n",
      "Time for epoch[s]: 0.47327756881713867\n",
      "Epoch 3: validation data loss: 0.004433749880109515, training data loss: 0.00010212361812591552\n",
      "Time for epoch[s]: 0.48726916313171387\n",
      "Epoch 4: validation data loss: 0.004345704487391881, training data loss: 8.234414884022303e-05\n",
      "Time for epoch[s]: 0.4009108543395996\n",
      "Epoch 5: validation data loss: 0.004199553625924246, training data loss: 7.057221446718488e-05\n",
      "Time for epoch[s]: 0.39235520362854004\n",
      "Epoch 6: validation data loss: 0.004137655530657087, training data loss: 6.050078783716474e-05\n",
      "Time for epoch[s]: 0.4072573184967041\n",
      "Epoch 7: validation data loss: 0.0038486172812325615, training data loss: 5.4640139852251326e-05\n",
      "Time for epoch[s]: 0.41512322425842285\n",
      "Epoch 8: validation data loss: 0.0038214473724365234, training data loss: 4.9103992325919014e-05\n",
      "Time for epoch[s]: 0.3918154239654541\n",
      "Epoch 9: validation data loss: 0.0037963466644287108, training data loss: 4.451135226658412e-05\n",
      "Time for epoch[s]: 0.3876621723175049\n",
      "Epoch 10: validation data loss: 0.003604072570800781, training data loss: 4.074361068861825e-05\n",
      "Time for epoch[s]: 0.4146387577056885\n",
      "Epoch 11: validation data loss: 0.0034924561636788505, training data loss: 3.7991144827434e-05\n",
      "Time for epoch[s]: 0.41200685501098633\n",
      "Epoch 12: validation data loss: 0.003527862548828125, training data loss: 3.530997676508767e-05\n",
      "Time for epoch[s]: 0.4142136573791504\n",
      "Epoch 13: validation data loss: 0.003384603772844587, training data loss: 3.338563442230225e-05\n",
      "Time for epoch[s]: 0.46420907974243164\n",
      "Epoch 14: validation data loss: 0.003344726017543248, training data loss: 3.1996726989746096e-05\n",
      "Time for epoch[s]: 0.4794769287109375\n",
      "Epoch 15: validation data loss: 0.003364438465663365, training data loss: 2.990806528500148e-05\n",
      "Time for epoch[s]: 0.4105398654937744\n",
      "Epoch 16: validation data loss: 0.0031825719560895646, training data loss: 2.8103406940187726e-05\n",
      "Time for epoch[s]: 0.39943408966064453\n",
      "Epoch 17: validation data loss: 0.0032328248705182755, training data loss: 2.6495288525308882e-05\n",
      "Time for epoch[s]: 0.3908841609954834\n",
      "Epoch 18: validation data loss: 0.0032210576193673272, training data loss: 2.5668150612286158e-05\n",
      "Time for epoch[s]: 0.39919137954711914\n",
      "Epoch 19: validation data loss: 0.003159924370901925, training data loss: 2.4086564779281617e-05\n",
      "Time for epoch[s]: 0.4818265438079834\n",
      "Epoch 20: validation data loss: 0.0030978036608014787, training data loss: 2.341599123818534e-05\n",
      "Time for epoch[s]: 0.4600670337677002\n",
      "Epoch 21: validation data loss: 0.003045166015625, training data loss: 2.2212750145367213e-05\n",
      "Time for epoch[s]: 0.44528627395629883\n",
      "Epoch 22: validation data loss: 0.0030066452026367187, training data loss: 2.128727946962629e-05\n",
      "Time for epoch[s]: 0.42533230781555176\n",
      "Epoch 23: validation data loss: 0.003029665538242885, training data loss: 1.990381521838052e-05\n",
      "Time for epoch[s]: 0.4607231616973877\n",
      "Epoch 24: validation data loss: 0.0029104159218924387, training data loss: 1.961992042405265e-05\n",
      "Time for epoch[s]: 0.4286520481109619\n",
      "Epoch 25: validation data loss: 0.0029627685546875, training data loss: 1.9040678228650773e-05\n",
      "Time for epoch[s]: 0.41148900985717773\n",
      "Epoch 26: validation data loss: 0.002813303538731166, training data loss: 1.8437136496816362e-05\n",
      "Time for epoch[s]: 0.43645358085632324\n",
      "Epoch 27: validation data loss: 0.002902818134852818, training data loss: 1.795161621911185e-05\n",
      "Time for epoch[s]: 0.4361543655395508\n",
      "Epoch 28: validation data loss: 0.0028042567116873603, training data loss: 1.7495824822357724e-05\n",
      "Time for epoch[s]: 0.4093017578125\n",
      "Epoch 29: validation data loss: 0.0027938690185546873, training data loss: 1.6563719936779568e-05\n",
      "Time for epoch[s]: 0.4037511348724365\n",
      "Epoch 30: validation data loss: 0.0027851905822753906, training data loss: 1.5679066734654563e-05\n",
      "Time for epoch[s]: 0.4170539379119873\n",
      "Epoch 31: validation data loss: 0.0028246021270751953, training data loss: 1.5323619757379804e-05\n",
      "Time for epoch[s]: 0.4115312099456787\n",
      "Epoch 32: validation data loss: 0.002690527779715402, training data loss: 1.4979449766022818e-05\n",
      "Time for epoch[s]: 0.41831088066101074\n",
      "Epoch 33: validation data loss: 0.002711484364100865, training data loss: 1.4888060944420951e-05\n",
      "Time for epoch[s]: 0.41959214210510254\n",
      "Epoch 34: validation data loss: 0.002625955309186663, training data loss: 1.4088737113135201e-05\n",
      "Time for epoch[s]: 0.45502638816833496\n",
      "Epoch 35: validation data loss: 0.0026993756975446427, training data loss: 1.3837364103112902e-05\n",
      "Time for epoch[s]: 0.42018580436706543\n",
      "Epoch 36: validation data loss: 0.0025987047467912945, training data loss: 1.3872135962758745e-05\n",
      "Time for epoch[s]: 0.40758180618286133\n",
      "Epoch 37: validation data loss: 0.0026126172201974053, training data loss: 1.282433101109096e-05\n",
      "Time for epoch[s]: 0.43296241760253906\n",
      "Epoch 38: validation data loss: 0.0026369737897600446, training data loss: 1.3291973088468824e-05\n",
      "Time for epoch[s]: 0.4138023853302002\n",
      "Epoch 39: validation data loss: 0.002623125076293945, training data loss: 1.2908597077642168e-05\n",
      "Time for epoch[s]: 0.40144801139831543\n",
      "Epoch 40: validation data loss: 0.0025862671988351005, training data loss: 1.2332271252359663e-05\n",
      "Time for epoch[s]: 0.41210412979125977\n",
      "Epoch 41: validation data loss: 0.0025728705269949777, training data loss: 1.2384135808263507e-05\n",
      "Time for epoch[s]: 0.41452622413635254\n",
      "Epoch 42: validation data loss: 0.0025507425580705914, training data loss: 1.1896021664142608e-05\n",
      "Time for epoch[s]: 0.4043445587158203\n",
      "Epoch 43: validation data loss: 0.0025385556902204243, training data loss: 1.1638239026069641e-05\n",
      "Time for epoch[s]: 0.44728994369506836\n",
      "Epoch 44: validation data loss: 0.002495064871651786, training data loss: 1.0847645146506174e-05\n",
      "Time for epoch[s]: 0.4001610279083252\n",
      "Epoch 45: validation data loss: 0.0023940176282610214, training data loss: 1.1053303522723062e-05\n",
      "Time for epoch[s]: 0.3976900577545166\n",
      "Epoch 46: validation data loss: 0.002516383307320731, training data loss: 1.0702901652881078e-05\n",
      "Time for epoch[s]: 0.3958883285522461\n",
      "Epoch 47: validation data loss: 0.002521176201956613, training data loss: 1.069664316517966e-05\n",
      "Time for epoch[s]: 0.3953554630279541\n",
      "Epoch 48: validation data loss: 0.0024692467280796595, training data loss: 1.029118150472641e-05\n",
      "Time for epoch[s]: 0.388538122177124\n",
      "Epoch 49: validation data loss: 0.0025110645294189452, training data loss: 1.0681752647672381e-05\n",
      "Time for epoch[s]: 0.3918166160583496\n",
      "Epoch 50: validation data loss: 0.0023877620697021484, training data loss: 1.0153899235384805e-05\n",
      "Time for epoch[s]: 0.39596056938171387\n",
      "Epoch 51: validation data loss: 0.0024466446467808316, training data loss: 9.886738445077623e-06\n",
      "Time for epoch[s]: 0.40648460388183594\n",
      "Epoch 52: validation data loss: 0.002418077196393694, training data loss: 9.93635186127254e-06\n",
      "Time for epoch[s]: 0.4027981758117676\n",
      "Epoch 53: validation data loss: 0.0024005385807582312, training data loss: 9.70515183040074e-06\n",
      "Time for epoch[s]: 0.44172000885009766\n",
      "Epoch 54: validation data loss: 0.002433061327253069, training data loss: 9.86452294247491e-06\n",
      "Time for epoch[s]: 0.43831801414489746\n",
      "Epoch 55: validation data loss: 0.002402321951729911, training data loss: 9.123830923012325e-06\n",
      "Time for epoch[s]: 0.4766659736633301\n",
      "Epoch 56: validation data loss: 0.0023772166115897044, training data loss: 8.857053305421556e-06\n",
      "Time for epoch[s]: 0.4653170108795166\n",
      "Epoch 57: validation data loss: 0.002364264896937779, training data loss: 8.876792022160121e-06\n",
      "Time for epoch[s]: 0.48350071907043457\n",
      "Epoch 58: validation data loss: 0.0023037569863455635, training data loss: 8.48681213600295e-06\n",
      "Time for epoch[s]: 0.5122535228729248\n",
      "Epoch 59: validation data loss: 0.002329183850969587, training data loss: 8.730231119053704e-06\n",
      "Time for epoch[s]: 0.40257883071899414\n",
      "Epoch 60: validation data loss: 0.002357107707432338, training data loss: 8.458825094359261e-06\n",
      "Time for epoch[s]: 0.46357274055480957\n",
      "Epoch 61: validation data loss: 0.0023541927337646486, training data loss: 8.280670004231588e-06\n",
      "Time for epoch[s]: 0.4697871208190918\n",
      "Epoch 62: validation data loss: 0.002281731060573033, training data loss: 8.20723335657801e-06\n",
      "Time for epoch[s]: 0.5195093154907227\n",
      "Epoch 63: validation data loss: 0.0023074493408203126, training data loss: 7.9225982938494e-06\n",
      "Time for epoch[s]: 0.5537447929382324\n",
      "Epoch 64: validation data loss: 0.002280211584908622, training data loss: 8.038830544267382e-06\n",
      "Time for epoch[s]: 0.5171189308166504\n",
      "Epoch 65: validation data loss: 0.0022938782828194754, training data loss: 7.717728614807129e-06\n",
      "Time for epoch[s]: 0.4346959590911865\n",
      "Epoch 66: validation data loss: 0.0022992125919887, training data loss: 7.697110729558128e-06\n",
      "Time for epoch[s]: 0.42299914360046387\n",
      "Epoch 67: validation data loss: 0.002294536590576172, training data loss: 7.629804313182831e-06\n",
      "Time for epoch[s]: 0.4227297306060791\n",
      "Epoch 68: validation data loss: 0.0023029417310442245, training data loss: 7.4769705533981325e-06\n",
      "Time for epoch[s]: 0.4237697124481201\n",
      "Epoch 69: validation data loss: 0.0022542433057512554, training data loss: 7.322309272629874e-06\n",
      "Time for epoch[s]: 0.4078378677368164\n",
      "Epoch 70: validation data loss: 0.002208630153111049, training data loss: 7.146569234984262e-06\n",
      "Time for epoch[s]: 0.3967761993408203\n",
      "Epoch 71: validation data loss: 0.0022178340639386856, training data loss: 7.40970777613776e-06\n",
      "Time for epoch[s]: 0.4253885746002197\n",
      "Epoch 72: validation data loss: 0.002247774396623884, training data loss: 6.948984095028468e-06\n",
      "Time for epoch[s]: 0.39454102516174316\n",
      "Epoch 73: validation data loss: 0.002244729859488351, training data loss: 6.7867173680237365e-06\n",
      "Time for epoch[s]: 0.40417909622192383\n",
      "Epoch 74: validation data loss: 0.002222381319318499, training data loss: 6.7781502647059306e-06\n",
      "Time for epoch[s]: 0.3934633731842041\n",
      "Epoch 75: validation data loss: 0.0021965227127075195, training data loss: 6.767006857054574e-06\n",
      "Time for epoch[s]: 0.40454936027526855\n",
      "Epoch 76: validation data loss: 0.00218835939679827, training data loss: 6.662487983703614e-06\n",
      "Time for epoch[s]: 0.4176449775695801\n",
      "Epoch 77: validation data loss: 0.002200015068054199, training data loss: 6.7827808005469185e-06\n",
      "Time for epoch[s]: 0.4229273796081543\n",
      "Epoch 78: validation data loss: 0.0021700425829206194, training data loss: 6.582636386156082e-06\n",
      "Time for epoch[s]: 0.4367680549621582\n",
      "Epoch 79: validation data loss: 0.0021405745915004186, training data loss: 6.204424692051752e-06\n",
      "Time for epoch[s]: 0.4229395389556885\n",
      "Epoch 80: validation data loss: 0.00219405460357666, training data loss: 6.389953728233065e-06\n",
      "Time for epoch[s]: 0.4695601463317871\n",
      "Epoch 81: validation data loss: 0.002186969212123326, training data loss: 6.633773446083069e-06\n",
      "Time for epoch[s]: 0.4239017963409424\n",
      "Epoch 82: validation data loss: 0.002148679869515555, training data loss: 6.268545453037534e-06\n",
      "Time for epoch[s]: 0.41341352462768555\n",
      "Epoch 83: validation data loss: 0.0021630584171840124, training data loss: 6.086444216115134e-06\n",
      "Time for epoch[s]: 0.39042019844055176\n",
      "Epoch 84: validation data loss: 0.0021416803087506977, training data loss: 5.86393049785069e-06\n",
      "Time for epoch[s]: 0.4125545024871826\n",
      "Epoch 85: validation data loss: 0.0021167898178100584, training data loss: 5.731753472770963e-06\n",
      "Time for epoch[s]: 0.39409899711608887\n",
      "Epoch 86: validation data loss: 0.002136255809238979, training data loss: 5.755219076360975e-06\n",
      "Time for epoch[s]: 0.4122598171234131\n",
      "Epoch 87: validation data loss: 0.0021182872227260043, training data loss: 5.7464167475700375e-06\n",
      "Time for epoch[s]: 0.3991880416870117\n",
      "Epoch 88: validation data loss: 0.0021277119772774833, training data loss: 5.716156214475632e-06\n",
      "Time for epoch[s]: 0.39869093894958496\n",
      "Epoch 89: validation data loss: 0.002137368202209473, training data loss: 5.601302321468081e-06\n",
      "Time for epoch[s]: 0.4546189308166504\n",
      "Epoch 90: validation data loss: 0.0021274285997663223, training data loss: 5.6093468197754455e-06\n",
      "Time for epoch[s]: 0.42525219917297363\n",
      "Epoch 91: validation data loss: 0.0021103692735944474, training data loss: 5.693997655596052e-06\n",
      "Time for epoch[s]: 0.45383715629577637\n",
      "Epoch 92: validation data loss: 0.002102403232029506, training data loss: 5.5932109909398215e-06\n",
      "Time for epoch[s]: 0.4276244640350342\n",
      "Epoch 93: validation data loss: 0.0021010849816458566, training data loss: 5.302683583327702e-06\n",
      "Time for epoch[s]: 0.40276503562927246\n",
      "Epoch 94: validation data loss: 0.002100345747811454, training data loss: 5.4391297910894665e-06\n",
      "Time for epoch[s]: 0.4040851593017578\n",
      "Epoch 95: validation data loss: 0.002098571913582938, training data loss: 5.386707506009511e-06\n",
      "Time for epoch[s]: 0.4004859924316406\n",
      "Epoch 96: validation data loss: 0.0021090251377650668, training data loss: 5.266247583287103e-06\n",
      "Time for epoch[s]: 0.39354395866394043\n",
      "Epoch 97: validation data loss: 0.0020634384155273438, training data loss: 5.17364644578525e-06\n",
      "Time for epoch[s]: 0.3999757766723633\n",
      "Epoch 98: validation data loss: 0.0020992469787597654, training data loss: 5.0521535532815115e-06\n",
      "Time for epoch[s]: 0.401090145111084\n",
      "Epoch 99: validation data loss: 0.002057680947440011, training data loss: 5.338533648422786e-06\n",
      "Time for epoch[s]: 0.40978550910949707\n",
      "Final validation data loss:  0.002057680947440011\n",
      "Checking the snapshots and your inputs for consistency.\n",
      "Checking descriptor file  Be_snapshot2.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot2.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking descriptor file  Be_snapshot3.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot3.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "DataHandler prepared for inference. No training possible with this setup. If this is not what you wanted, please revise the input script. Validation snapshots you may have entered willbe ignored.\n",
      "Consistency check successful.\n",
      "Build datasets.\n",
      "Build dataset: Done.\n",
      "Had to readjust batch size from 10 to 12\n",
      "{'band_energy': [0.21546747334905803, 0.09541568381050425]}\n"
     ]
    }
   ],
   "source": [
    "parameters = mala.Parameters()\n",
    "parameters.verbosity = 2\n",
    "parameters.manual_seed = 2023\n",
    "parameters.network.layer_sizes = [100]\n",
    "parameters.data.input_rescaling_type = \"feature-wise-standard\"\n",
    "parameters.data.output_rescaling_type = \"normal\"\n",
    "\n",
    "# Sigmoid is default, try out ReLU or LeakyReLU!\n",
    "parameters.network.layer_activations = [\"LeakyReLU\"]\n",
    "\n",
    "\n",
    "parameters, data_handler, network = training(parameters)\n",
    "print(testing(parameters, data_handler, network))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you will have noticed, the result seem to vary quite a bit.\n",
    "\n",
    "So what do we make of all of this?\n",
    "We can build different models, and they give different accuracy. But that's also dependent on the data scaling to be used. And that is also dependent on the way we train them. What now?\n",
    "\n",
    "Well, welcome to hell. This complexity is what makes (deep) ML hard, just as the obvious speed-up makes it worthwhile.\n",
    "\n",
    "There are two important concepts at play here:\n",
    "\n",
    "1. Intuition and experience - understanding your data. Your data has inherent structure that can be exploited in several ways. As a rule of thumb activation functions may be chosen in a way that reflects the data itself, i.e. if we have a lot of smooth curves, Sigmoid functions may work well (they do here). LeakyReLU on the other hand is a standard choice for more complicated data. Model size should reflect the complexity in your data set, and a simple data set does not warrant a huge model. We have seen this just now. Training routines should be optimized for the model.\n",
    "2. Good, old fashioned non-linear optimization. Essentially we have a N-dimensional hypersphere with some distinct or float valued choices and can simply test out all the combinations.\n",
    "\n",
    "The best possible route is a combination of the two. By adding as much prior experience from 1. into 2. we can perform a limited optimization and still end up with the best possible model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tuning the Hyperparameters (hands-on)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "MALA provides a convenient way to tune the hyperparameters of models. There are multiple algorithms implemented, but we will focus on the `optuna` library, to which MALA provides an interface. The idea is easy: we give MALA a set of hyperparameters to optimize, fix the others, give it some data and let optuna do its work. Optuna internally uses elaborate algorithms to determine optimal hyperparameters from observed data.\n",
    "\n",
    "First, let us think about which hyperparameters to fix. We want to fix as many as possible and reasonable to get the best trade-off between optimal results and minimal computational effort.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "parameters = mala.Parameters()\n",
    "parameters.verbosity = 2\n",
    "parameters.manual_seed = 2023\n",
    "parameters.data.input_rescaling_type = \"feature-wise-standard\"\n",
    "parameters.data.output_rescaling_type = \"normal\"\n",
    "parameters.running.max_number_epochs = 100\n",
    "parameters.running.mini_batch_size = 40\n",
    "parameters.running.trainingtype = \"SGD\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can specify data to be used.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the snapshots and your inputs for consistency.\n",
      "Checking descriptor file  Be_snapshot0.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot0.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking descriptor file  Be_snapshot1.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot1.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Consistency check successful.\n",
      "Initializing the data scalers.\n",
      "Input scaler parametrized.\n",
      "Output scaler parametrized.\n",
      "Data scalers initialized.\n",
      "Build datasets.\n",
      "Build dataset: Done.\n"
     ]
    }
   ],
   "source": [
    "data_handler = mala.DataHandler(parameters)\n",
    "data_handler.clear_data()\n",
    "\n",
    "data_handler.add_snapshot(\"Be_snapshot0.in.npy\", data_path,\n",
    "                          \"Be_snapshot0.out.npy\", data_path, \"tr\")\n",
    "data_handler.add_snapshot(\"Be_snapshot1.in.npy\", data_path,\n",
    "                          \"Be_snapshot1.out.npy\", data_path, \"va\")\n",
    "\n",
    "# This already loads data into RAM!\n",
    "data_handler.prepare_data()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we instantiate the hyperparameter optimizer, two important parameters to set are\n",
    "\n",
    "1. The number of trials (test networks) to train\n",
    "2. How often to train a test network - if we train each proposed network a number of times and evaluate the average performant, we can discard unrobust outliers. In the interest of time, let's keep it at 1 here."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fiedlerl/.local/lib/python3.10/site-packages/optuna/samplers/_tpe/sampler.py:263: ExperimentalWarning: ``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2023-03-10 18:10:47,053]\u001B[0m A new study created in memory with name: no-name-bfbd7129-dd58-4c02-8bf5-2147967f53f5\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "parameters.hyperparameters.n_trials = 20\n",
    "parameters.hyperparameters.number_training_per_trial = 1\n",
    "\n",
    "hyperoptimizer = mala.HyperOptOptuna(parameters, data_handler)\n",
    "\n",
    "hyperoptimizer.add_hyperparameter(\"categorical\", \"learning_rate\",\n",
    "                                     choices=[0.1, 0.2, 0.5])\n",
    "hyperoptimizer.add_hyperparameter(\"categorical\", \"ff_neurons_layer_00\", choices=[10, 100, 200])\n",
    "hyperoptimizer.add_hyperparameter(\"categorical\", \"ff_neurons_layer_01\", choices=[10, 100, 200])\n",
    "hyperoptimizer.add_hyperparameter(\"categorical\", \"early_stopping_epochs\", choices=[4, 8])\n",
    "\n",
    "# Choices for activation function at each layer will be optimized.\n",
    "hyperoptimizer.add_hyperparameter(\"categorical\", \"layer_activation_00\",\n",
    "                                  choices=[\"ReLU\", \"Sigmoid\", \"LeakyReLU\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The syntax is a bit clunky at times, since we are trying to cramp a complicated system into a few function calls. Now let's run. This may take a while. Grab a coffee and sit back :)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Guess - validation data loss:  0.13116421982577947\n",
      "Epoch 0: validation data loss: 0.0031034954606670223, training data loss: 0.008716743286341836\n",
      "Time for epoch[s]: 0.22608017921447754\n",
      "Epoch 1: validation data loss: 0.0025456060557604923, training data loss: 0.0005864466978534716\n",
      "Time for epoch[s]: 0.24631476402282715\n",
      "Epoch 2: validation data loss: 0.0023794136090910054, training data loss: 0.0003730091670332434\n",
      "Time for epoch[s]: 0.2503647804260254\n",
      "Epoch 3: validation data loss: 0.0021896199004290853, training data loss: 0.00028405842035328415\n",
      "Time for epoch[s]: 0.2285161018371582\n",
      "Epoch 4: validation data loss: 0.0021643593703230767, training data loss: 0.0002323468492183511\n",
      "Time for epoch[s]: 0.27465343475341797\n",
      "Epoch 5: validation data loss: 0.0020938209474903264, training data loss: 0.00019799883915409106\n",
      "Time for epoch[s]: 0.25708627700805664\n",
      "Epoch 6: validation data loss: 0.0021044136998860257, training data loss: 0.00017276930210252876\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2615230083465576\n",
      "Epoch 7: validation data loss: 0.0020575311085949204, training data loss: 0.0001533102907546579\n",
      "Time for epoch[s]: 0.22417879104614258\n",
      "Epoch 8: validation data loss: 0.002037269067546548, training data loss: 0.00013851007812371537\n",
      "Time for epoch[s]: 0.2288815975189209\n",
      "Epoch 9: validation data loss: 0.0020569415941630326, training data loss: 0.00012681835658474055\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2748751640319824\n",
      "Epoch 10: validation data loss: 0.002081947783901267, training data loss: 0.00011735446921222286\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.24234366416931152\n",
      "Epoch 11: validation data loss: 0.0020356189170384516, training data loss: 0.00010851739170072286\n",
      "Time for epoch[s]: 0.21208906173706055\n",
      "Epoch 12: validation data loss: 0.0020662640055564984, training data loss: 0.00010159840492625214\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.22633719444274902\n",
      "Epoch 13: validation data loss: 0.0020529750275285275, training data loss: 9.592618265924933e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.20830249786376953\n",
      "Epoch 14: validation data loss: 0.0020433389432898395, training data loss: 9.064494576900517e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2826120853424072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-10 18:10:53,364]\u001B[0m Trial 0 finished with value: 0.0020488141334220156 and parameters: {'learning_rate': 0.2, 'ff_neurons_layer_00': 200, 'ff_neurons_layer_01': 100, 'early_stopping_epochs': 4, 'layer_activation_00': 'ReLU'}. Best is trial 0 with value: 0.0020488141334220156.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: validation data loss: 0.0020488141334220156, training data loss: 8.603363174553876e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Stopping the training, validation accuracy has not improved for 4 epochs.\n",
      "Final validation data loss:  0.0020488141334220156\n",
      "Initial Guess - validation data loss:  0.06764695851225831\n",
      "Epoch 0: validation data loss: 0.018154446937177823, training data loss: 0.03201883350877457\n",
      "Time for epoch[s]: 0.24033522605895996\n",
      "Epoch 1: validation data loss: 0.00959580674019034, training data loss: 0.009023511246459125\n",
      "Time for epoch[s]: 0.20544791221618652\n",
      "Epoch 2: validation data loss: 0.007900271241523359, training data loss: 0.004891595884000874\n",
      "Time for epoch[s]: 0.2844085693359375\n",
      "Epoch 3: validation data loss: 0.007445743639175206, training data loss: 0.004006068183951182\n",
      "Time for epoch[s]: 0.18819618225097656\n",
      "Epoch 4: validation data loss: 0.007277229605200084, training data loss: 0.0037422215557533856\n",
      "Time for epoch[s]: 0.2650492191314697\n",
      "Epoch 5: validation data loss: 0.007193303543683057, training data loss: 0.0036367082704692128\n",
      "Time for epoch[s]: 0.2824435234069824\n",
      "Epoch 6: validation data loss: 0.007140714283947531, training data loss: 0.0035832385494284436\n",
      "Time for epoch[s]: 0.18979263305664062\n",
      "Epoch 7: validation data loss: 0.0071003404382157, training data loss: 0.0035486014466307478\n",
      "Time for epoch[s]: 0.18895888328552246\n",
      "Epoch 8: validation data loss: 0.007065751781202343, training data loss: 0.0035230843988183426\n",
      "Time for epoch[s]: 0.20392537117004395\n",
      "Epoch 9: validation data loss: 0.007033767765515471, training data loss: 0.003496989539769142\n",
      "Time for epoch[s]: 0.24008393287658691\n",
      "Epoch 10: validation data loss: 0.007002812542327463, training data loss: 0.003477328988515079\n",
      "Time for epoch[s]: 0.31118035316467285\n",
      "Epoch 11: validation data loss: 0.0069716782330378, training data loss: 0.0034548398022237977\n",
      "Time for epoch[s]: 0.21950125694274902\n",
      "Epoch 12: validation data loss: 0.006939253306279988, training data loss: 0.003441198775757393\n",
      "Time for epoch[s]: 0.228623628616333\n",
      "Epoch 13: validation data loss: 0.006906626975699647, training data loss: 0.00341400762671205\n",
      "Time for epoch[s]: 0.2046947479248047\n",
      "Epoch 14: validation data loss: 0.006872167870334294, training data loss: 0.0033925583917800693\n",
      "Time for epoch[s]: 0.16601181030273438\n",
      "Epoch 15: validation data loss: 0.006836669630111624, training data loss: 0.0033720086698662746\n",
      "Time for epoch[s]: 0.1688826084136963\n",
      "Epoch 16: validation data loss: 0.006800310796798636, training data loss: 0.0033438412566163225\n",
      "Time for epoch[s]: 0.2174057960510254\n",
      "Epoch 17: validation data loss: 0.006761115435595926, training data loss: 0.0033184434180934677\n",
      "Time for epoch[s]: 0.17653632164001465\n",
      "Epoch 18: validation data loss: 0.006721571155879051, training data loss: 0.0032943394630466968\n",
      "Time for epoch[s]: 0.21900391578674316\n",
      "Epoch 19: validation data loss: 0.006679680249462389, training data loss: 0.0032704135054322683\n",
      "Time for epoch[s]: 0.22697687149047852\n",
      "Epoch 20: validation data loss: 0.006636493282231022, training data loss: 0.0032411130595969284\n",
      "Time for epoch[s]: 0.2346351146697998\n",
      "Epoch 21: validation data loss: 0.006591149116759975, training data loss: 0.0032148810282145463\n",
      "Time for epoch[s]: 0.2162485122680664\n",
      "Epoch 22: validation data loss: 0.006543132812465163, training data loss: 0.003184035488459618\n",
      "Time for epoch[s]: 0.24934601783752441\n",
      "Epoch 23: validation data loss: 0.00649350527759012, training data loss: 0.0031540886452208917\n",
      "Time for epoch[s]: 0.2386023998260498\n",
      "Epoch 24: validation data loss: 0.0064419284803137935, training data loss: 0.0031220278783475972\n",
      "Time for epoch[s]: 0.2557859420776367\n",
      "Epoch 25: validation data loss: 0.0063883550635211546, training data loss: 0.00308879733629967\n",
      "Time for epoch[s]: 0.22733783721923828\n",
      "Epoch 26: validation data loss: 0.0063326658179226536, training data loss: 0.0030561456941578485\n",
      "Time for epoch[s]: 0.17688322067260742\n",
      "Epoch 27: validation data loss: 0.006274739901224772, training data loss: 0.0030222978766106033\n",
      "Time for epoch[s]: 0.17224478721618652\n",
      "Epoch 28: validation data loss: 0.0062149452836546175, training data loss: 0.002990730004767849\n",
      "Time for epoch[s]: 0.15549302101135254\n",
      "Epoch 29: validation data loss: 0.006152777366986557, training data loss: 0.00294991007678585\n",
      "Time for epoch[s]: 0.1911332607269287\n",
      "Epoch 30: validation data loss: 0.0060885262815919645, training data loss: 0.002912422021230062\n",
      "Time for epoch[s]: 0.16623806953430176\n",
      "Epoch 31: validation data loss: 0.006022442965747015, training data loss: 0.0028728763806765483\n",
      "Time for epoch[s]: 0.1879267692565918\n",
      "Epoch 32: validation data loss: 0.005955028751669409, training data loss: 0.002833974688020471\n",
      "Time for epoch[s]: 0.17568206787109375\n",
      "Epoch 33: validation data loss: 0.005885507962475084, training data loss: 0.0027936411230531457\n",
      "Time for epoch[s]: 0.18087387084960938\n",
      "Epoch 34: validation data loss: 0.005814205021618708, training data loss: 0.002752698175439007\n",
      "Time for epoch[s]: 0.2677793502807617\n",
      "Epoch 35: validation data loss: 0.005741676783452839, training data loss: 0.0027121678334937247\n",
      "Time for epoch[s]: 0.17793059349060059\n",
      "Epoch 36: validation data loss: 0.0056677125904658066, training data loss: 0.002669048635926965\n",
      "Time for epoch[s]: 0.27455830574035645\n",
      "Epoch 37: validation data loss: 0.005593612858149559, training data loss: 0.0026267209009492778\n",
      "Time for epoch[s]: 0.4082522392272949\n",
      "Epoch 38: validation data loss: 0.005518313956587282, training data loss: 0.002586382981304709\n",
      "Time for epoch[s]: 0.41364455223083496\n",
      "Epoch 39: validation data loss: 0.005442576321292685, training data loss: 0.0025460755444008465\n",
      "Time for epoch[s]: 0.3282284736633301\n",
      "Epoch 40: validation data loss: 0.005366088592842834, training data loss: 0.002498993046207515\n",
      "Time for epoch[s]: 0.26078367233276367\n",
      "Epoch 41: validation data loss: 0.0052891811823736045, training data loss: 0.0024568329663037167\n",
      "Time for epoch[s]: 0.27890563011169434\n",
      "Epoch 42: validation data loss: 0.005213459877118672, training data loss: 0.002414265179742961\n",
      "Time for epoch[s]: 0.26175665855407715\n",
      "Epoch 43: validation data loss: 0.005137458783850822, training data loss: 0.0023715991407768913\n",
      "Time for epoch[s]: 0.2560300827026367\n",
      "Epoch 44: validation data loss: 0.005061110405072774, training data loss: 0.002331655047255564\n",
      "Time for epoch[s]: 0.2556724548339844\n",
      "Epoch 45: validation data loss: 0.004986657399565117, training data loss: 0.0022902423388337437\n",
      "Time for epoch[s]: 0.30144619941711426\n",
      "Epoch 46: validation data loss: 0.004912516297815053, training data loss: 0.0022471835624137426\n",
      "Time for epoch[s]: 0.3198845386505127\n",
      "Epoch 47: validation data loss: 0.004838410577817595, training data loss: 0.002208124965293222\n",
      "Time for epoch[s]: 0.3249940872192383\n",
      "Epoch 48: validation data loss: 0.00476644953636274, training data loss: 0.0021682035977437617\n",
      "Time for epoch[s]: 0.3841209411621094\n",
      "Epoch 49: validation data loss: 0.004695481905654141, training data loss: 0.002130613343356407\n",
      "Time for epoch[s]: 0.3548297882080078\n",
      "Epoch 50: validation data loss: 0.0046251010677041526, training data loss: 0.0020915234469931964\n",
      "Time for epoch[s]: 0.32616424560546875\n",
      "Epoch 51: validation data loss: 0.004557477829118842, training data loss: 0.0020539914364139785\n",
      "Time for epoch[s]: 0.27132654190063477\n",
      "Epoch 52: validation data loss: 0.004491719208895888, training data loss: 0.0020189271669953926\n",
      "Time for epoch[s]: 0.24294042587280273\n",
      "Epoch 53: validation data loss: 0.004426627942960556, training data loss: 0.001981837700491082\n",
      "Time for epoch[s]: 0.28447437286376953\n",
      "Epoch 54: validation data loss: 0.0043640980437465995, training data loss: 0.0019468720917288026\n",
      "Time for epoch[s]: 0.3099069595336914\n",
      "Epoch 55: validation data loss: 0.004302953476230848, training data loss: 0.001913702787329617\n",
      "Time for epoch[s]: 0.32663416862487793\n",
      "Epoch 56: validation data loss: 0.004242750335501754, training data loss: 0.0018826731263774715\n",
      "Time for epoch[s]: 0.3750166893005371\n",
      "Epoch 57: validation data loss: 0.00418451632538887, training data loss: 0.0018485290546939798\n",
      "Time for epoch[s]: 0.4304533004760742\n",
      "Epoch 58: validation data loss: 0.004130212955823227, training data loss: 0.0018176154731071158\n",
      "Time for epoch[s]: 0.35292887687683105\n",
      "Epoch 59: validation data loss: 0.00407496434912834, training data loss: 0.0017876710793743396\n",
      "Time for epoch[s]: 0.43149447441101074\n",
      "Epoch 60: validation data loss: 0.004022502463702197, training data loss: 0.0017580622679566685\n",
      "Time for epoch[s]: 0.4228503704071045\n",
      "Epoch 61: validation data loss: 0.003971949015578178, training data loss: 0.0017293914812340583\n",
      "Time for epoch[s]: 0.3558356761932373\n",
      "Epoch 62: validation data loss: 0.003924073421791808, training data loss: 0.0017058130812971559\n",
      "Time for epoch[s]: 0.30441927909851074\n",
      "Epoch 63: validation data loss: 0.003876628396717925, training data loss: 0.0016747915853648424\n",
      "Time for epoch[s]: 0.29692673683166504\n",
      "Epoch 64: validation data loss: 0.003832553619663465, training data loss: 0.0016485858181295875\n",
      "Time for epoch[s]: 0.5081655979156494\n",
      "Epoch 65: validation data loss: 0.003787875447643402, training data loss: 0.0016243367978971298\n",
      "Time for epoch[s]: 0.380279541015625\n",
      "Epoch 66: validation data loss: 0.0037480432148937763, training data loss: 0.0016000266760995943\n",
      "Time for epoch[s]: 0.34398603439331055\n",
      "Epoch 67: validation data loss: 0.003705742968816191, training data loss: 0.0015773403045793648\n",
      "Time for epoch[s]: 0.27089571952819824\n",
      "Epoch 68: validation data loss: 0.0036683430954745915, training data loss: 0.0015545396227814835\n",
      "Time for epoch[s]: 0.22818684577941895\n",
      "Epoch 69: validation data loss: 0.0036297549395800724, training data loss: 0.0015315111369302828\n",
      "Time for epoch[s]: 0.23711800575256348\n",
      "Epoch 70: validation data loss: 0.0035927902617955317, training data loss: 0.0015091879727089242\n",
      "Time for epoch[s]: 0.23073482513427734\n",
      "Epoch 71: validation data loss: 0.003558790302712079, training data loss: 0.0014888242771636405\n",
      "Time for epoch[s]: 0.2595093250274658\n",
      "Epoch 72: validation data loss: 0.0035257905585580765, training data loss: 0.0014697121430749763\n",
      "Time for epoch[s]: 0.22089123725891113\n",
      "Epoch 73: validation data loss: 0.0034932605752117556, training data loss: 0.0014486348247963543\n",
      "Time for epoch[s]: 0.2573816776275635\n",
      "Epoch 74: validation data loss: 0.0034638055383342586, training data loss: 0.001430273464281265\n",
      "Time for epoch[s]: 0.228485107421875\n",
      "Epoch 75: validation data loss: 0.0034361397294693343, training data loss: 0.0014116274167413582\n",
      "Time for epoch[s]: 0.24357390403747559\n",
      "Epoch 76: validation data loss: 0.0034054871563497743, training data loss: 0.0013940437743652902\n",
      "Time for epoch[s]: 0.24128174781799316\n",
      "Epoch 77: validation data loss: 0.00337857089630545, training data loss: 0.0013766471109433805\n",
      "Time for epoch[s]: 0.23425698280334473\n",
      "Epoch 78: validation data loss: 0.0033516559970977644, training data loss: 0.001360128595404429\n",
      "Time for epoch[s]: 0.2540152072906494\n",
      "Epoch 79: validation data loss: 0.003327084458581933, training data loss: 0.0013440188751917452\n",
      "Time for epoch[s]: 0.23920178413391113\n",
      "Epoch 80: validation data loss: 0.0033007615233120852, training data loss: 0.0013285175850402274\n",
      "Time for epoch[s]: 0.2361907958984375\n",
      "Epoch 81: validation data loss: 0.0032773053265053387, training data loss: 0.0013132157935399442\n",
      "Time for epoch[s]: 0.22972393035888672\n",
      "Epoch 82: validation data loss: 0.003255711026387672, training data loss: 0.0012976436037995499\n",
      "Time for epoch[s]: 0.24798870086669922\n",
      "Epoch 83: validation data loss: 0.0032335010837746536, training data loss: 0.001288002076214307\n",
      "Time for epoch[s]: 0.2620103359222412\n",
      "Epoch 84: validation data loss: 0.003210508660094379, training data loss: 0.0012696641220894035\n",
      "Time for epoch[s]: 0.3893918991088867\n",
      "Epoch 85: validation data loss: 0.0031911028574590815, training data loss: 0.0012619500835192258\n",
      "Time for epoch[s]: 0.43012499809265137\n",
      "Epoch 86: validation data loss: 0.0031697883453543326, training data loss: 0.0012434224287668865\n",
      "Time for epoch[s]: 0.33072328567504883\n",
      "Epoch 87: validation data loss: 0.0031492971394160023, training data loss: 0.001230488901268946\n",
      "Time for epoch[s]: 0.34246277809143066\n",
      "Epoch 88: validation data loss: 0.003130416619723246, training data loss: 0.0012176051803919822\n",
      "Time for epoch[s]: 0.3835866451263428\n",
      "Epoch 89: validation data loss: 0.003112889588151348, training data loss: 0.0012055962053063797\n",
      "Time for epoch[s]: 0.3680565357208252\n",
      "Epoch 90: validation data loss: 0.0030939266017582863, training data loss: 0.001194071960231485\n",
      "Time for epoch[s]: 0.3804190158843994\n",
      "Epoch 91: validation data loss: 0.003077777281199416, training data loss: 0.0011828032802773394\n",
      "Time for epoch[s]: 0.3334987163543701\n",
      "Epoch 92: validation data loss: 0.0030606206693605744, training data loss: 0.0011714887401284692\n",
      "Time for epoch[s]: 0.302325963973999\n",
      "Epoch 93: validation data loss: 0.0030433021179617267, training data loss: 0.0011605561868240845\n",
      "Time for epoch[s]: 0.2821063995361328\n",
      "Epoch 94: validation data loss: 0.003029237871300684, training data loss: 0.0011502554699710514\n",
      "Time for epoch[s]: 0.256314754486084\n",
      "Epoch 95: validation data loss: 0.003012523259202095, training data loss: 0.0011396719712644951\n",
      "Time for epoch[s]: 0.2664005756378174\n",
      "Epoch 96: validation data loss: 0.0029985820321731917, training data loss: 0.0011292952516851905\n",
      "Time for epoch[s]: 0.25822949409484863\n",
      "Epoch 97: validation data loss: 0.0029816344448420555, training data loss: 0.0011274549242568342\n",
      "Time for epoch[s]: 0.28342175483703613\n",
      "Epoch 98: validation data loss: 0.002968095753290882, training data loss: 0.00111012841196365\n",
      "Time for epoch[s]: 0.27575254440307617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-10 18:11:20,808]\u001B[0m Trial 1 finished with value: 0.002954056001689336 and parameters: {'learning_rate': 0.1, 'ff_neurons_layer_00': 100, 'ff_neurons_layer_01': 10, 'early_stopping_epochs': 4, 'layer_activation_00': 'Sigmoid'}. Best is trial 0 with value: 0.0020488141334220156.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: validation data loss: 0.002954056001689336, training data loss: 0.001100732243224366\n",
      "Time for epoch[s]: 0.25937390327453613\n",
      "Final validation data loss:  0.002954056001689336\n",
      "Initial Guess - validation data loss:  0.13168105904914473\n",
      "Epoch 0: validation data loss: 0.003099715600819348, training data loss: 0.00836452555983034\n",
      "Time for epoch[s]: 0.4618358612060547\n",
      "Epoch 1: validation data loss: 0.002546436982612087, training data loss: 0.0005816882603789029\n",
      "Time for epoch[s]: 0.6318860054016113\n",
      "Epoch 2: validation data loss: 0.0023768721105845553, training data loss: 0.0003706208306904797\n",
      "Time for epoch[s]: 0.3504481315612793\n",
      "Epoch 3: validation data loss: 0.0021895511781788307, training data loss: 0.00028284489428071673\n",
      "Time for epoch[s]: 0.4906129837036133\n",
      "Epoch 4: validation data loss: 0.002161558360269625, training data loss: 0.00023146063837830878\n",
      "Time for epoch[s]: 0.5771760940551758\n",
      "Epoch 5: validation data loss: 0.0020894713053420254, training data loss: 0.000197194482637867\n",
      "Time for epoch[s]: 0.4703850746154785\n",
      "Epoch 6: validation data loss: 0.0021021479068825777, training data loss: 0.0001720103124777476\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4272744655609131\n",
      "Epoch 7: validation data loss: 0.002053712873154035, training data loss: 0.00015265523502815804\n",
      "Time for epoch[s]: 0.43025970458984375\n",
      "Epoch 8: validation data loss: 0.002034600466898043, training data loss: 0.00013792213715919076\n",
      "Time for epoch[s]: 0.36193037033081055\n",
      "Epoch 9: validation data loss: 0.0020566205727999614, training data loss: 0.00012626834043629094\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3885629177093506\n",
      "Epoch 10: validation data loss: 0.0020808887808290246, training data loss: 0.00011681674311966657\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3640618324279785\n",
      "Epoch 11: validation data loss: 0.002037320779338819, training data loss: 0.00010792664401063091\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.34644341468811035\n",
      "Epoch 12: validation data loss: 0.002067057373316865, training data loss: 0.00010102339190979526\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.29023289680480957\n",
      "Epoch 13: validation data loss: 0.0020553451967021646, training data loss: 9.537249225187519e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.29263949394226074\n",
      "Epoch 14: validation data loss: 0.002047660688287047, training data loss: 9.010546759927654e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.24772953987121582\n",
      "Epoch 15: validation data loss: 0.0020494456977060396, training data loss: 8.555869125340083e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.25988006591796875\n",
      "Epoch 16: validation data loss: 0.0020338189384164333, training data loss: 8.13069613012549e-05\n",
      "Time for epoch[s]: 0.2606680393218994\n",
      "Epoch 17: validation data loss: 0.00203055592432414, training data loss: 7.762742062954054e-05\n",
      "Time for epoch[s]: 0.2684025764465332\n",
      "Epoch 18: validation data loss: 0.0020588469015408867, training data loss: 7.408596235051002e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.259293794631958\n",
      "Epoch 19: validation data loss: 0.0020583429837335737, training data loss: 7.126737624134648e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.270855188369751\n",
      "Epoch 20: validation data loss: 0.0020343702133387735, training data loss: 6.848772707050794e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2642703056335449\n",
      "Epoch 21: validation data loss: 0.0020403886494571215, training data loss: 6.600615215492031e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3205258846282959\n",
      "Epoch 22: validation data loss: 0.0020057606642649053, training data loss: 6.395955591305206e-05\n",
      "Time for epoch[s]: 0.4559206962585449\n",
      "Epoch 23: validation data loss: 0.0019917331601931078, training data loss: 6.152293113268674e-05\n",
      "Time for epoch[s]: 0.4024038314819336\n",
      "Epoch 24: validation data loss: 0.002001547241864139, training data loss: 6.000463293703724e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.354949951171875\n",
      "Epoch 25: validation data loss: 0.002035518623378179, training data loss: 5.809403105413533e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3029325008392334\n",
      "Epoch 26: validation data loss: 0.00200639821623014, training data loss: 5.639767163692544e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2785313129425049\n",
      "Epoch 27: validation data loss: 0.001999645880912537, training data loss: 5.493012839528524e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2586078643798828\n",
      "Epoch 28: validation data loss: 0.001987141425206781, training data loss: 5.3165608072934084e-05\n",
      "Time for epoch[s]: 0.2601284980773926\n",
      "Epoch 29: validation data loss: 0.0020238171973729242, training data loss: 5.171982545966971e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.25740551948547363\n",
      "Epoch 30: validation data loss: 0.0019935858031930446, training data loss: 5.066095421847688e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2860145568847656\n",
      "Epoch 31: validation data loss: 0.0019864502562779813, training data loss: 4.9526871237308466e-05\n",
      "Time for epoch[s]: 0.23679018020629883\n",
      "Epoch 32: validation data loss: 0.0019998011523730133, training data loss: 4.811766536132386e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.26331138610839844\n",
      "Epoch 33: validation data loss: 0.0020057878809976795, training data loss: 4.720754206997074e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.23916101455688477\n",
      "Epoch 34: validation data loss: 0.00199430881569919, training data loss: 4.629274940790107e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.26038312911987305\n",
      "Epoch 35: validation data loss: 0.0019898191434607657, training data loss: 4.50898718820315e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.25861310958862305\n",
      "Epoch 36: validation data loss: 0.00197956755281039, training data loss: 4.432958520982908e-05\n",
      "Time for epoch[s]: 0.2761192321777344\n",
      "Epoch 37: validation data loss: 0.001963493078266649, training data loss: 4.337363149205299e-05\n",
      "Time for epoch[s]: 0.26644229888916016\n",
      "Epoch 38: validation data loss: 0.001963542884887626, training data loss: 4.23052343943892e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2638580799102783\n",
      "Epoch 39: validation data loss: 0.001943906692609395, training data loss: 4.152698042594135e-05\n",
      "Time for epoch[s]: 0.24733328819274902\n",
      "Epoch 40: validation data loss: 0.002007439256258751, training data loss: 4.0777980192611206e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2589445114135742\n",
      "Epoch 41: validation data loss: 0.0019603306300019566, training data loss: 3.9902753855811954e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4684436321258545\n",
      "Epoch 42: validation data loss: 0.001943547703904104, training data loss: 3.9341834003794684e-05\n",
      "Time for epoch[s]: 0.4288763999938965\n",
      "Epoch 43: validation data loss: 0.0019493255441047285, training data loss: 3.85015939264537e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4488499164581299\n",
      "Epoch 44: validation data loss: 0.001969440750879784, training data loss: 3.7989196405835346e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.39610934257507324\n",
      "Epoch 45: validation data loss: 0.0019823322285255885, training data loss: 3.7369131191408256e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.33326005935668945\n",
      "Epoch 46: validation data loss: 0.001955951185531268, training data loss: 3.6781891908275486e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.29973506927490234\n",
      "Epoch 47: validation data loss: 0.00195774109396216, training data loss: 3.616522453283066e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.37400150299072266\n",
      "Epoch 48: validation data loss: 0.001936141486581602, training data loss: 3.557973520056298e-05\n",
      "Time for epoch[s]: 0.39015817642211914\n",
      "Epoch 49: validation data loss: 0.0019141812574917868, training data loss: 3.509965967642118e-05\n",
      "Time for epoch[s]: 0.2639181613922119\n",
      "Epoch 50: validation data loss: 0.0019263554381453284, training data loss: 3.45354334166333e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.321444034576416\n",
      "Epoch 51: validation data loss: 0.0019278342593206119, training data loss: 3.4090522761758605e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3733501434326172\n",
      "Epoch 52: validation data loss: 0.0019166220540869725, training data loss: 3.3451465370992544e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.5149002075195312\n",
      "Epoch 53: validation data loss: 0.0019201689387020999, training data loss: 3.314234069697389e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3943469524383545\n",
      "Epoch 54: validation data loss: 0.0019588343901176977, training data loss: 3.263853133133013e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.353085994720459\n",
      "Epoch 55: validation data loss: 0.0019394869673742007, training data loss: 3.226836888145094e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.42116260528564453\n",
      "Epoch 56: validation data loss: 0.001911586550272763, training data loss: 3.19191122844339e-05\n",
      "Time for epoch[s]: 0.39618945121765137\n",
      "Epoch 57: validation data loss: 0.0019029397942704153, training data loss: 3.137295538363936e-05\n",
      "Time for epoch[s]: 0.3887951374053955\n",
      "Epoch 58: validation data loss: 0.0019185058602459355, training data loss: 3.101973747621932e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3987159729003906\n",
      "Epoch 59: validation data loss: 0.001922521144832106, training data loss: 3.0580586987679405e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.450824499130249\n",
      "Epoch 60: validation data loss: 0.0019124069986822397, training data loss: 3.033101380075494e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4297060966491699\n",
      "Epoch 61: validation data loss: 0.001901301619124739, training data loss: 2.988405338569319e-05\n",
      "Time for epoch[s]: 0.33572840690612793\n",
      "Epoch 62: validation data loss: 0.0019035834700005239, training data loss: 2.9602328306872006e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3527095317840576\n",
      "Epoch 63: validation data loss: 0.0019043166887814597, training data loss: 2.922953986754156e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.36081814765930176\n",
      "Epoch 64: validation data loss: 0.001903819166906348, training data loss: 2.892061719271146e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.319333553314209\n",
      "Epoch 65: validation data loss: 0.0019006971354898253, training data loss: 2.8629727729651482e-05\n",
      "Time for epoch[s]: 0.382535457611084\n",
      "Epoch 66: validation data loss: 0.001908606862368649, training data loss: 2.8362682676070358e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4068586826324463\n",
      "Epoch 67: validation data loss: 0.0018927562454519751, training data loss: 2.7971625004865262e-05\n",
      "Time for epoch[s]: 0.4281766414642334\n",
      "Epoch 68: validation data loss: 0.0019071257277710797, training data loss: 2.758197920167283e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.43364930152893066\n",
      "Epoch 69: validation data loss: 0.0018926153988598688, training data loss: 2.7470975331792004e-05\n",
      "Time for epoch[s]: 0.3951444625854492\n",
      "Epoch 70: validation data loss: 0.0018950522490288025, training data loss: 2.70897730946813e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.46298980712890625\n",
      "Epoch 71: validation data loss: 0.0019152175345921625, training data loss: 2.6770408124956366e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4533689022064209\n",
      "Epoch 72: validation data loss: 0.0018922998208433526, training data loss: 2.6521185778727816e-05\n",
      "Time for epoch[s]: 0.3937382698059082\n",
      "Epoch 73: validation data loss: 0.0018973336916535957, training data loss: 2.632174879040348e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3743577003479004\n",
      "Epoch 74: validation data loss: 0.0018847233628573484, training data loss: 2.6030338382067744e-05\n",
      "Time for epoch[s]: 0.44017767906188965\n",
      "Epoch 75: validation data loss: 0.0018947261925701682, training data loss: 2.5810110361472658e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.469388484954834\n",
      "Epoch 76: validation data loss: 0.0018893578281141308, training data loss: 2.5551389800767375e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4429001808166504\n",
      "Epoch 77: validation data loss: 0.0018939420784989448, training data loss: 2.531895465026163e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.39850878715515137\n",
      "Epoch 78: validation data loss: 0.0018921913621632476, training data loss: 2.5072943027977528e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3745558261871338\n",
      "Epoch 79: validation data loss: 0.0018906474929966338, training data loss: 2.482810937990881e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.38628530502319336\n",
      "Epoch 80: validation data loss: 0.001887822532218341, training data loss: 2.464895523642296e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4096181392669678\n",
      "Epoch 81: validation data loss: 0.0018902573411323164, training data loss: 2.442450649594063e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4840049743652344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-10 18:11:51,198]\u001B[0m Trial 2 finished with value: 0.001890825218261649 and parameters: {'learning_rate': 0.2, 'ff_neurons_layer_00': 200, 'ff_neurons_layer_01': 100, 'early_stopping_epochs': 8, 'layer_activation_00': 'LeakyReLU'}. Best is trial 2 with value: 0.001890825218261649.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82: validation data loss: 0.001890825218261649, training data loss: 2.424158666231861e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Stopping the training, validation accuracy has not improved for 8 epochs.\n",
      "Final validation data loss:  0.001890825218261649\n",
      "Initial Guess - validation data loss:  0.1280695684424274\n",
      "Epoch 0: validation data loss: 0.026301059548713302, training data loss: 0.058791334770585846\n",
      "Time for epoch[s]: 0.25991082191467285\n",
      "Epoch 1: validation data loss: 0.019513974995373592, training data loss: 0.017112139697488586\n",
      "Time for epoch[s]: 0.2546558380126953\n",
      "Epoch 2: validation data loss: 0.007752214936905256, training data loss: 0.013614544585415217\n",
      "Time for epoch[s]: 0.2639751434326172\n",
      "Epoch 3: validation data loss: 0.004707333159773317, training data loss: 0.0022906606600164824\n",
      "Time for epoch[s]: 0.26372623443603516\n",
      "Epoch 4: validation data loss: 0.00394625043215817, training data loss: 0.0012456377347310383\n",
      "Time for epoch[s]: 0.26315808296203613\n",
      "Epoch 5: validation data loss: 0.0034004180398705887, training data loss: 0.0009438537027193531\n",
      "Time for epoch[s]: 0.2790520191192627\n",
      "Epoch 6: validation data loss: 0.0030304087351446284, training data loss: 0.0007554049769492999\n",
      "Time for epoch[s]: 0.26871800422668457\n",
      "Epoch 7: validation data loss: 0.002806793609166254, training data loss: 0.0006076572420390229\n",
      "Time for epoch[s]: 0.2683866024017334\n",
      "Epoch 8: validation data loss: 0.002616472440223171, training data loss: 0.0004984356769143719\n",
      "Time for epoch[s]: 0.3283529281616211\n",
      "Epoch 9: validation data loss: 0.00243860564819754, training data loss: 0.0004168272902976432\n",
      "Time for epoch[s]: 0.35213303565979004\n",
      "Epoch 10: validation data loss: 0.002274291700424125, training data loss: 0.0003560352815340643\n",
      "Time for epoch[s]: 0.26888084411621094\n",
      "Epoch 11: validation data loss: 0.0021630273834211096, training data loss: 0.00031237096683075436\n",
      "Time for epoch[s]: 0.2565133571624756\n",
      "Epoch 12: validation data loss: 0.0020947024974648813, training data loss: 0.0002799715047285437\n",
      "Time for epoch[s]: 0.24356937408447266\n",
      "Epoch 13: validation data loss: 0.0019746194147083856, training data loss: 0.0002558973349937021\n",
      "Time for epoch[s]: 0.199232816696167\n",
      "Epoch 14: validation data loss: 0.0019323457049452552, training data loss: 0.0002367326556003257\n",
      "Time for epoch[s]: 0.16867661476135254\n",
      "Epoch 15: validation data loss: 0.001855532736538752, training data loss: 0.00022200899853553946\n",
      "Time for epoch[s]: 0.1670372486114502\n",
      "Epoch 16: validation data loss: 0.0018002513336808715, training data loss: 0.00020871467922376172\n",
      "Time for epoch[s]: 0.15700721740722656\n",
      "Epoch 17: validation data loss: 0.0017652896713448443, training data loss: 0.0001981688416711816\n",
      "Time for epoch[s]: 0.15042543411254883\n",
      "Epoch 18: validation data loss: 0.0017439067091571687, training data loss: 0.00018930190230069096\n",
      "Time for epoch[s]: 0.20448637008666992\n",
      "Epoch 19: validation data loss: 0.0016775442857176201, training data loss: 0.00018111786477641972\n",
      "Time for epoch[s]: 0.18226146697998047\n",
      "Epoch 20: validation data loss: 0.0016865451314133596, training data loss: 0.00017293494994237543\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.22060513496398926\n",
      "Epoch 21: validation data loss: 0.0016400042462022338, training data loss: 0.00016582337076261164\n",
      "Time for epoch[s]: 0.19972825050354004\n",
      "Epoch 22: validation data loss: 0.0016224881013234456, training data loss: 0.00015982837505536536\n",
      "Time for epoch[s]: 0.20273733139038086\n",
      "Epoch 23: validation data loss: 0.0015908496020591422, training data loss: 0.00015340110958983366\n",
      "Time for epoch[s]: 0.2184457778930664\n",
      "Epoch 24: validation data loss: 0.001549896992504869, training data loss: 0.00014762064977867962\n",
      "Time for epoch[s]: 0.21686220169067383\n",
      "Epoch 25: validation data loss: 0.0015567444503035174, training data loss: 0.00014181826348718443\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.20738625526428223\n",
      "Epoch 26: validation data loss: 0.0015347797304527944, training data loss: 0.00013700760390660533\n",
      "Time for epoch[s]: 0.19040179252624512\n",
      "Epoch 27: validation data loss: 0.00150409218383162, training data loss: 0.00013237366597402043\n",
      "Time for epoch[s]: 0.208665132522583\n",
      "Epoch 28: validation data loss: 0.00148442999957359, training data loss: 0.00012909455148324574\n",
      "Time for epoch[s]: 0.19068503379821777\n",
      "Epoch 29: validation data loss: 0.0014765020374838075, training data loss: 0.000125125850036264\n",
      "Time for epoch[s]: 0.1906416416168213\n",
      "Epoch 30: validation data loss: 0.0014543289735436983, training data loss: 0.00012116432700255145\n",
      "Time for epoch[s]: 0.21063685417175293\n",
      "Epoch 31: validation data loss: 0.0014294851316164617, training data loss: 0.00011827920873959859\n",
      "Time for epoch[s]: 0.18358898162841797\n",
      "Epoch 32: validation data loss: 0.0014148302818542203, training data loss: 0.00011524721367718423\n",
      "Time for epoch[s]: 0.16366219520568848\n",
      "Epoch 33: validation data loss: 0.0014109038599005573, training data loss: 0.00011250494073515069\n",
      "Time for epoch[s]: 0.16304993629455566\n",
      "Epoch 34: validation data loss: 0.0014060210419572106, training data loss: 0.00010961470232434469\n",
      "Time for epoch[s]: 0.24166011810302734\n",
      "Epoch 35: validation data loss: 0.001377213082901419, training data loss: 0.0001073996855379784\n",
      "Time for epoch[s]: 0.28598904609680176\n",
      "Epoch 36: validation data loss: 0.0013803121161787477, training data loss: 0.00010499803000661336\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2487494945526123\n",
      "Epoch 37: validation data loss: 0.0013538348620340704, training data loss: 0.00010281568180480503\n",
      "Time for epoch[s]: 0.23874211311340332\n",
      "Epoch 38: validation data loss: 0.0013488751568206368, training data loss: 0.00010051696505992924\n",
      "Time for epoch[s]: 0.26047205924987793\n",
      "Epoch 39: validation data loss: 0.0013412524303889166, training data loss: 9.855326077981627e-05\n",
      "Time for epoch[s]: 0.20540738105773926\n",
      "Epoch 40: validation data loss: 0.0013389090696970622, training data loss: 9.664106416647838e-05\n",
      "Time for epoch[s]: 0.19971609115600586\n",
      "Epoch 41: validation data loss: 0.001319207012925518, training data loss: 9.477300118637956e-05\n",
      "Time for epoch[s]: 0.17998051643371582\n",
      "Epoch 42: validation data loss: 0.0013137463837453764, training data loss: 9.27602132297542e-05\n",
      "Time for epoch[s]: 0.17464184761047363\n",
      "Epoch 43: validation data loss: 0.0013075974433933762, training data loss: 9.08185374927303e-05\n",
      "Time for epoch[s]: 0.16200566291809082\n",
      "Epoch 44: validation data loss: 0.0013059449794629936, training data loss: 8.931258463696258e-05\n",
      "Time for epoch[s]: 0.1478736400604248\n",
      "Epoch 45: validation data loss: 0.0013068367357123387, training data loss: 8.745189569039977e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.19367051124572754\n",
      "Epoch 46: validation data loss: 0.001296615899969998, training data loss: 8.627738310321826e-05\n",
      "Time for epoch[s]: 0.15776371955871582\n",
      "Epoch 47: validation data loss: 0.0012933304320731664, training data loss: 8.530389160326082e-05\n",
      "Time for epoch[s]: 0.17877745628356934\n",
      "Epoch 48: validation data loss: 0.0012809134781632794, training data loss: 8.287894024968692e-05\n",
      "Time for epoch[s]: 0.18834519386291504\n",
      "Epoch 49: validation data loss: 0.0012664579909686085, training data loss: 8.228843921123575e-05\n",
      "Time for epoch[s]: 0.2283954620361328\n",
      "Epoch 50: validation data loss: 0.001281207554960904, training data loss: 8.063727658088894e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.20948028564453125\n",
      "Epoch 51: validation data loss: 0.0012793742358412372, training data loss: 7.933049067242504e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.1980907917022705\n",
      "Epoch 52: validation data loss: 0.0012620397626537166, training data loss: 7.754215570889651e-05\n",
      "Time for epoch[s]: 0.19108200073242188\n",
      "Epoch 53: validation data loss: 0.0012557299714110214, training data loss: 7.647049685591433e-05\n",
      "Time for epoch[s]: 0.16638469696044922\n",
      "Epoch 54: validation data loss: 0.0012533495959625941, training data loss: 7.552807495746439e-05\n",
      "Time for epoch[s]: 0.16201519966125488\n",
      "Epoch 55: validation data loss: 0.0012469810165771065, training data loss: 7.356968644547136e-05\n",
      "Time for epoch[s]: 0.15007567405700684\n",
      "Epoch 56: validation data loss: 0.0012425123012229189, training data loss: 7.289779887079648e-05\n",
      "Time for epoch[s]: 0.18074798583984375\n",
      "Epoch 57: validation data loss: 0.0012386706593918474, training data loss: 7.163356462297919e-05\n",
      "Time for epoch[s]: 0.1808629035949707\n",
      "Epoch 58: validation data loss: 0.0012330303997753961, training data loss: 7.039397852878048e-05\n",
      "Time for epoch[s]: 0.20517349243164062\n",
      "Epoch 59: validation data loss: 0.0012297552742370186, training data loss: 6.976119799701047e-05\n",
      "Time for epoch[s]: 0.18550682067871094\n",
      "Epoch 60: validation data loss: 0.0012332218694904623, training data loss: 6.865748293595771e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.18785977363586426\n",
      "Epoch 61: validation data loss: 0.0012360153949424013, training data loss: 6.754254097263562e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.18613481521606445\n",
      "Epoch 62: validation data loss: 0.0012217621280722422, training data loss: 6.665972371896108e-05\n",
      "Time for epoch[s]: 0.1828913688659668\n",
      "Epoch 63: validation data loss: 0.0012299538203026062, training data loss: 6.599610747948085e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.17338132858276367\n",
      "Epoch 64: validation data loss: 0.0012254069929253566, training data loss: 6.488074450732366e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.20531582832336426\n",
      "Epoch 65: validation data loss: 0.0012102699987420209, training data loss: 6.394818016927536e-05\n",
      "Time for epoch[s]: 0.2430565357208252\n",
      "Epoch 66: validation data loss: 0.0012136458261916626, training data loss: 6.353235952386028e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.20795798301696777\n",
      "Epoch 67: validation data loss: 0.0012115551728636162, training data loss: 6.249942072586381e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2053239345550537\n",
      "Epoch 68: validation data loss: 0.0012136825687809078, training data loss: 6.194550918253589e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.24980878829956055\n",
      "Epoch 69: validation data loss: 0.0011957841648903068, training data loss: 6.106658307112516e-05\n",
      "Time for epoch[s]: 0.29007387161254883\n",
      "Epoch 70: validation data loss: 0.0012090134021898384, training data loss: 6.0644723713125815e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3005070686340332\n",
      "Epoch 71: validation data loss: 0.0012049423233014807, training data loss: 5.963574414519959e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.27150750160217285\n",
      "Epoch 72: validation data loss: 0.0011860333621229755, training data loss: 5.8877315865531903e-05\n",
      "Time for epoch[s]: 0.22287511825561523\n",
      "Epoch 73: validation data loss: 0.0012034822816718114, training data loss: 5.8022268184515984e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.241180419921875\n",
      "Epoch 74: validation data loss: 0.0011911071054467328, training data loss: 5.743738059719948e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.18838882446289062\n",
      "Epoch 75: validation data loss: 0.0011791185429107108, training data loss: 5.695558914310856e-05\n",
      "Time for epoch[s]: 0.19750285148620605\n",
      "Epoch 76: validation data loss: 0.0011804415482908623, training data loss: 5.622351006285785e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.17388248443603516\n",
      "Epoch 77: validation data loss: 0.0011842248102301332, training data loss: 5.5597346599243545e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.22038507461547852\n",
      "Epoch 78: validation data loss: 0.0011865778328621224, training data loss: 5.503138314643407e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.17583250999450684\n",
      "Epoch 79: validation data loss: 0.0011764564742780712, training data loss: 5.434154103335725e-05\n",
      "Time for epoch[s]: 0.16079354286193848\n",
      "Epoch 80: validation data loss: 0.0011713802814483643, training data loss: 5.39378020657252e-05\n",
      "Time for epoch[s]: 0.1511695384979248\n",
      "Epoch 81: validation data loss: 0.0011635419984930727, training data loss: 5.338780718034805e-05\n",
      "Time for epoch[s]: 0.2036128044128418\n",
      "Epoch 82: validation data loss: 0.001191474531339184, training data loss: 5.256402046849194e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.1746530532836914\n",
      "Epoch 83: validation data loss: 0.00116573076814277, training data loss: 5.2282152927085144e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.23760271072387695\n",
      "Epoch 84: validation data loss: 0.0011610961668023236, training data loss: 5.163982952838619e-05\n",
      "Time for epoch[s]: 0.33533215522766113\n",
      "Epoch 85: validation data loss: 0.001160159775111229, training data loss: 5.095884986391895e-05\n",
      "Time for epoch[s]: 0.22124147415161133\n",
      "Epoch 86: validation data loss: 0.0011565541023533094, training data loss: 5.0639244621474994e-05\n",
      "Time for epoch[s]: 0.19461846351623535\n",
      "Epoch 87: validation data loss: 0.0011650200031663729, training data loss: 4.9896189543210206e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.17337441444396973\n",
      "Epoch 88: validation data loss: 0.0011660648535375725, training data loss: 4.9374487302074694e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.1969306468963623\n",
      "Epoch 89: validation data loss: 0.001172857741787009, training data loss: 4.876885120863239e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.20606684684753418\n",
      "Epoch 90: validation data loss: 0.0011569728317870397, training data loss: 4.818514584814577e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.26668858528137207\n",
      "Epoch 91: validation data loss: 0.0011612450423305982, training data loss: 4.771490024241138e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3111898899078369\n",
      "Epoch 92: validation data loss: 0.0011530321210486705, training data loss: 4.714261740446091e-05\n",
      "Time for epoch[s]: 0.32321596145629883\n",
      "Epoch 93: validation data loss: 0.0011521266203492744, training data loss: 4.676182554464906e-05\n",
      "Time for epoch[s]: 0.28969240188598633\n",
      "Epoch 94: validation data loss: 0.0011457540945375346, training data loss: 4.604277647521398e-05\n",
      "Time for epoch[s]: 0.2532179355621338\n",
      "Epoch 95: validation data loss: 0.0011454255885729506, training data loss: 4.584577410868858e-05\n",
      "Time for epoch[s]: 0.29268360137939453\n",
      "Epoch 96: validation data loss: 0.0011522983579330792, training data loss: 4.53158260480454e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.22106504440307617\n",
      "Epoch 97: validation data loss: 0.0011439871842458369, training data loss: 4.500885957588344e-05\n",
      "Time for epoch[s]: 0.20258307456970215\n",
      "Epoch 98: validation data loss: 0.0011327926698885007, training data loss: 4.4318868621299256e-05\n",
      "Time for epoch[s]: 0.17362403869628906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-10 18:12:12,878]\u001B[0m Trial 3 finished with value: 0.0011358471357659117 and parameters: {'learning_rate': 0.2, 'ff_neurons_layer_00': 10, 'ff_neurons_layer_01': 10, 'early_stopping_epochs': 8, 'layer_activation_00': 'LeakyReLU'}. Best is trial 3 with value: 0.0011358471357659117.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: validation data loss: 0.0011358471357659117, training data loss: 4.404742423802206e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.17209601402282715\n",
      "Final validation data loss:  0.0011358471357659117\n",
      "Initial Guess - validation data loss:  0.056024612357082976\n",
      "Epoch 0: validation data loss: 0.007264077935588959, training data loss: 0.00581682218264227\n",
      "Time for epoch[s]: 0.3419816493988037\n",
      "Epoch 1: validation data loss: 0.00719345378004797, training data loss: 0.003626645155693298\n",
      "Time for epoch[s]: 0.4356043338775635\n",
      "Epoch 2: validation data loss: 0.0071079567687152186, training data loss: 0.003570293454819074\n",
      "Time for epoch[s]: 0.3664565086364746\n",
      "Epoch 3: validation data loss: 0.0070283407490003055, training data loss: 0.003512501172279114\n",
      "Time for epoch[s]: 0.38257503509521484\n",
      "Epoch 4: validation data loss: 0.006942951515929339, training data loss: 0.003453353768614329\n",
      "Time for epoch[s]: 0.3995647430419922\n",
      "Epoch 5: validation data loss: 0.006831633990213751, training data loss: 0.0033907416748673947\n",
      "Time for epoch[s]: 0.3641641139984131\n",
      "Epoch 6: validation data loss: 0.006719848881029103, training data loss: 0.0033147302392410906\n",
      "Time for epoch[s]: 0.503328800201416\n",
      "Epoch 7: validation data loss: 0.006601111529624625, training data loss: 0.0032400546008593415\n",
      "Time for epoch[s]: 0.45745015144348145\n",
      "Epoch 8: validation data loss: 0.00646820569147258, training data loss: 0.0031544419184123002\n",
      "Time for epoch[s]: 0.3887443542480469\n",
      "Epoch 9: validation data loss: 0.00631572503477471, training data loss: 0.0030603827951161283\n",
      "Time for epoch[s]: 0.3113229274749756\n",
      "Epoch 10: validation data loss: 0.006158027474738692, training data loss: 0.0029604647257556655\n",
      "Time for epoch[s]: 0.24504303932189941\n",
      "Epoch 11: validation data loss: 0.005979061126708984, training data loss: 0.0028546860773269443\n",
      "Time for epoch[s]: 0.2702791690826416\n",
      "Epoch 12: validation data loss: 0.005791754483087967, training data loss: 0.0027413253914820006\n",
      "Time for epoch[s]: 0.38037872314453125\n",
      "Epoch 13: validation data loss: 0.0055925133021454835, training data loss: 0.0026260745035458916\n",
      "Time for epoch[s]: 0.4124786853790283\n",
      "Epoch 14: validation data loss: 0.005383527986535199, training data loss: 0.0025065881476554696\n",
      "Time for epoch[s]: 0.4123404026031494\n",
      "Epoch 15: validation data loss: 0.005174739720070199, training data loss: 0.0023864605655408884\n",
      "Time for epoch[s]: 0.4050436019897461\n",
      "Epoch 16: validation data loss: 0.004975319453026062, training data loss: 0.002271517907103447\n",
      "Time for epoch[s]: 0.36652255058288574\n",
      "Epoch 17: validation data loss: 0.004771573358474801, training data loss: 0.002161020149379016\n",
      "Time for epoch[s]: 0.31581830978393555\n",
      "Epoch 18: validation data loss: 0.004581809043884277, training data loss: 0.0020588537057240805\n",
      "Time for epoch[s]: 0.32697415351867676\n",
      "Epoch 19: validation data loss: 0.004407836421983971, training data loss: 0.001964637541879802\n",
      "Time for epoch[s]: 0.280592679977417\n",
      "Epoch 20: validation data loss: 0.0042372269717525675, training data loss: 0.0018790961672726287\n",
      "Time for epoch[s]: 0.2680816650390625\n",
      "Epoch 21: validation data loss: 0.004092312566765912, training data loss: 0.001802488276947579\n",
      "Time for epoch[s]: 0.2769176959991455\n",
      "Epoch 22: validation data loss: 0.003962661033351671, training data loss: 0.0017334239123618766\n",
      "Time for epoch[s]: 0.26309871673583984\n",
      "Epoch 23: validation data loss: 0.003837252588576922, training data loss: 0.001673930992274524\n",
      "Time for epoch[s]: 0.2619781494140625\n",
      "Epoch 24: validation data loss: 0.0037308218272309327, training data loss: 0.0016176368547901171\n",
      "Time for epoch[s]: 0.23537111282348633\n",
      "Epoch 25: validation data loss: 0.0036313541947978817, training data loss: 0.0015692640112959632\n",
      "Time for epoch[s]: 0.25951409339904785\n",
      "Epoch 26: validation data loss: 0.003535649820005513, training data loss: 0.0015190030069656025\n",
      "Time for epoch[s]: 0.23446965217590332\n",
      "Epoch 27: validation data loss: 0.003455485927459856, training data loss: 0.0014753341674804688\n",
      "Time for epoch[s]: 0.26659417152404785\n",
      "Epoch 28: validation data loss: 0.003385765367446969, training data loss: 0.0014343635948825646\n",
      "Time for epoch[s]: 0.2340691089630127\n",
      "Epoch 29: validation data loss: 0.003317074688602256, training data loss: 0.0013961072109605623\n",
      "Time for epoch[s]: 0.2509944438934326\n",
      "Epoch 30: validation data loss: 0.003253321669417429, training data loss: 0.0013597523240738262\n",
      "Time for epoch[s]: 0.23018527030944824\n",
      "Epoch 31: validation data loss: 0.0032112133013059013, training data loss: 0.0013254904039374225\n",
      "Time for epoch[s]: 0.253110408782959\n",
      "Epoch 32: validation data loss: 0.003158817824707728, training data loss: 0.0012947903376191719\n",
      "Time for epoch[s]: 0.22995328903198242\n",
      "Epoch 33: validation data loss: 0.0030962318590242567, training data loss: 0.001265731984621858\n",
      "Time for epoch[s]: 0.4761083126068115\n",
      "Epoch 34: validation data loss: 0.003057546539393734, training data loss: 0.001235458676673506\n",
      "Time for epoch[s]: 0.4264814853668213\n",
      "Epoch 35: validation data loss: 0.0030152811307340996, training data loss: 0.0012126699702380455\n",
      "Time for epoch[s]: 0.458681583404541\n",
      "Epoch 36: validation data loss: 0.0029708928169180815, training data loss: 0.0011810920282041646\n",
      "Time for epoch[s]: 0.31619691848754883\n",
      "Epoch 37: validation data loss: 0.002934419401160114, training data loss: 0.001156097405577359\n",
      "Time for epoch[s]: 0.29018163681030273\n",
      "Epoch 38: validation data loss: 0.0029071504122590366, training data loss: 0.0011319284705810894\n",
      "Time for epoch[s]: 0.25696516036987305\n",
      "Epoch 39: validation data loss: 0.002859026601869766, training data loss: 0.001110921439514857\n",
      "Time for epoch[s]: 0.2609083652496338\n",
      "Epoch 40: validation data loss: 0.002840678441470072, training data loss: 0.0010898636492420006\n",
      "Time for epoch[s]: 0.23796701431274414\n",
      "Epoch 41: validation data loss: 0.0027993618081149445, training data loss: 0.0010693993878691163\n",
      "Time for epoch[s]: 0.24391508102416992\n",
      "Epoch 42: validation data loss: 0.002770141107306633, training data loss: 0.0010498940400336975\n",
      "Time for epoch[s]: 0.22779178619384766\n",
      "Epoch 43: validation data loss: 0.002739777303721807, training data loss: 0.001031153958682056\n",
      "Time for epoch[s]: 0.24361443519592285\n",
      "Epoch 44: validation data loss: 0.0027129397000351997, training data loss: 0.0010142861163779482\n",
      "Time for epoch[s]: 0.27280426025390625\n",
      "Epoch 45: validation data loss: 0.0026837209043981823, training data loss: 0.0009969670886862767\n",
      "Time for epoch[s]: 0.3417067527770996\n",
      "Epoch 46: validation data loss: 0.0026570207452120847, training data loss: 0.0009813583604821332\n",
      "Time for epoch[s]: 0.37418508529663086\n",
      "Epoch 47: validation data loss: 0.0026336579017987535, training data loss: 0.0009658191710302274\n",
      "Time for epoch[s]: 0.41164588928222656\n",
      "Epoch 48: validation data loss: 0.002610873141789545, training data loss: 0.0009506606075861683\n",
      "Time for epoch[s]: 0.38506317138671875\n",
      "Epoch 49: validation data loss: 0.0025874046974530504, training data loss: 0.0009366827740516836\n",
      "Time for epoch[s]: 0.3883073329925537\n",
      "Epoch 50: validation data loss: 0.002566262467266762, training data loss: 0.0009231283648373329\n",
      "Time for epoch[s]: 0.4195232391357422\n",
      "Epoch 51: validation data loss: 0.0025402068003127563, training data loss: 0.0009095827330193019\n",
      "Time for epoch[s]: 0.5005199909210205\n",
      "Epoch 52: validation data loss: 0.002528747467145528, training data loss: 0.0008964725008838253\n",
      "Time for epoch[s]: 0.4916214942932129\n",
      "Epoch 53: validation data loss: 0.0025028831338229245, training data loss: 0.000884725486851174\n",
      "Time for epoch[s]: 0.5050637722015381\n",
      "Epoch 54: validation data loss: 0.0024853250751756643, training data loss: 0.0008728246302365169\n",
      "Time for epoch[s]: 0.4959092140197754\n",
      "Epoch 55: validation data loss: 0.0024636736199191715, training data loss: 0.0008611376971414644\n",
      "Time for epoch[s]: 0.4342517852783203\n",
      "Epoch 56: validation data loss: 0.002445036962152072, training data loss: 0.0008498279879626618\n",
      "Time for epoch[s]: 0.4131166934967041\n",
      "Epoch 57: validation data loss: 0.0024292953482501585, training data loss: 0.0008389251689388328\n",
      "Time for epoch[s]: 0.45079708099365234\n",
      "Epoch 58: validation data loss: 0.002412186910028327, training data loss: 0.0008299412656592452\n",
      "Time for epoch[s]: 0.4314742088317871\n",
      "Epoch 59: validation data loss: 0.002394453847789329, training data loss: 0.0008192035841615233\n",
      "Time for epoch[s]: 0.4468071460723877\n",
      "Epoch 60: validation data loss: 0.002373557384699991, training data loss: 0.0008100996550904017\n",
      "Time for epoch[s]: 0.35860323905944824\n",
      "Epoch 61: validation data loss: 0.002360189886397967, training data loss: 0.0008035356595635959\n",
      "Time for epoch[s]: 0.3045065402984619\n",
      "Epoch 62: validation data loss: 0.002348168527698952, training data loss: 0.0007900699768980888\n",
      "Time for epoch[s]: 0.33698177337646484\n",
      "Epoch 63: validation data loss: 0.0023296973476671194, training data loss: 0.0007816282309353623\n",
      "Time for epoch[s]: 0.5090076923370361\n",
      "Epoch 64: validation data loss: 0.0023176746281314657, training data loss: 0.0007725450546229811\n",
      "Time for epoch[s]: 0.5024511814117432\n",
      "Epoch 65: validation data loss: 0.0023030445455960486, training data loss: 0.0007640403019238825\n",
      "Time for epoch[s]: 0.3711421489715576\n",
      "Epoch 66: validation data loss: 0.0022853117555243782, training data loss: 0.0007554268183773511\n",
      "Time for epoch[s]: 0.3561379909515381\n",
      "Epoch 67: validation data loss: 0.0022722757570275433, training data loss: 0.0007483563874954503\n",
      "Time for epoch[s]: 0.45675086975097656\n",
      "Epoch 68: validation data loss: 0.0022609912913683886, training data loss: 0.0007402532176884342\n",
      "Time for epoch[s]: 0.38843679428100586\n",
      "Epoch 69: validation data loss: 0.0022463326312635585, training data loss: 0.0007317835745746142\n",
      "Time for epoch[s]: 0.4535801410675049\n",
      "Epoch 70: validation data loss: 0.002230930410019339, training data loss: 0.0007247623379372026\n",
      "Time for epoch[s]: 0.27611374855041504\n",
      "Epoch 71: validation data loss: 0.0022197050046703043, training data loss: 0.0007169582800233746\n",
      "Time for epoch[s]: 0.27526354789733887\n",
      "Epoch 72: validation data loss: 0.0022093935372078256, training data loss: 0.0007103969381280141\n",
      "Time for epoch[s]: 0.2769041061401367\n",
      "Epoch 73: validation data loss: 0.002195203004906711, training data loss: 0.0007035756083928286\n",
      "Time for epoch[s]: 0.24911212921142578\n",
      "Epoch 74: validation data loss: 0.0021831190477223155, training data loss: 0.0006965235488055504\n",
      "Time for epoch[s]: 0.23983311653137207\n",
      "Epoch 75: validation data loss: 0.0021700223559113943, training data loss: 0.0006908436888429127\n",
      "Time for epoch[s]: 0.3234522342681885\n",
      "Epoch 76: validation data loss: 0.0021564342659902355, training data loss: 0.0006832607668828747\n",
      "Time for epoch[s]: 0.31891655921936035\n",
      "Epoch 77: validation data loss: 0.002146953589295688, training data loss: 0.0006767882195781899\n",
      "Time for epoch[s]: 0.27167320251464844\n",
      "Epoch 78: validation data loss: 0.002130371794853036, training data loss: 0.000670417054603089\n",
      "Time for epoch[s]: 0.25115299224853516\n",
      "Epoch 79: validation data loss: 0.002118211766900537, training data loss: 0.0006643684759531936\n",
      "Time for epoch[s]: 0.31087493896484375\n",
      "Epoch 80: validation data loss: 0.002111358321420678, training data loss: 0.0006587288967550617\n",
      "Time for epoch[s]: 0.29058218002319336\n",
      "Epoch 81: validation data loss: 0.002096623318380417, training data loss: 0.0006524597945278638\n",
      "Time for epoch[s]: 0.300173282623291\n",
      "Epoch 82: validation data loss: 0.002084906514921145, training data loss: 0.0006462529505768868\n",
      "Time for epoch[s]: 0.32885098457336426\n",
      "Epoch 83: validation data loss: 0.002075843070740025, training data loss: 0.0006404334007332859\n",
      "Time for epoch[s]: 0.4136641025543213\n",
      "Epoch 84: validation data loss: 0.0020657100089608808, training data loss: 0.0006350921169263588\n",
      "Time for epoch[s]: 0.3289070129394531\n",
      "Epoch 85: validation data loss: 0.002050064606209324, training data loss: 0.0006292400430870927\n",
      "Time for epoch[s]: 0.336897611618042\n",
      "Epoch 86: validation data loss: 0.0020379954821442905, training data loss: 0.0006236484333804754\n",
      "Time for epoch[s]: 0.4118216037750244\n",
      "Epoch 87: validation data loss: 0.00202895802994297, training data loss: 0.0006184481322493183\n",
      "Time for epoch[s]: 0.43856120109558105\n",
      "Epoch 88: validation data loss: 0.002017826250154678, training data loss: 0.0006130109775011942\n",
      "Time for epoch[s]: 0.5001435279846191\n",
      "Epoch 89: validation data loss: 0.0020107394211912807, training data loss: 0.0006074715558796712\n",
      "Time for epoch[s]: 0.44613194465637207\n",
      "Epoch 90: validation data loss: 0.0020011014317812986, training data loss: 0.0006024352355634785\n",
      "Time for epoch[s]: 0.4769573211669922\n",
      "Epoch 91: validation data loss: 0.001983156487277654, training data loss: 0.0005969939302636064\n",
      "Time for epoch[s]: 0.4095475673675537\n",
      "Epoch 92: validation data loss: 0.0019755500908855976, training data loss: 0.00059198877310644\n",
      "Time for epoch[s]: 0.44406676292419434\n",
      "Epoch 93: validation data loss: 0.001961024792771361, training data loss: 0.0005865921032483175\n",
      "Time for epoch[s]: 0.4354259967803955\n",
      "Epoch 94: validation data loss: 0.001958640198729354, training data loss: 0.0005845587411427607\n",
      "Time for epoch[s]: 0.4873943328857422\n",
      "Epoch 95: validation data loss: 0.0019499886797987708, training data loss: 0.0005774513635461189\n",
      "Time for epoch[s]: 0.3937191963195801\n",
      "Epoch 96: validation data loss: 0.0019354437856369366, training data loss: 0.0005748094352957321\n",
      "Time for epoch[s]: 0.45391201972961426\n",
      "Epoch 97: validation data loss: 0.0019193714884318174, training data loss: 0.0005664586952832192\n",
      "Time for epoch[s]: 0.5270543098449707\n",
      "Epoch 98: validation data loss: 0.0019083511611642358, training data loss: 0.0005617242765753236\n",
      "Time for epoch[s]: 0.4032742977142334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-10 18:12:48,879]\u001B[0m Trial 4 finished with value: 0.0018973287926416964 and parameters: {'learning_rate': 0.2, 'ff_neurons_layer_00': 100, 'ff_neurons_layer_01': 100, 'early_stopping_epochs': 8, 'layer_activation_00': 'Sigmoid'}. Best is trial 3 with value: 0.0011358471357659117.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: validation data loss: 0.0018973287926416964, training data loss: 0.0005569027643225508\n",
      "Time for epoch[s]: 0.401684045791626\n",
      "Final validation data loss:  0.0018973287926416964\n",
      "Initial Guess - validation data loss:  0.1221297799724422\n",
      "Epoch 0: validation data loss: 0.0030802387625115104, training data loss: 0.007001734215374951\n",
      "Time for epoch[s]: 0.5278866291046143\n",
      "Epoch 1: validation data loss: 0.0024007417295621412, training data loss: 0.0005932767369431447\n",
      "Time for epoch[s]: 0.5153043270111084\n",
      "Epoch 2: validation data loss: 0.0021296640237172446, training data loss: 0.0003707124490171807\n",
      "Time for epoch[s]: 0.4584624767303467\n",
      "Epoch 3: validation data loss: 0.0019581026682570645, training data loss: 0.0002778194606576336\n",
      "Time for epoch[s]: 0.4254951477050781\n",
      "Epoch 4: validation data loss: 0.0018572872632170376, training data loss: 0.0002251100900782842\n",
      "Time for epoch[s]: 0.526536226272583\n",
      "Epoch 5: validation data loss: 0.0018416523389075989, training data loss: 0.0001914035784055109\n",
      "Time for epoch[s]: 0.4612603187561035\n",
      "Epoch 6: validation data loss: 0.0018109837079157024, training data loss: 0.0001674492303367075\n",
      "Time for epoch[s]: 0.44055724143981934\n",
      "Epoch 7: validation data loss: 0.0017412777088548494, training data loss: 0.00014863187047444522\n",
      "Time for epoch[s]: 0.4424459934234619\n",
      "Epoch 8: validation data loss: 0.001735593630298632, training data loss: 0.00013454479577878838\n",
      "Time for epoch[s]: 0.5211060047149658\n",
      "Epoch 9: validation data loss: 0.0016934168121041772, training data loss: 0.000123289588107366\n",
      "Time for epoch[s]: 0.47988319396972656\n",
      "Epoch 10: validation data loss: 0.0017060512277089297, training data loss: 0.00011403002185092125\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3985288143157959\n",
      "Epoch 11: validation data loss: 0.001684581035892713, training data loss: 0.00010606563186536641\n",
      "Time for epoch[s]: 0.4607720375061035\n",
      "Epoch 12: validation data loss: 0.0016843454750705528, training data loss: 9.93430223230902e-05\n",
      "Time for epoch[s]: 0.4452242851257324\n",
      "Epoch 13: validation data loss: 0.0016820400817209183, training data loss: 9.36204405952262e-05\n",
      "Time for epoch[s]: 0.5091860294342041\n",
      "Epoch 14: validation data loss: 0.001675294278419181, training data loss: 8.825413440460484e-05\n",
      "Time for epoch[s]: 0.6004314422607422\n",
      "Epoch 15: validation data loss: 0.0016424703543588995, training data loss: 8.426983436765192e-05\n",
      "Time for epoch[s]: 0.5701603889465332\n",
      "Epoch 16: validation data loss: 0.0016550612231912133, training data loss: 8.048396982830953e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.472792387008667\n",
      "Epoch 17: validation data loss: 0.0016562857039987223, training data loss: 7.677193067661703e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.44223594665527344\n",
      "Epoch 18: validation data loss: 0.0016188816120635428, training data loss: 7.375848551863405e-05\n",
      "Time for epoch[s]: 0.46102190017700195\n",
      "Epoch 19: validation data loss: 0.0016161750440728175, training data loss: 7.084515115850047e-05\n",
      "Time for epoch[s]: 0.4287736415863037\n",
      "Epoch 20: validation data loss: 0.001613020896911621, training data loss: 6.843772057664993e-05\n",
      "Time for epoch[s]: 0.4333012104034424\n",
      "Epoch 21: validation data loss: 0.0016208131835885244, training data loss: 6.596254584587873e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4574708938598633\n",
      "Epoch 22: validation data loss: 0.00164100432504802, training data loss: 6.369264906944205e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.46967124938964844\n",
      "Epoch 23: validation data loss: 0.0016143931645780938, training data loss: 6.166877029421122e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3296322822570801\n",
      "Epoch 24: validation data loss: 0.0016307315053460806, training data loss: 5.9839895156692695e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.310565710067749\n",
      "Epoch 25: validation data loss: 0.0016194282601413117, training data loss: 5.82597852093444e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.28763818740844727\n",
      "Epoch 26: validation data loss: 0.0015997370900628775, training data loss: 5.668108713000877e-05\n",
      "Time for epoch[s]: 0.2967231273651123\n",
      "Epoch 27: validation data loss: 0.001603126798046234, training data loss: 5.508550191850967e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3031802177429199\n",
      "Epoch 28: validation data loss: 0.0015986397113974236, training data loss: 5.358835622736308e-05\n",
      "Time for epoch[s]: 0.45085930824279785\n",
      "Epoch 29: validation data loss: 0.0015953505692416674, training data loss: 5.2141144735628065e-05\n",
      "Time for epoch[s]: 0.46442532539367676\n",
      "Epoch 30: validation data loss: 0.0015901305359792492, training data loss: 5.088023177973211e-05\n",
      "Time for epoch[s]: 0.536243200302124\n",
      "Epoch 31: validation data loss: 0.0015911933493940797, training data loss: 4.983400781405027e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.6061558723449707\n",
      "Epoch 32: validation data loss: 0.0015904703368879345, training data loss: 4.8735412900850656e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4851686954498291\n",
      "Epoch 33: validation data loss: 0.0015878629738881708, training data loss: 4.748770856557916e-05\n",
      "Time for epoch[s]: 0.44492149353027344\n",
      "Epoch 34: validation data loss: 0.001581821131379637, training data loss: 4.656048976395228e-05\n",
      "Time for epoch[s]: 0.45855069160461426\n",
      "Epoch 35: validation data loss: 0.0015710037048548868, training data loss: 4.5566955691875386e-05\n",
      "Time for epoch[s]: 0.4446747303009033\n",
      "Epoch 36: validation data loss: 0.001560840840753355, training data loss: 4.467472740232128e-05\n",
      "Time for epoch[s]: 0.4164924621582031\n",
      "Epoch 37: validation data loss: 0.0015585628002201585, training data loss: 4.38088950909436e-05\n",
      "Time for epoch[s]: 0.47227025032043457\n",
      "Epoch 38: validation data loss: 0.001565587438949167, training data loss: 4.29882638190435e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3198373317718506\n",
      "Epoch 39: validation data loss: 0.001571481358515073, training data loss: 4.213850639046055e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3232693672180176\n",
      "Epoch 40: validation data loss: 0.0015699500090455357, training data loss: 4.1343122890822966e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4580399990081787\n",
      "Epoch 41: validation data loss: 0.0015525824675276943, training data loss: 4.0520820341425945e-05\n",
      "Time for epoch[s]: 0.4589557647705078\n",
      "Epoch 42: validation data loss: 0.0015565199122581307, training data loss: 3.993643881523446e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3866875171661377\n",
      "Epoch 43: validation data loss: 0.0015593633804147102, training data loss: 3.9281578709001414e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3928558826446533\n",
      "Epoch 44: validation data loss: 0.0015565306628675766, training data loss: 3.853927209088791e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4333980083465576\n",
      "Epoch 45: validation data loss: 0.0015498900522380114, training data loss: 3.785944488494908e-05\n",
      "Time for epoch[s]: 0.44762706756591797\n",
      "Epoch 46: validation data loss: 0.0015484100063097531, training data loss: 3.7335518526432174e-05\n",
      "Time for epoch[s]: 0.39167118072509766\n",
      "Epoch 47: validation data loss: 0.0015655313724796522, training data loss: 3.670702037745959e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.33519959449768066\n",
      "Epoch 48: validation data loss: 0.001537306259756219, training data loss: 3.6166844778953624e-05\n",
      "Time for epoch[s]: 0.3322944641113281\n",
      "Epoch 49: validation data loss: 0.0015402517906607015, training data loss: 3.5654294164213416e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3569464683532715\n",
      "Epoch 50: validation data loss: 0.0015371391490169857, training data loss: 3.516256222373819e-05\n",
      "Time for epoch[s]: 0.37898778915405273\n",
      "Epoch 51: validation data loss: 0.0015368985530992622, training data loss: 3.464293313216945e-05\n",
      "Time for epoch[s]: 0.38007307052612305\n",
      "Epoch 52: validation data loss: 0.0015297295296028868, training data loss: 3.4137711898513035e-05\n",
      "Time for epoch[s]: 0.4002845287322998\n",
      "Epoch 53: validation data loss: 0.0015348540321332678, training data loss: 3.37488549553096e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.42204713821411133\n",
      "Epoch 54: validation data loss: 0.001532892386118571, training data loss: 3.323657438158989e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4219481945037842\n",
      "Epoch 55: validation data loss: 0.0015336622114050878, training data loss: 3.278940346211059e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4486825466156006\n",
      "Epoch 56: validation data loss: 0.001530731921870959, training data loss: 3.244205628900223e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3906993865966797\n",
      "Epoch 57: validation data loss: 0.0015350682278202005, training data loss: 3.19773879871793e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.33939361572265625\n",
      "Epoch 58: validation data loss: 0.0015246391840721375, training data loss: 3.1602451977528395e-05\n",
      "Time for epoch[s]: 0.36442017555236816\n",
      "Epoch 59: validation data loss: 0.001523035029842429, training data loss: 3.120332922293171e-05\n",
      "Time for epoch[s]: 0.5809526443481445\n",
      "Epoch 60: validation data loss: 0.0015424945855249553, training data loss: 3.085116596390667e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.49239063262939453\n",
      "Epoch 61: validation data loss: 0.001515361272036757, training data loss: 3.042673377413728e-05\n",
      "Time for epoch[s]: 0.6005604267120361\n",
      "Epoch 62: validation data loss: 0.0015146740495342098, training data loss: 3.0171332379046096e-05\n",
      "Time for epoch[s]: 0.4205925464630127\n",
      "Epoch 63: validation data loss: 0.001515614115484229, training data loss: 2.9780419296733865e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.38419198989868164\n",
      "Epoch 64: validation data loss: 0.001514649418391049, training data loss: 2.9434107635255275e-05\n",
      "Time for epoch[s]: 0.3990306854248047\n",
      "Epoch 65: validation data loss: 0.0015138953788095414, training data loss: 2.918074749512215e-05\n",
      "Time for epoch[s]: 0.5105798244476318\n",
      "Epoch 66: validation data loss: 0.0015028135417258903, training data loss: 2.8815445783611845e-05\n",
      "Time for epoch[s]: 0.45640015602111816\n",
      "Epoch 67: validation data loss: 0.0015159231614848795, training data loss: 2.854857933983955e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.46701765060424805\n",
      "Epoch 68: validation data loss: 0.0014987299431404567, training data loss: 2.8196197071167977e-05\n",
      "Time for epoch[s]: 0.42682600021362305\n",
      "Epoch 69: validation data loss: 0.0014898348344515447, training data loss: 2.798015787585141e-05\n",
      "Time for epoch[s]: 0.39813995361328125\n",
      "Epoch 70: validation data loss: 0.0015143115226536582, training data loss: 2.7640399493311095e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3577852249145508\n",
      "Epoch 71: validation data loss: 0.0014926904140542087, training data loss: 2.742858952311076e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.33968091011047363\n",
      "Epoch 72: validation data loss: 0.0014928832446059136, training data loss: 2.7154587192214243e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3491780757904053\n",
      "Epoch 73: validation data loss: 0.001497470488831333, training data loss: 2.6883882763859343e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.34013891220092773\n",
      "Epoch 74: validation data loss: 0.0014850065860574104, training data loss: 2.666427137236617e-05\n",
      "Time for epoch[s]: 0.4411487579345703\n",
      "Epoch 75: validation data loss: 0.001479177034064515, training data loss: 2.638881677361928e-05\n",
      "Time for epoch[s]: 0.4709005355834961\n",
      "Epoch 76: validation data loss: 0.0014985892326320144, training data loss: 2.6112805082373424e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.43859410285949707\n",
      "Epoch 77: validation data loss: 0.0014789632466285741, training data loss: 2.5919442953856568e-05\n",
      "Time for epoch[s]: 0.4184224605560303\n",
      "Epoch 78: validation data loss: 0.0014697912076836852, training data loss: 2.5670329049297663e-05\n",
      "Time for epoch[s]: 0.40138983726501465\n",
      "Epoch 79: validation data loss: 0.0014860979770416538, training data loss: 2.5420190264645233e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4413630962371826\n",
      "Epoch 80: validation data loss: 0.001483441215671905, training data loss: 2.5210912726377243e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.5861146450042725\n",
      "Epoch 81: validation data loss: 0.0014731268904524851, training data loss: 2.4959855376993685e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.5275053977966309\n",
      "Epoch 82: validation data loss: 0.0014800108186730511, training data loss: 2.4850597205363454e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.5699441432952881\n",
      "Epoch 83: validation data loss: 0.0014602252065318904, training data loss: 2.4541489541802777e-05\n",
      "Time for epoch[s]: 0.49472570419311523\n",
      "Epoch 84: validation data loss: 0.0014719064921548922, training data loss: 2.4351937755874304e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.41160106658935547\n",
      "Epoch 85: validation data loss: 0.001466979038769796, training data loss: 2.4189481503206845e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3694651126861572\n",
      "Epoch 86: validation data loss: 0.0014724440226271816, training data loss: 2.394785220016083e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3870666027069092\n",
      "Epoch 87: validation data loss: 0.0014691528392164674, training data loss: 2.3754630407922343e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.33629536628723145\n",
      "Epoch 88: validation data loss: 0.001467919240803479, training data loss: 2.35388272327103e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4356882572174072\n",
      "Epoch 89: validation data loss: 0.0014679343460901686, training data loss: 2.3401578352467654e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.6926681995391846\n",
      "Epoch 90: validation data loss: 0.0014497074362349836, training data loss: 2.3232058507122406e-05\n",
      "Time for epoch[s]: 0.659142017364502\n",
      "Epoch 91: validation data loss: 0.0014628185529142754, training data loss: 2.3043197771049526e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.5205342769622803\n",
      "Epoch 92: validation data loss: 0.0014572346319346667, training data loss: 2.2778740433389194e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.41806888580322266\n",
      "Epoch 93: validation data loss: 0.0014555972732909737, training data loss: 2.2662422921695664e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.40234899520874023\n",
      "Epoch 94: validation data loss: 0.0014573615979930582, training data loss: 2.2504823154782594e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4166755676269531\n",
      "Epoch 95: validation data loss: 0.001455676201816019, training data loss: 2.2371167733789033e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3689537048339844\n",
      "Epoch 96: validation data loss: 0.0014550857347984836, training data loss: 2.221619608263447e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.36782026290893555\n",
      "Epoch 97: validation data loss: 0.0014507858992711595, training data loss: 2.204513679084168e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3860342502593994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-10 18:13:32,318]\u001B[0m Trial 5 finished with value: 0.0014523690966166318 and parameters: {'learning_rate': 0.2, 'ff_neurons_layer_00': 100, 'ff_neurons_layer_01': 200, 'early_stopping_epochs': 8, 'layer_activation_00': 'ReLU'}. Best is trial 3 with value: 0.0011358471357659117.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98: validation data loss: 0.0014523690966166318, training data loss: 2.183109419803097e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Stopping the training, validation accuracy has not improved for 8 epochs.\n",
      "Final validation data loss:  0.0014523690966166318\n",
      "Initial Guess - validation data loss:  0.12340168539247556\n",
      "Epoch 0: validation data loss: 0.002214156329359638, training data loss: 0.005744568833477421\n",
      "Time for epoch[s]: 0.41217947006225586\n",
      "Epoch 1: validation data loss: 0.0018462971465228355, training data loss: 0.00030026843422624076\n",
      "Time for epoch[s]: 0.5958061218261719\n",
      "Epoch 2: validation data loss: 0.0017654684852791704, training data loss: 0.00019764947836801885\n",
      "Time for epoch[s]: 0.5897159576416016\n",
      "Epoch 3: validation data loss: 0.0016132699300165046, training data loss: 0.00015249868778333272\n",
      "Time for epoch[s]: 0.627415657043457\n",
      "Epoch 4: validation data loss: 0.001583177341173773, training data loss: 0.0001258874167454297\n",
      "Time for epoch[s]: 0.5028781890869141\n",
      "Epoch 5: validation data loss: 0.0015820766965003862, training data loss: 0.0001083303532099615\n",
      "Time for epoch[s]: 0.3857574462890625\n",
      "Epoch 6: validation data loss: 0.0015267977431484554, training data loss: 9.667806191259323e-05\n",
      "Time for epoch[s]: 0.3890848159790039\n",
      "Epoch 7: validation data loss: 0.001473661971418825, training data loss: 8.732360281628561e-05\n",
      "Time for epoch[s]: 0.41716861724853516\n",
      "Epoch 8: validation data loss: 0.0014892072165937729, training data loss: 7.912403474387513e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.40074634552001953\n",
      "Epoch 9: validation data loss: 0.0014799955773026976, training data loss: 7.277936355708396e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.5104489326477051\n",
      "Epoch 10: validation data loss: 0.00142114796594942, training data loss: 6.790062387103904e-05\n",
      "Time for epoch[s]: 0.3595256805419922\n",
      "Epoch 11: validation data loss: 0.0014338810422104787, training data loss: 6.357676107181262e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.39246487617492676\n",
      "Epoch 12: validation data loss: 0.0014327993131663702, training data loss: 5.9042444886410075e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.34178686141967773\n",
      "Epoch 13: validation data loss: 0.0014122384323921378, training data loss: 5.640322980407166e-05\n",
      "Time for epoch[s]: 0.43932294845581055\n",
      "Epoch 14: validation data loss: 0.0013836216708840844, training data loss: 5.318854242291081e-05\n",
      "Time for epoch[s]: 0.4079442024230957\n",
      "Epoch 15: validation data loss: 0.0013998561798165378, training data loss: 5.079816482518906e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3851172924041748\n",
      "Epoch 16: validation data loss: 0.001377733330748397, training data loss: 4.865634403952725e-05\n",
      "Time for epoch[s]: 0.3418903350830078\n",
      "Epoch 17: validation data loss: 0.0013574614916762261, training data loss: 4.627305980278477e-05\n",
      "Time for epoch[s]: 0.3199648857116699\n",
      "Epoch 18: validation data loss: 0.001346165050654651, training data loss: 4.4539064747285626e-05\n",
      "Time for epoch[s]: 0.32151079177856445\n",
      "Epoch 19: validation data loss: 0.001353965774518714, training data loss: 4.281154642366383e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.510932207107544\n",
      "Epoch 20: validation data loss: 0.0013454382278059172, training data loss: 4.132870652768166e-05\n",
      "Time for epoch[s]: 0.5702221393585205\n",
      "Epoch 21: validation data loss: 0.0013132054511814902, training data loss: 3.973315108448403e-05\n",
      "Time for epoch[s]: 0.4148139953613281\n",
      "Epoch 22: validation data loss: 0.0013196529590920225, training data loss: 3.861913193850757e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.35726451873779297\n",
      "Epoch 23: validation data loss: 0.0013450890371244247, training data loss: 3.7234017123642574e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3404097557067871\n",
      "Epoch 24: validation data loss: 0.001305565714291786, training data loss: 3.6337906197053656e-05\n",
      "Time for epoch[s]: 0.28439807891845703\n",
      "Epoch 25: validation data loss: 0.0013188316941805626, training data loss: 3.5077773595917715e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2680244445800781\n",
      "Epoch 26: validation data loss: 0.0012987329535288352, training data loss: 3.413128194539514e-05\n",
      "Time for epoch[s]: 0.3803880214691162\n",
      "Epoch 27: validation data loss: 0.0012816339050798111, training data loss: 3.3135685355287706e-05\n",
      "Time for epoch[s]: 0.3090817928314209\n",
      "Epoch 28: validation data loss: 0.001312895044344201, training data loss: 3.2205819301137094e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2929108142852783\n",
      "Epoch 29: validation data loss: 0.001295635825422801, training data loss: 3.160794635545718e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2642982006072998\n",
      "Epoch 30: validation data loss: 0.0012692958796949691, training data loss: 3.0634941779859534e-05\n",
      "Time for epoch[s]: 0.28720712661743164\n",
      "Epoch 31: validation data loss: 0.0012714402860702445, training data loss: 2.9868140102249303e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.26776766777038574\n",
      "Epoch 32: validation data loss: 0.001257494976531425, training data loss: 2.925351610806979e-05\n",
      "Time for epoch[s]: 0.27095484733581543\n",
      "Epoch 33: validation data loss: 0.001257004258839507, training data loss: 2.881053401386901e-05\n",
      "Time for epoch[s]: 0.2697131633758545\n",
      "Epoch 34: validation data loss: 0.0012425747636246355, training data loss: 2.8007045031002122e-05\n",
      "Time for epoch[s]: 0.2977426052093506\n",
      "Epoch 35: validation data loss: 0.0012420126020091854, training data loss: 2.7380031044488627e-05\n",
      "Time for epoch[s]: 0.2752542495727539\n",
      "Epoch 36: validation data loss: 0.001232335692671336, training data loss: 2.679597909592058e-05\n",
      "Time for epoch[s]: 0.25652432441711426\n",
      "Epoch 37: validation data loss: 0.001231994530926012, training data loss: 2.63894823077879e-05\n",
      "Time for epoch[s]: 0.2763667106628418\n",
      "Epoch 38: validation data loss: 0.0012304669917990627, training data loss: 2.5924078303657166e-05\n",
      "Time for epoch[s]: 0.281095027923584\n",
      "Epoch 39: validation data loss: 0.001219988277513687, training data loss: 2.536866558741217e-05\n",
      "Time for epoch[s]: 0.32229137420654297\n",
      "Epoch 40: validation data loss: 0.001220508389277001, training data loss: 2.4697724219462643e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.5241189002990723\n",
      "Epoch 41: validation data loss: 0.0012223566775996935, training data loss: 2.4475199787039734e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.40599822998046875\n",
      "Epoch 42: validation data loss: 0.0012127531173566704, training data loss: 2.4158137608064363e-05\n",
      "Time for epoch[s]: 0.35683560371398926\n",
      "Epoch 43: validation data loss: 0.0011936592184789649, training data loss: 2.362219973990362e-05\n",
      "Time for epoch[s]: 0.32889389991760254\n",
      "Epoch 44: validation data loss: 0.001207031071458233, training data loss: 2.3242702801205797e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2802708148956299\n",
      "Epoch 45: validation data loss: 0.001169689714092098, training data loss: 2.2841228350792845e-05\n",
      "Time for epoch[s]: 0.27263545989990234\n",
      "Epoch 46: validation data loss: 0.001180249262073813, training data loss: 2.2572881997175958e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3297536373138428\n",
      "Epoch 47: validation data loss: 0.0011909058377078678, training data loss: 2.215194545652224e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.29661011695861816\n",
      "Epoch 48: validation data loss: 0.0011929986683745363, training data loss: 2.1779463205435504e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3291501998901367\n",
      "Epoch 49: validation data loss: 0.001163058357151676, training data loss: 2.151920957720443e-05\n",
      "Time for epoch[s]: 0.35187721252441406\n",
      "Epoch 50: validation data loss: 0.0011827454447201943, training data loss: 2.1169081194215715e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.47101426124572754\n",
      "Epoch 51: validation data loss: 0.0011763052853275109, training data loss: 2.0988034637279162e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.43517255783081055\n",
      "Epoch 52: validation data loss: 0.00116981599973217, training data loss: 2.0553715118806658e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.343217134475708\n",
      "Epoch 53: validation data loss: 0.001158729807971275, training data loss: 2.0196378554247286e-05\n",
      "Time for epoch[s]: 0.32354283332824707\n",
      "Epoch 54: validation data loss: 0.001144519679622563, training data loss: 1.9889399324241837e-05\n",
      "Time for epoch[s]: 0.3017923831939697\n",
      "Epoch 55: validation data loss: 0.0011548237985671928, training data loss: 1.9790925782973362e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.31039857864379883\n",
      "Epoch 56: validation data loss: 0.0011477620089979477, training data loss: 1.951216902907036e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.38466572761535645\n",
      "Epoch 57: validation data loss: 0.0011504057063359647, training data loss: 1.9219499846843824e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.508702278137207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-10 18:13:54,494]\u001B[0m Trial 6 finished with value: 0.0011616977926802961 and parameters: {'learning_rate': 0.5, 'ff_neurons_layer_00': 100, 'ff_neurons_layer_01': 100, 'early_stopping_epochs': 4, 'layer_activation_00': 'LeakyReLU'}. Best is trial 3 with value: 0.0011358471357659117.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: validation data loss: 0.0011616977926802961, training data loss: 1.8862692732789202e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Stopping the training, validation accuracy has not improved for 4 epochs.\n",
      "Final validation data loss:  0.0011616977926802961\n",
      "Initial Guess - validation data loss:  0.09179015137833547\n",
      "Epoch 0: validation data loss: 0.004120888775342131, training data loss: 0.005378122743406252\n",
      "Time for epoch[s]: 0.36455273628234863\n",
      "Epoch 1: validation data loss: 0.003531540909858599, training data loss: 0.0005750323403371524\n",
      "Time for epoch[s]: 0.40655946731567383\n",
      "Epoch 2: validation data loss: 0.003356931416411378, training data loss: 0.0003638046381136054\n",
      "Time for epoch[s]: 0.41387224197387695\n",
      "Epoch 3: validation data loss: 0.0032717079332430067, training data loss: 0.0002684506957661616\n",
      "Time for epoch[s]: 0.49228334426879883\n",
      "Epoch 4: validation data loss: 0.0033669199573394914, training data loss: 0.00020465196948073227\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4595143795013428\n",
      "Epoch 5: validation data loss: 0.00336080190797919, training data loss: 0.00016361716539348096\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.49802637100219727\n",
      "Epoch 6: validation data loss: 0.003273580988792524, training data loss: 0.00013732990955925422\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.38384246826171875\n",
      "Epoch 7: validation data loss: 0.0034205951647127055, training data loss: 0.00011879885271531806\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.33016252517700195\n",
      "Epoch 8: validation data loss: 0.003254529548017946, training data loss: 0.00010417259753294731\n",
      "Time for epoch[s]: 0.28191494941711426\n",
      "Epoch 9: validation data loss: 0.0034051532070386355, training data loss: 9.360451880655332e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.23922085762023926\n",
      "Epoch 10: validation data loss: 0.0033310142826271927, training data loss: 8.482827919803254e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.25693440437316895\n",
      "Epoch 11: validation data loss: 0.003403916478701378, training data loss: 8.041277255641816e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.24939608573913574\n",
      "Epoch 12: validation data loss: 0.0034617231861097083, training data loss: 7.452501928153104e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.26286768913269043\n",
      "Epoch 13: validation data loss: 0.0032915086506708573, training data loss: 6.954143138508819e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.24486637115478516\n",
      "Epoch 14: validation data loss: 0.0032296730503099695, training data loss: 6.524568262029456e-05\n",
      "Time for epoch[s]: 0.26868486404418945\n",
      "Epoch 15: validation data loss: 0.0032275651143566114, training data loss: 6.20919692257768e-05\n",
      "Time for epoch[s]: 0.275040864944458\n",
      "Epoch 16: validation data loss: 0.0030946339646430866, training data loss: 5.9002381005243626e-05\n",
      "Time for epoch[s]: 0.3945157527923584\n",
      "Epoch 17: validation data loss: 0.0032296929185248948, training data loss: 5.53118473250572e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4819173812866211\n",
      "Epoch 18: validation data loss: 0.0031845324660000735, training data loss: 5.334124955684627e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3496673107147217\n",
      "Epoch 19: validation data loss: 0.003181901968777452, training data loss: 5.0726474250016145e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.30362677574157715\n",
      "Epoch 20: validation data loss: 0.0031360164624915274, training data loss: 4.909425701724884e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2893855571746826\n",
      "Epoch 21: validation data loss: 0.0030989279485728644, training data loss: 4.69778456032004e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.24524188041687012\n",
      "Epoch 22: validation data loss: 0.00312426536594896, training data loss: 4.5149553073867816e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.26601576805114746\n",
      "Epoch 23: validation data loss: 0.0030549519682583743, training data loss: 4.390254616737366e-05\n",
      "Time for epoch[s]: 0.2482306957244873\n",
      "Epoch 24: validation data loss: 0.003152564780352867, training data loss: 4.264960686365763e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4645977020263672\n",
      "Epoch 25: validation data loss: 0.0030405804446843117, training data loss: 4.125345226156113e-05\n",
      "Time for epoch[s]: 0.3796982765197754\n",
      "Epoch 26: validation data loss: 0.003024179913681936, training data loss: 3.98988074295597e-05\n",
      "Time for epoch[s]: 0.43621349334716797\n",
      "Epoch 27: validation data loss: 0.0030290919896130146, training data loss: 3.86726936180842e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3504812717437744\n",
      "Epoch 28: validation data loss: 0.0030017410783462874, training data loss: 3.7830544116834524e-05\n",
      "Time for epoch[s]: 0.26737284660339355\n",
      "Epoch 29: validation data loss: 0.0029860125284761055, training data loss: 3.6136434332693e-05\n",
      "Time for epoch[s]: 0.2720975875854492\n",
      "Epoch 30: validation data loss: 0.002969808502284359, training data loss: 3.5405278069787916e-05\n",
      "Time for epoch[s]: 0.2581920623779297\n",
      "Epoch 31: validation data loss: 0.0029736365357490436, training data loss: 3.468707739694478e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.26984453201293945\n",
      "Epoch 32: validation data loss: 0.002984938011866182, training data loss: 3.385028618996002e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.24578285217285156\n",
      "Epoch 33: validation data loss: 0.002932852261686978, training data loss: 3.247627495154398e-05\n",
      "Time for epoch[s]: 0.29120397567749023\n",
      "Epoch 34: validation data loss: 0.0028745257146826617, training data loss: 3.250632605187969e-05\n",
      "Time for epoch[s]: 0.35241150856018066\n",
      "Epoch 35: validation data loss: 0.002907976167931404, training data loss: 3.159594760365682e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.48700809478759766\n",
      "Epoch 36: validation data loss: 0.0028772109175381594, training data loss: 3.0820923495918646e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4993412494659424\n",
      "Epoch 37: validation data loss: 0.002894227363203214, training data loss: 2.9916432792465436e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.41733479499816895\n",
      "Epoch 38: validation data loss: 0.002883829754781505, training data loss: 2.9281404753934304e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.33675432205200195\n",
      "Epoch 39: validation data loss: 0.002876806204721808, training data loss: 2.852155184841047e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.31183481216430664\n",
      "Epoch 40: validation data loss: 0.0028028183331772617, training data loss: 2.8119758451910322e-05\n",
      "Time for epoch[s]: 0.25664830207824707\n",
      "Epoch 41: validation data loss: 0.002791821412299866, training data loss: 2.7724224903948232e-05\n",
      "Time for epoch[s]: 0.26752805709838867\n",
      "Epoch 42: validation data loss: 0.002835195086318064, training data loss: 2.7055705399954154e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.24940204620361328\n",
      "Epoch 43: validation data loss: 0.002804986962444706, training data loss: 2.6365586866935093e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2858011722564697\n",
      "Epoch 44: validation data loss: 0.0027198347871162033, training data loss: 2.5838783614711675e-05\n",
      "Time for epoch[s]: 0.256350040435791\n",
      "Epoch 45: validation data loss: 0.0026962144734108285, training data loss: 2.5164591122979987e-05\n",
      "Time for epoch[s]: 0.4734663963317871\n",
      "Epoch 46: validation data loss: 0.0027872691415760614, training data loss: 2.491813297617381e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.39040064811706543\n",
      "Epoch 47: validation data loss: 0.002696310548477521, training data loss: 2.4320783102076892e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4693338871002197\n",
      "Epoch 48: validation data loss: 0.0026653830863569423, training data loss: 2.3825693722457103e-05\n",
      "Time for epoch[s]: 0.4092094898223877\n",
      "Epoch 49: validation data loss: 0.0026826610848239567, training data loss: 2.330164191181257e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3391711711883545\n",
      "Epoch 50: validation data loss: 0.0027623690970956464, training data loss: 2.3041324494364056e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.450793981552124\n",
      "Epoch 51: validation data loss: 0.0027196352884649687, training data loss: 2.25689929812194e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.44419431686401367\n",
      "Epoch 52: validation data loss: 0.00267998295831898, training data loss: 2.209526661052007e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.5052709579467773\n",
      "Epoch 53: validation data loss: 0.00268656396430377, training data loss: 2.1803214057395447e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.40302562713623047\n",
      "Epoch 54: validation data loss: 0.0026197858052710962, training data loss: 2.11722004869485e-05\n",
      "Time for epoch[s]: 0.37369656562805176\n",
      "Epoch 55: validation data loss: 0.002614985045777064, training data loss: 2.09690424609402e-05\n",
      "Time for epoch[s]: 0.34117627143859863\n",
      "Epoch 56: validation data loss: 0.002627000961129524, training data loss: 2.0422511330070017e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.32352137565612793\n",
      "Epoch 57: validation data loss: 0.0025601455065757716, training data loss: 2.0130983974835644e-05\n",
      "Time for epoch[s]: 0.33661532402038574\n",
      "Epoch 58: validation data loss: 0.002583010011612008, training data loss: 2.0158498390624512e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.32076168060302734\n",
      "Epoch 59: validation data loss: 0.0026038185646544854, training data loss: 1.949062953664832e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4863262176513672\n",
      "Epoch 60: validation data loss: 0.002555721970997989, training data loss: 1.9127498784838202e-05\n",
      "Time for epoch[s]: 0.49015140533447266\n",
      "Epoch 61: validation data loss: 0.0025971167163761784, training data loss: 1.8840672694929114e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4017908573150635\n",
      "Epoch 62: validation data loss: 0.002550971018124933, training data loss: 1.8622149972746907e-05\n",
      "Time for epoch[s]: 0.3189818859100342\n",
      "Epoch 63: validation data loss: 0.002532311226134975, training data loss: 1.816721590550523e-05\n",
      "Time for epoch[s]: 0.2662198543548584\n",
      "Epoch 64: validation data loss: 0.0025455916308921222, training data loss: 1.7987568458737848e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2782130241394043\n",
      "Epoch 65: validation data loss: 0.0025324222704046937, training data loss: 1.772651427582791e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3923041820526123\n",
      "Epoch 66: validation data loss: 0.002519624962654288, training data loss: 1.7345200407586685e-05\n",
      "Time for epoch[s]: 0.4321732521057129\n",
      "Epoch 67: validation data loss: 0.0025150950096513584, training data loss: 1.7062169776115242e-05\n",
      "Time for epoch[s]: 0.3883483409881592\n",
      "Epoch 68: validation data loss: 0.0024608076979580535, training data loss: 1.6853589666624593e-05\n",
      "Time for epoch[s]: 0.3325324058532715\n",
      "Epoch 69: validation data loss: 0.0024611729465118825, training data loss: 1.661031141230777e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3106720447540283\n",
      "Epoch 70: validation data loss: 0.002492448510644643, training data loss: 1.6367002326535853e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.28829169273376465\n",
      "Epoch 71: validation data loss: 0.0024601661995665666, training data loss: 1.6249446240870376e-05\n",
      "Time for epoch[s]: 0.3006424903869629\n",
      "Epoch 72: validation data loss: 0.002510277375783006, training data loss: 1.5959358395642886e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.27640604972839355\n",
      "Epoch 73: validation data loss: 0.0024532417184141672, training data loss: 1.58583184015261e-05\n",
      "Time for epoch[s]: 0.23668575286865234\n",
      "Epoch 74: validation data loss: 0.00241251187781765, training data loss: 1.5584573353806586e-05\n",
      "Time for epoch[s]: 0.22238922119140625\n",
      "Epoch 75: validation data loss: 0.0024712733482116977, training data loss: 1.523202842263054e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.23695850372314453\n",
      "Epoch 76: validation data loss: 0.0024509753810760637, training data loss: 1.5118359226614373e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.28328967094421387\n",
      "Epoch 77: validation data loss: 0.002395422491308761, training data loss: 1.4796437372423742e-05\n",
      "Time for epoch[s]: 0.2517669200897217\n",
      "Epoch 78: validation data loss: 0.002354602835494089, training data loss: 1.4777721616027018e-05\n",
      "Time for epoch[s]: 0.21378040313720703\n",
      "Epoch 79: validation data loss: 0.0023730642175021238, training data loss: 1.457966672741387e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.26533961296081543\n",
      "Epoch 80: validation data loss: 0.0023869210726594273, training data loss: 1.4372404928416966e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.22244930267333984\n",
      "Epoch 81: validation data loss: 0.002387798267956738, training data loss: 1.4226815607994114e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.28481483459472656\n",
      "Epoch 82: validation data loss: 0.002381565091816802, training data loss: 1.4049931297495485e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3169591426849365\n",
      "Epoch 83: validation data loss: 0.002383774001848752, training data loss: 1.3889382342429466e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2575867176055908\n",
      "Epoch 84: validation data loss: 0.0023511457660971165, training data loss: 1.3709371297122682e-05\n",
      "Time for epoch[s]: 0.25759077072143555\n",
      "Epoch 85: validation data loss: 0.002367465463403153, training data loss: 1.3693233688264133e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3384881019592285\n",
      "Epoch 86: validation data loss: 0.0022779727635318285, training data loss: 1.3518422527332284e-05\n",
      "Time for epoch[s]: 0.3137691020965576\n",
      "Epoch 87: validation data loss: 0.002359681477829746, training data loss: 1.3326945370190765e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.32972121238708496\n",
      "Epoch 88: validation data loss: 0.0023111733671737043, training data loss: 1.3306776282790044e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.389162540435791\n",
      "Epoch 89: validation data loss: 0.0022798368375595303, training data loss: 1.307728819413e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.45505857467651367\n",
      "Epoch 90: validation data loss: 0.0022963671923772384, training data loss: 1.2978460622704737e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.46636199951171875\n",
      "Epoch 91: validation data loss: 0.002283006498258408, training data loss: 1.2758481380057662e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.47617030143737793\n",
      "Epoch 92: validation data loss: 0.0022785301622190433, training data loss: 1.2684777191920911e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.45496344566345215\n",
      "Epoch 93: validation data loss: 0.002265803890141178, training data loss: 1.2583033390105042e-05\n",
      "Time for epoch[s]: 0.43203043937683105\n",
      "Epoch 94: validation data loss: 0.002308473195115181, training data loss: 1.2527091309564299e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.34882497787475586\n",
      "Epoch 95: validation data loss: 0.0022183257967369743, training data loss: 1.2271755829997804e-05\n",
      "Time for epoch[s]: 0.3222086429595947\n",
      "Epoch 96: validation data loss: 0.002218862102456289, training data loss: 1.2215966843578913e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.335507869720459\n",
      "Epoch 97: validation data loss: 0.002237297084233532, training data loss: 1.214436663646404e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.303363561630249\n",
      "Epoch 98: validation data loss: 0.0022314165008666853, training data loss: 1.1989430069379066e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3169832229614258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-10 18:14:28,810]\u001B[0m Trial 7 finished with value: 0.002178017135080137 and parameters: {'learning_rate': 0.5, 'ff_neurons_layer_00': 100, 'ff_neurons_layer_01': 10, 'early_stopping_epochs': 8, 'layer_activation_00': 'ReLU'}. Best is trial 3 with value: 0.0011358471357659117.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: validation data loss: 0.002178017135080137, training data loss: 1.1871682616061272e-05\n",
      "Time for epoch[s]: 0.2828035354614258\n",
      "Final validation data loss:  0.002178017135080137\n",
      "Initial Guess - validation data loss:  0.12288565613907766\n",
      "Epoch 0: validation data loss: 0.0036933119982889254, training data loss: 0.008292464360798875\n",
      "Time for epoch[s]: 0.2803955078125\n",
      "Epoch 1: validation data loss: 0.0027088280682150087, training data loss: 0.0008328467199247178\n",
      "Time for epoch[s]: 0.3051276206970215\n",
      "Epoch 2: validation data loss: 0.0022929724492983185, training data loss: 0.0005457040313716348\n",
      "Time for epoch[s]: 0.36469221115112305\n",
      "Epoch 3: validation data loss: 0.0020916245571554522, training data loss: 0.00042429073080080287\n",
      "Time for epoch[s]: 0.39666223526000977\n",
      "Epoch 4: validation data loss: 0.0019485584404914892, training data loss: 0.00035034065649389677\n",
      "Time for epoch[s]: 0.5105469226837158\n",
      "Epoch 5: validation data loss: 0.0018989952731894575, training data loss: 0.0003022259296891896\n",
      "Time for epoch[s]: 0.3866863250732422\n",
      "Epoch 6: validation data loss: 0.0017871180353643686, training data loss: 0.0002655271253629362\n",
      "Time for epoch[s]: 0.31081628799438477\n",
      "Epoch 7: validation data loss: 0.0017831215303238123, training data loss: 0.00023733203609784445\n",
      "Time for epoch[s]: 0.3062276840209961\n",
      "Epoch 8: validation data loss: 0.0018090175711400977, training data loss: 0.00021786368600854047\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.29711008071899414\n",
      "Epoch 9: validation data loss: 0.0017420337896913154, training data loss: 0.0001999905936794194\n",
      "Time for epoch[s]: 0.2980461120605469\n",
      "Epoch 10: validation data loss: 0.0017492505785537092, training data loss: 0.00018505927294356638\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.279695987701416\n",
      "Epoch 11: validation data loss: 0.0017301459290665578, training data loss: 0.0001737487812836965\n",
      "Time for epoch[s]: 0.23860573768615723\n",
      "Epoch 12: validation data loss: 0.0017236354144196532, training data loss: 0.00016392957946481226\n",
      "Time for epoch[s]: 0.20632457733154297\n",
      "Epoch 13: validation data loss: 0.0017688692160393006, training data loss: 0.0001552761723733928\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2023024559020996\n",
      "Epoch 14: validation data loss: 0.001727584018010527, training data loss: 0.00014741330930631455\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2033538818359375\n",
      "Epoch 15: validation data loss: 0.0017104367959444925, training data loss: 0.00014109290353783735\n",
      "Time for epoch[s]: 0.1871936321258545\n",
      "Epoch 16: validation data loss: 0.0017373210763278074, training data loss: 0.00013438097656317497\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.18434810638427734\n",
      "Epoch 17: validation data loss: 0.0017696829963492475, training data loss: 0.0001288176637534137\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.19066572189331055\n",
      "Epoch 18: validation data loss: 0.0017605652547862433, training data loss: 0.00012388087162688442\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2178788185119629\n",
      "Epoch 19: validation data loss: 0.0017990997937171972, training data loss: 0.00011959307950381275\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.24037790298461914\n",
      "Epoch 20: validation data loss: 0.0017317815186226205, training data loss: 0.00011518951420370303\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2885410785675049\n",
      "Epoch 21: validation data loss: 0.0017607084148006352, training data loss: 0.00011220986032050494\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2566206455230713\n",
      "Epoch 22: validation data loss: 0.0016996898607576274, training data loss: 0.00010847358126618547\n",
      "Time for epoch[s]: 0.19765138626098633\n",
      "Epoch 23: validation data loss: 0.0017373209402441435, training data loss: 0.00010457563481918753\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.19784235954284668\n",
      "Epoch 24: validation data loss: 0.0017123857861784495, training data loss: 0.00010193115500010312\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.23552846908569336\n",
      "Epoch 25: validation data loss: 0.0017522374788920085, training data loss: 9.945689883406304e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2939791679382324\n",
      "Epoch 26: validation data loss: 0.0017140059982804947, training data loss: 9.730274546636294e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2712068557739258\n",
      "Epoch 27: validation data loss: 0.0016842206863507832, training data loss: 9.487076028841272e-05\n",
      "Time for epoch[s]: 0.22340035438537598\n",
      "Epoch 28: validation data loss: 0.0017531936027143644, training data loss: 9.244127564778611e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.19568276405334473\n",
      "Epoch 29: validation data loss: 0.0017132629814757604, training data loss: 9.075170340331178e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.19715523719787598\n",
      "Epoch 30: validation data loss: 0.0016774414064677338, training data loss: 8.735748764858942e-05\n",
      "Time for epoch[s]: 0.22223854064941406\n",
      "Epoch 31: validation data loss: 0.0016897337077415153, training data loss: 8.651960351967921e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.20237064361572266\n",
      "Epoch 32: validation data loss: 0.0016781989842245023, training data loss: 8.417061236623216e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.20928263664245605\n",
      "Epoch 33: validation data loss: 0.0016910827051014661, training data loss: 8.279379440224878e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.20185589790344238\n",
      "Epoch 34: validation data loss: 0.0016572278112036997, training data loss: 8.114887460998204e-05\n",
      "Time for epoch[s]: 0.2392733097076416\n",
      "Epoch 35: validation data loss: 0.001692530499201387, training data loss: 7.983534405492757e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.22179889678955078\n",
      "Epoch 36: validation data loss: 0.0016516129992323924, training data loss: 7.800711106219792e-05\n",
      "Time for epoch[s]: 0.24196290969848633\n",
      "Epoch 37: validation data loss: 0.001677355945926823, training data loss: 7.649858112204565e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.19782114028930664\n",
      "Epoch 38: validation data loss: 0.0016591245453107303, training data loss: 7.496389760274321e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.190568208694458\n",
      "Epoch 39: validation data loss: 0.001661022095919744, training data loss: 7.395946407971316e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.21446728706359863\n",
      "Epoch 40: validation data loss: 0.0016466418629911936, training data loss: 7.220179046670051e-05\n",
      "Time for epoch[s]: 0.20679903030395508\n",
      "Epoch 41: validation data loss: 0.00166911050064923, training data loss: 7.120088661370212e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.19288420677185059\n",
      "Epoch 42: validation data loss: 0.001628537564517156, training data loss: 6.99474157331741e-05\n",
      "Time for epoch[s]: 0.18465256690979004\n",
      "Epoch 43: validation data loss: 0.001634170747783086, training data loss: 6.882163385550182e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.19919180870056152\n",
      "Epoch 44: validation data loss: 0.001642198187031158, training data loss: 6.791355607172126e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2082078456878662\n",
      "Epoch 45: validation data loss: 0.0016268226381850569, training data loss: 6.656871776874752e-05\n",
      "Time for epoch[s]: 0.2338099479675293\n",
      "Epoch 46: validation data loss: 0.0016160478058470983, training data loss: 6.555379304575594e-05\n",
      "Time for epoch[s]: 0.21201109886169434\n",
      "Epoch 47: validation data loss: 0.0016286032929268058, training data loss: 6.478469920893238e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.18924236297607422\n",
      "Epoch 48: validation data loss: 0.0016050312889221052, training data loss: 6.349680766667405e-05\n",
      "Time for epoch[s]: 0.18686175346374512\n",
      "Epoch 49: validation data loss: 0.0016174941030267166, training data loss: 6.313579046426843e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.21561360359191895\n",
      "Epoch 50: validation data loss: 0.0016140845268284348, training data loss: 6.204581985326662e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3879818916320801\n",
      "Epoch 51: validation data loss: 0.0016562063672226858, training data loss: 6.122568613726255e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.36750030517578125\n",
      "Epoch 52: validation data loss: 0.0016126110129160423, training data loss: 6.042886950653982e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2972426414489746\n",
      "Epoch 53: validation data loss: 0.0016039865746345695, training data loss: 5.922301089790858e-05\n",
      "Time for epoch[s]: 0.21731019020080566\n",
      "Epoch 54: validation data loss: 0.0016053340750742176, training data loss: 5.85417505608846e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.19826436042785645\n",
      "Epoch 55: validation data loss: 0.0015825177436549914, training data loss: 5.76735240275457e-05\n",
      "Time for epoch[s]: 0.18951010704040527\n",
      "Epoch 56: validation data loss: 0.0015929900620081654, training data loss: 5.683575897184137e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2176380157470703\n",
      "Epoch 57: validation data loss: 0.0015768857851420363, training data loss: 5.634620649629532e-05\n",
      "Time for epoch[s]: 0.21197104454040527\n",
      "Epoch 58: validation data loss: 0.0015941935859314384, training data loss: 5.543581103761446e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.1985635757446289\n",
      "Epoch 59: validation data loss: 0.0015614714524517322, training data loss: 5.478166537061674e-05\n",
      "Time for epoch[s]: 0.20296669006347656\n",
      "Epoch 60: validation data loss: 0.001542756138326915, training data loss: 5.398546536899593e-05\n",
      "Time for epoch[s]: 0.3149840831756592\n",
      "Epoch 61: validation data loss: 0.0015931332220225573, training data loss: 5.326732635906298e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.25905323028564453\n",
      "Epoch 62: validation data loss: 0.0015518357764640355, training data loss: 5.2903226011147783e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2382972240447998\n",
      "Epoch 63: validation data loss: 0.0015454407968477572, training data loss: 5.2091886701921347e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.20501065254211426\n",
      "Epoch 64: validation data loss: 0.0015291372934977214, training data loss: 5.1468023902749364e-05\n",
      "Time for epoch[s]: 0.19234490394592285\n",
      "Epoch 65: validation data loss: 0.0015457566470316012, training data loss: 5.0659414772029335e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.18297767639160156\n",
      "Epoch 66: validation data loss: 0.0015428230914895393, training data loss: 5.029688363886315e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.20573925971984863\n",
      "Epoch 67: validation data loss: 0.0015447258132777802, training data loss: 4.9619310318607176e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2687807083129883\n",
      "Epoch 68: validation data loss: 0.0015403937259221186, training data loss: 4.925145065947755e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.24042868614196777\n",
      "Epoch 69: validation data loss: 0.0015369323018479021, training data loss: 4.865366914500929e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2259831428527832\n",
      "Epoch 70: validation data loss: 0.0015217721734417083, training data loss: 4.801170721854249e-05\n",
      "Time for epoch[s]: 0.20374631881713867\n",
      "Epoch 71: validation data loss: 0.0015466756200137204, training data loss: 4.753796171107793e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.1909651756286621\n",
      "Epoch 72: validation data loss: 0.0015295507156685606, training data loss: 4.703984021732252e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.1920764446258545\n",
      "Epoch 73: validation data loss: 0.001554571874609821, training data loss: 4.651878012097589e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.23671865463256836\n",
      "Epoch 74: validation data loss: 0.001522355019774067, training data loss: 4.618670621282978e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3453643321990967\n",
      "Epoch 75: validation data loss: 0.0015169999915170887, training data loss: 4.5441401501496635e-05\n",
      "Time for epoch[s]: 0.3691067695617676\n",
      "Epoch 76: validation data loss: 0.0014977412953224357, training data loss: 4.4919822586181504e-05\n",
      "Time for epoch[s]: 0.2552814483642578\n",
      "Epoch 77: validation data loss: 0.0015139438245938793, training data loss: 4.485814691814658e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.22838521003723145\n",
      "Epoch 78: validation data loss: 0.0014954241987777082, training data loss: 4.42314263620333e-05\n",
      "Time for epoch[s]: 0.1990814208984375\n",
      "Epoch 79: validation data loss: 0.0014994522752282827, training data loss: 4.368980062198421e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.19616937637329102\n",
      "Epoch 80: validation data loss: 0.00153367037642492, training data loss: 4.335733547330447e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.24538111686706543\n",
      "Epoch 81: validation data loss: 0.0015152608422928204, training data loss: 4.2742301101826095e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.21267294883728027\n",
      "Epoch 82: validation data loss: 0.0015017676026853797, training data loss: 4.23330252301203e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.20330572128295898\n",
      "Epoch 83: validation data loss: 0.001532226800918579, training data loss: 4.202569728572619e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.19380879402160645\n",
      "Epoch 84: validation data loss: 0.0014909968528573371, training data loss: 4.1568294575769606e-05\n",
      "Time for epoch[s]: 0.20293211936950684\n",
      "Epoch 85: validation data loss: 0.0015110059143745736, training data loss: 4.130134520601464e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.21226048469543457\n",
      "Epoch 86: validation data loss: 0.0014818586987447521, training data loss: 4.080552862931604e-05\n",
      "Time for epoch[s]: 0.2421104907989502\n",
      "Epoch 87: validation data loss: 0.0014967167214171527, training data loss: 4.039986747993182e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.25996994972229004\n",
      "Epoch 88: validation data loss: 0.0014804088633898732, training data loss: 4.017279062369098e-05\n",
      "Time for epoch[s]: 0.22392702102661133\n",
      "Epoch 89: validation data loss: 0.001492944890505647, training data loss: 3.975314687784404e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.21503901481628418\n",
      "Epoch 90: validation data loss: 0.0014680913866382756, training data loss: 3.943134303370567e-05\n",
      "Time for epoch[s]: 0.19462203979492188\n",
      "Epoch 91: validation data loss: 0.0014512880479908424, training data loss: 3.9163109374372926e-05\n",
      "Time for epoch[s]: 0.2201979160308838\n",
      "Epoch 92: validation data loss: 0.0014660448244173234, training data loss: 3.831035385257033e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.24237775802612305\n",
      "Epoch 93: validation data loss: 0.001472395304675516, training data loss: 3.836342222886543e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2233881950378418\n",
      "Epoch 94: validation data loss: 0.0014667482408758713, training data loss: 3.805214785821906e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2031383514404297\n",
      "Epoch 95: validation data loss: 0.0014568760514803672, training data loss: 3.7655846713340444e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.22440242767333984\n",
      "Epoch 96: validation data loss: 0.001461674497552114, training data loss: 3.7293319832788756e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.312788724899292\n",
      "Epoch 97: validation data loss: 0.0014963359593256423, training data loss: 3.7130257582555625e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.22235107421875\n",
      "Epoch 98: validation data loss: 0.0014787660613996252, training data loss: 3.6752765751592645e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.20450043678283691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-10 18:14:52,759]\u001B[0m Trial 8 finished with value: 0.0014414896159411566 and parameters: {'learning_rate': 0.1, 'ff_neurons_layer_00': 10, 'ff_neurons_layer_01': 200, 'early_stopping_epochs': 8, 'layer_activation_00': 'LeakyReLU'}. Best is trial 3 with value: 0.0011358471357659117.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: validation data loss: 0.0014414896159411566, training data loss: 3.626585840226308e-05\n",
      "Time for epoch[s]: 0.2034437656402588\n",
      "Final validation data loss:  0.0014414896159411566\n",
      "Initial Guess - validation data loss:  0.06764695851225831\n",
      "Epoch 0: validation data loss: 0.009591757979022857, training data loss: 0.020574212618614442\n",
      "Time for epoch[s]: 0.28265833854675293\n",
      "Epoch 1: validation data loss: 0.00744390542104364, training data loss: 0.0044461028760971\n",
      "Time for epoch[s]: 0.3287827968597412\n",
      "Epoch 2: validation data loss: 0.007192898014364722, training data loss: 0.0036868755131551664\n",
      "Time for epoch[s]: 0.5371334552764893\n",
      "Epoch 3: validation data loss: 0.007099978455669804, training data loss: 0.003569377339593896\n",
      "Time for epoch[s]: 0.4911172389984131\n",
      "Epoch 4: validation data loss: 0.007035216784368367, training data loss: 0.003511929620890857\n",
      "Time for epoch[s]: 0.40350818634033203\n",
      "Epoch 5: validation data loss: 0.006970365297848775, training data loss: 0.0034659445013629792\n",
      "Time for epoch[s]: 0.4976320266723633\n",
      "Epoch 6: validation data loss: 0.006905932949013906, training data loss: 0.003423621121062536\n",
      "Time for epoch[s]: 0.5594205856323242\n",
      "Epoch 7: validation data loss: 0.006835819923714416, training data loss: 0.003379108698945067\n",
      "Time for epoch[s]: 0.4697415828704834\n",
      "Epoch 8: validation data loss: 0.00676073167966381, training data loss: 0.003333492638313607\n",
      "Time for epoch[s]: 0.5164034366607666\n",
      "Epoch 9: validation data loss: 0.006679339495968057, training data loss: 0.0032804034071970203\n",
      "Time for epoch[s]: 0.4278383255004883\n",
      "Epoch 10: validation data loss: 0.006591052769525955, training data loss: 0.0032280601867257733\n",
      "Time for epoch[s]: 0.4574751853942871\n",
      "Epoch 11: validation data loss: 0.0064951611436121, training data loss: 0.0031687451279870994\n",
      "Time for epoch[s]: 0.43186068534851074\n",
      "Epoch 12: validation data loss: 0.006388514009240555, training data loss: 0.003113305731995465\n",
      "Time for epoch[s]: 0.3830454349517822\n",
      "Epoch 13: validation data loss: 0.006275370785090477, training data loss: 0.003040723604698704\n",
      "Time for epoch[s]: 0.3042123317718506\n",
      "Epoch 14: validation data loss: 0.006152581950845239, training data loss: 0.0029700022854217112\n",
      "Time for epoch[s]: 0.2796287536621094\n",
      "Epoch 15: validation data loss: 0.006021443567319548, training data loss: 0.0028967215045946374\n",
      "Time for epoch[s]: 0.2435593605041504\n",
      "Epoch 16: validation data loss: 0.005884684928475994, training data loss: 0.0028138294067556998\n",
      "Time for epoch[s]: 0.29520320892333984\n",
      "Epoch 17: validation data loss: 0.005742715373975501, training data loss: 0.002731475928058363\n",
      "Time for epoch[s]: 0.30716586112976074\n",
      "Epoch 18: validation data loss: 0.005592038642325902, training data loss: 0.0026486491503780836\n",
      "Time for epoch[s]: 0.31622910499572754\n",
      "Epoch 19: validation data loss: 0.005441630811996111, training data loss: 0.0025653757461129804\n",
      "Time for epoch[s]: 0.2919628620147705\n",
      "Epoch 20: validation data loss: 0.005288102855421093, training data loss: 0.002478246547315763\n",
      "Time for epoch[s]: 0.32950520515441895\n",
      "Epoch 21: validation data loss: 0.005135514420461437, training data loss: 0.0023946887281931697\n",
      "Time for epoch[s]: 0.3397939205169678\n",
      "Epoch 22: validation data loss: 0.0049860891141847935, training data loss: 0.0023100046806683824\n",
      "Time for epoch[s]: 0.4662802219390869\n",
      "Epoch 23: validation data loss: 0.004837190179520002, training data loss: 0.002228347405995408\n",
      "Time for epoch[s]: 0.4842965602874756\n",
      "Epoch 24: validation data loss: 0.004697422458700938, training data loss: 0.002148754928754345\n",
      "Time for epoch[s]: 0.449474573135376\n",
      "Epoch 25: validation data loss: 0.004558638622771659, training data loss: 0.002072250462013837\n",
      "Time for epoch[s]: 0.4266233444213867\n",
      "Epoch 26: validation data loss: 0.004425269827995126, training data loss: 0.001999637443725377\n",
      "Time for epoch[s]: 0.35564541816711426\n",
      "Epoch 27: validation data loss: 0.004303462429133724, training data loss: 0.001930640032302299\n",
      "Time for epoch[s]: 0.30777716636657715\n",
      "Epoch 28: validation data loss: 0.004186477290985246, training data loss: 0.001868376312734874\n",
      "Time for epoch[s]: 0.26641178131103516\n",
      "Epoch 29: validation data loss: 0.004076479232474549, training data loss: 0.001802657565025434\n",
      "Time for epoch[s]: 0.2880136966705322\n",
      "Epoch 30: validation data loss: 0.0039737790686899125, training data loss: 0.0017439667220529357\n",
      "Time for epoch[s]: 0.30716562271118164\n",
      "Epoch 31: validation data loss: 0.003879714502047186, training data loss: 0.0016882502597216603\n",
      "Time for epoch[s]: 0.34916234016418457\n",
      "Epoch 32: validation data loss: 0.003793178900191773, training data loss: 0.001636698774006813\n",
      "Time for epoch[s]: 0.4249725341796875\n",
      "Epoch 33: validation data loss: 0.003710565229529115, training data loss: 0.0015876610257309865\n",
      "Time for epoch[s]: 0.33615851402282715\n",
      "Epoch 34: validation data loss: 0.00363274388117333, training data loss: 0.0015423205345188645\n",
      "Time for epoch[s]: 0.2955913543701172\n",
      "Epoch 35: validation data loss: 0.003558990345697969, training data loss: 0.001499810039180599\n",
      "Time for epoch[s]: 0.2514157295227051\n",
      "Epoch 36: validation data loss: 0.003490705196171591, training data loss: 0.0014591923315231115\n",
      "Time for epoch[s]: 0.2651684284210205\n",
      "Epoch 37: validation data loss: 0.0034351865994875835, training data loss: 0.0014213471924333268\n",
      "Time for epoch[s]: 0.26254916191101074\n",
      "Epoch 38: validation data loss: 0.003378783731155744, training data loss: 0.0013863992745473505\n",
      "Time for epoch[s]: 0.2634007930755615\n",
      "Epoch 39: validation data loss: 0.003329320585346657, training data loss: 0.0013560019943812123\n",
      "Time for epoch[s]: 0.2505910396575928\n",
      "Epoch 40: validation data loss: 0.003279211858636168, training data loss: 0.0013208046351393609\n",
      "Time for epoch[s]: 0.26581692695617676\n",
      "Epoch 41: validation data loss: 0.0032320820033278097, training data loss: 0.0012909142666211412\n",
      "Time for epoch[s]: 0.25617194175720215\n",
      "Epoch 42: validation data loss: 0.003192398373939131, training data loss: 0.0012633525617590778\n",
      "Time for epoch[s]: 0.26921987533569336\n",
      "Epoch 43: validation data loss: 0.003153355426439956, training data loss: 0.0012364855640010746\n",
      "Time for epoch[s]: 0.25480151176452637\n",
      "Epoch 44: validation data loss: 0.003110636587012304, training data loss: 0.0012128438851604724\n",
      "Time for epoch[s]: 0.29250597953796387\n",
      "Epoch 45: validation data loss: 0.0030810506376501632, training data loss: 0.0011892496995185608\n",
      "Time for epoch[s]: 0.33647871017456055\n",
      "Epoch 46: validation data loss: 0.003046730065454631, training data loss: 0.0011657480507680815\n",
      "Time for epoch[s]: 0.38880276679992676\n",
      "Epoch 47: validation data loss: 0.003012272593093245, training data loss: 0.0011449276584468476\n",
      "Time for epoch[s]: 0.4777233600616455\n",
      "Epoch 48: validation data loss: 0.0029852893798862964, training data loss: 0.0011247478119314534\n",
      "Time for epoch[s]: 0.4033992290496826\n",
      "Epoch 49: validation data loss: 0.0029545717587754062, training data loss: 0.0011071783222564278\n",
      "Time for epoch[s]: 0.47391247749328613\n",
      "Epoch 50: validation data loss: 0.0029235087573256124, training data loss: 0.0010878234829532502\n",
      "Time for epoch[s]: 0.3445546627044678\n",
      "Epoch 51: validation data loss: 0.0028993291397617286, training data loss: 0.0010699985642411392\n",
      "Time for epoch[s]: 0.27918148040771484\n",
      "Epoch 52: validation data loss: 0.0028763908773796745, training data loss: 0.0010542193232061655\n",
      "Time for epoch[s]: 0.3603675365447998\n",
      "Epoch 53: validation data loss: 0.0028506571843743868, training data loss: 0.0010370241996904487\n",
      "Time for epoch[s]: 0.33960485458374023\n",
      "Epoch 54: validation data loss: 0.0028282899290459343, training data loss: 0.0010212272677791717\n",
      "Time for epoch[s]: 0.2987062931060791\n",
      "Epoch 55: validation data loss: 0.0028067350931907896, training data loss: 0.0010071399549371031\n",
      "Time for epoch[s]: 0.3313910961151123\n",
      "Epoch 56: validation data loss: 0.0027850245776241773, training data loss: 0.00099391663727695\n",
      "Time for epoch[s]: 0.3677201271057129\n",
      "Epoch 57: validation data loss: 0.002760464742303439, training data loss: 0.000978492506562847\n",
      "Time for epoch[s]: 0.29564404487609863\n",
      "Epoch 58: validation data loss: 0.0027442800399919624, training data loss: 0.0009650944574782838\n",
      "Time for epoch[s]: 0.24757647514343262\n",
      "Epoch 59: validation data loss: 0.0027209000500369833, training data loss: 0.0009523491336874766\n",
      "Time for epoch[s]: 0.24611735343933105\n",
      "Epoch 60: validation data loss: 0.002701234871938348, training data loss: 0.0009396666125075458\n",
      "Time for epoch[s]: 0.240645170211792\n",
      "Epoch 61: validation data loss: 0.002680313913789514, training data loss: 0.0009275679582874524\n",
      "Time for epoch[s]: 0.27941226959228516\n",
      "Epoch 62: validation data loss: 0.0026630669423978624, training data loss: 0.0009195486160173808\n",
      "Time for epoch[s]: 0.5123989582061768\n",
      "Epoch 63: validation data loss: 0.002643809470956184, training data loss: 0.0009039367578889681\n",
      "Time for epoch[s]: 0.580225944519043\n",
      "Epoch 64: validation data loss: 0.0026297923092428407, training data loss: 0.0008925015115302448\n",
      "Time for epoch[s]: 0.4471862316131592\n",
      "Epoch 65: validation data loss: 0.0026106466985728644, training data loss: 0.0008818981446087632\n",
      "Time for epoch[s]: 0.39861035346984863\n",
      "Epoch 66: validation data loss: 0.0025952352236395014, training data loss: 0.0008715227858660972\n",
      "Time for epoch[s]: 0.36797666549682617\n",
      "Epoch 67: validation data loss: 0.0025735873065582692, training data loss: 0.0008618394805960459\n",
      "Time for epoch[s]: 0.360515832901001\n",
      "Epoch 68: validation data loss: 0.0025612616647868397, training data loss: 0.000851485010695784\n",
      "Time for epoch[s]: 0.29486894607543945\n",
      "Epoch 69: validation data loss: 0.002541991673648085, training data loss: 0.0008405132652962045\n",
      "Time for epoch[s]: 0.2761349678039551\n",
      "Epoch 70: validation data loss: 0.002524609435094546, training data loss: 0.000830311004973982\n",
      "Time for epoch[s]: 0.26221513748168945\n",
      "Epoch 71: validation data loss: 0.0025103092193603516, training data loss: 0.0008211396464474124\n",
      "Time for epoch[s]: 0.27750277519226074\n",
      "Epoch 72: validation data loss: 0.0024947387987075877, training data loss: 0.000812118864494916\n",
      "Time for epoch[s]: 0.25812768936157227\n",
      "Epoch 73: validation data loss: 0.0024803495842572218, training data loss: 0.0008023805814246609\n",
      "Time for epoch[s]: 0.2693662643432617\n",
      "Epoch 74: validation data loss: 0.0024655668158509417, training data loss: 0.0007934743819171435\n",
      "Time for epoch[s]: 0.281862735748291\n",
      "Epoch 75: validation data loss: 0.0024517747365176406, training data loss: 0.0007844884373825979\n",
      "Time for epoch[s]: 0.3171730041503906\n",
      "Epoch 76: validation data loss: 0.00243420225300201, training data loss: 0.0007759670144346752\n",
      "Time for epoch[s]: 0.27077507972717285\n",
      "Epoch 77: validation data loss: 0.0024201467156954553, training data loss: 0.0007673478834160931\n",
      "Time for epoch[s]: 0.25807833671569824\n",
      "Epoch 78: validation data loss: 0.002406839094205534, training data loss: 0.0007590560336091203\n",
      "Time for epoch[s]: 0.27161502838134766\n",
      "Epoch 79: validation data loss: 0.0023920326472417403, training data loss: 0.0007508842776355134\n",
      "Time for epoch[s]: 0.276400089263916\n",
      "Epoch 80: validation data loss: 0.0023746419715010413, training data loss: 0.0007429286905619652\n",
      "Time for epoch[s]: 0.2762641906738281\n",
      "Epoch 81: validation data loss: 0.002362336742279192, training data loss: 0.0007351922662290808\n",
      "Time for epoch[s]: 0.2643461227416992\n",
      "Epoch 82: validation data loss: 0.002351269874398567, training data loss: 0.0007270795025237619\n",
      "Time for epoch[s]: 0.3677237033843994\n",
      "Epoch 83: validation data loss: 0.0023374949416069134, training data loss: 0.0007234613781105982\n",
      "Time for epoch[s]: 0.48478221893310547\n",
      "Epoch 84: validation data loss: 0.002320781146010307, training data loss: 0.0007119176050299379\n",
      "Time for epoch[s]: 0.4540572166442871\n",
      "Epoch 85: validation data loss: 0.002311406342406251, training data loss: 0.0007098485890044469\n",
      "Time for epoch[s]: 0.358548641204834\n",
      "Epoch 86: validation data loss: 0.0022927988065432198, training data loss: 0.0006976585546040644\n",
      "Time for epoch[s]: 0.32007408142089844\n",
      "Epoch 87: validation data loss: 0.0022780919728213793, training data loss: 0.000690286494281194\n",
      "Time for epoch[s]: 0.3471229076385498\n",
      "Epoch 88: validation data loss: 0.0022664581803970687, training data loss: 0.0006831342771173068\n",
      "Time for epoch[s]: 0.40267491340637207\n",
      "Epoch 89: validation data loss: 0.0022562916420366124, training data loss: 0.0006762824646414143\n",
      "Time for epoch[s]: 0.4112701416015625\n",
      "Epoch 90: validation data loss: 0.002240497227673117, training data loss: 0.0006696291301892773\n",
      "Time for epoch[s]: 0.34310460090637207\n",
      "Epoch 91: validation data loss: 0.0022275172956457965, training data loss: 0.0006629650451276945\n",
      "Time for epoch[s]: 0.3924219608306885\n",
      "Epoch 92: validation data loss: 0.0022153833957567607, training data loss: 0.0006562323058576889\n",
      "Time for epoch[s]: 0.38878750801086426\n",
      "Epoch 93: validation data loss: 0.0021998058987534753, training data loss: 0.0006497845257798287\n",
      "Time for epoch[s]: 0.348400354385376\n",
      "Epoch 94: validation data loss: 0.0021907035346444884, training data loss: 0.0006434741902024779\n",
      "Time for epoch[s]: 0.3208959102630615\n",
      "Epoch 95: validation data loss: 0.0021767629880339043, training data loss: 0.0006372127767022886\n",
      "Time for epoch[s]: 0.3369269371032715\n",
      "Epoch 96: validation data loss: 0.00216601714151635, training data loss: 0.0006308597788963144\n",
      "Time for epoch[s]: 0.32572245597839355\n",
      "Epoch 97: validation data loss: 0.0021518907046209185, training data loss: 0.0006317903189898625\n",
      "Time for epoch[s]: 0.39895057678222656\n",
      "Epoch 98: validation data loss: 0.0021381054294708113, training data loss: 0.0006187360172402368\n",
      "Time for epoch[s]: 0.33763742446899414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-10 18:15:27,687]\u001B[0m Trial 9 finished with value: 0.0021281216514709334 and parameters: {'learning_rate': 0.2, 'ff_neurons_layer_00': 100, 'ff_neurons_layer_01': 10, 'early_stopping_epochs': 4, 'layer_activation_00': 'Sigmoid'}. Best is trial 3 with value: 0.0011358471357659117.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: validation data loss: 0.0021281216514709334, training data loss: 0.000612817534573002\n",
      "Time for epoch[s]: 0.37757039070129395\n",
      "Final validation data loss:  0.0021281216514709334\n",
      "Initial Guess - validation data loss:  0.1280695684424274\n",
      "Epoch 0: validation data loss: 0.026301059548713302, training data loss: 0.058791334770585846\n",
      "Time for epoch[s]: 0.37727856636047363\n",
      "Epoch 1: validation data loss: 0.019513974995373592, training data loss: 0.017112139697488586\n",
      "Time for epoch[s]: 0.40183162689208984\n",
      "Epoch 2: validation data loss: 0.007752214936905256, training data loss: 0.013614544585415217\n",
      "Time for epoch[s]: 0.3727884292602539\n",
      "Epoch 3: validation data loss: 0.004707333159773317, training data loss: 0.0022906606600164824\n",
      "Time for epoch[s]: 0.3431863784790039\n",
      "Epoch 4: validation data loss: 0.00394625043215817, training data loss: 0.0012456377347310383\n",
      "Time for epoch[s]: 0.31336402893066406\n",
      "Epoch 5: validation data loss: 0.0034004180398705887, training data loss: 0.0009438537027193531\n",
      "Time for epoch[s]: 0.2784295082092285\n",
      "Epoch 6: validation data loss: 0.0030304087351446284, training data loss: 0.0007554049769492999\n",
      "Time for epoch[s]: 0.3035924434661865\n",
      "Epoch 7: validation data loss: 0.002806793609166254, training data loss: 0.0006076572420390229\n",
      "Time for epoch[s]: 0.38144874572753906\n",
      "Epoch 8: validation data loss: 0.002616472440223171, training data loss: 0.0004984356769143719\n",
      "Time for epoch[s]: 0.34003186225891113\n",
      "Epoch 9: validation data loss: 0.00243860564819754, training data loss: 0.0004168272902976432\n",
      "Time for epoch[s]: 0.30536794662475586\n",
      "Epoch 10: validation data loss: 0.002274291700424125, training data loss: 0.0003560352815340643\n",
      "Time for epoch[s]: 0.24979853630065918\n",
      "Epoch 11: validation data loss: 0.0021630273834211096, training data loss: 0.00031237096683075436\n",
      "Time for epoch[s]: 0.22742557525634766\n",
      "Epoch 12: validation data loss: 0.0020947024974648813, training data loss: 0.0002799715047285437\n",
      "Time for epoch[s]: 0.2166893482208252\n",
      "Epoch 13: validation data loss: 0.0019746194147083856, training data loss: 0.0002558973349937021\n",
      "Time for epoch[s]: 0.22659897804260254\n",
      "Epoch 14: validation data loss: 0.0019323457049452552, training data loss: 0.0002367326556003257\n",
      "Time for epoch[s]: 0.2617502212524414\n",
      "Epoch 15: validation data loss: 0.001855532736538752, training data loss: 0.00022200899853553946\n",
      "Time for epoch[s]: 0.24259305000305176\n",
      "Epoch 16: validation data loss: 0.0018002513336808715, training data loss: 0.00020871467922376172\n",
      "Time for epoch[s]: 0.24198102951049805\n",
      "Epoch 17: validation data loss: 0.0017652896713448443, training data loss: 0.0001981688416711816\n",
      "Time for epoch[s]: 0.21170568466186523\n",
      "Epoch 18: validation data loss: 0.0017439067091571687, training data loss: 0.00018930190230069096\n",
      "Time for epoch[s]: 0.19927263259887695\n",
      "Epoch 19: validation data loss: 0.0016775442857176201, training data loss: 0.00018111786477641972\n",
      "Time for epoch[s]: 0.23558640480041504\n",
      "Epoch 20: validation data loss: 0.0016865451314133596, training data loss: 0.00017293494994237543\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.23203063011169434\n",
      "Epoch 21: validation data loss: 0.0016400042462022338, training data loss: 0.00016582337076261164\n",
      "Time for epoch[s]: 0.21659064292907715\n",
      "Epoch 22: validation data loss: 0.0016224881013234456, training data loss: 0.00015982837505536536\n",
      "Time for epoch[s]: 0.2247769832611084\n",
      "Epoch 23: validation data loss: 0.0015908496020591422, training data loss: 0.00015340110958983366\n",
      "Time for epoch[s]: 0.2619340419769287\n",
      "Epoch 24: validation data loss: 0.001549896992504869, training data loss: 0.00014762064977867962\n",
      "Time for epoch[s]: 0.28298473358154297\n",
      "Epoch 25: validation data loss: 0.0015567444503035174, training data loss: 0.00014181826348718443\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2337350845336914\n",
      "Epoch 26: validation data loss: 0.0015347797304527944, training data loss: 0.00013700760390660533\n",
      "Time for epoch[s]: 0.23590373992919922\n",
      "Epoch 27: validation data loss: 0.00150409218383162, training data loss: 0.00013237366597402043\n",
      "Time for epoch[s]: 0.20784306526184082\n",
      "Epoch 28: validation data loss: 0.00148442999957359, training data loss: 0.00012909455148324574\n",
      "Time for epoch[s]: 0.2269275188446045\n",
      "Epoch 29: validation data loss: 0.0014765020374838075, training data loss: 0.000125125850036264\n",
      "Time for epoch[s]: 0.2684934139251709\n",
      "Epoch 30: validation data loss: 0.0014543289735436983, training data loss: 0.00012116432700255145\n",
      "Time for epoch[s]: 0.25428032875061035\n",
      "Epoch 31: validation data loss: 0.0014294851316164617, training data loss: 0.00011827920873959859\n",
      "Time for epoch[s]: 0.19385814666748047\n",
      "Epoch 32: validation data loss: 0.0014148302818542203, training data loss: 0.00011524721367718423\n",
      "Time for epoch[s]: 0.1765298843383789\n",
      "Epoch 33: validation data loss: 0.0014109038599005573, training data loss: 0.00011250494073515069\n",
      "Time for epoch[s]: 0.20035290718078613\n",
      "Epoch 34: validation data loss: 0.0014060210419572106, training data loss: 0.00010961470232434469\n",
      "Time for epoch[s]: 0.19353652000427246\n",
      "Epoch 35: validation data loss: 0.001377213082901419, training data loss: 0.0001073996855379784\n",
      "Time for epoch[s]: 0.24417519569396973\n",
      "Epoch 36: validation data loss: 0.0013803121161787477, training data loss: 0.00010499803000661336\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.19665980339050293\n",
      "Epoch 37: validation data loss: 0.0013538348620340704, training data loss: 0.00010281568180480503\n",
      "Time for epoch[s]: 0.19475913047790527\n",
      "Epoch 38: validation data loss: 0.0013488751568206368, training data loss: 0.00010051696505992924\n",
      "Time for epoch[s]: 0.21683740615844727\n",
      "Epoch 39: validation data loss: 0.0013412524303889166, training data loss: 9.855326077981627e-05\n",
      "Time for epoch[s]: 0.2673947811126709\n",
      "Epoch 40: validation data loss: 0.0013389090696970622, training data loss: 9.664106416647838e-05\n",
      "Time for epoch[s]: 0.26516246795654297\n",
      "Epoch 41: validation data loss: 0.001319207012925518, training data loss: 9.477300118637956e-05\n",
      "Time for epoch[s]: 0.21454071998596191\n",
      "Epoch 42: validation data loss: 0.0013137463837453764, training data loss: 9.27602132297542e-05\n",
      "Time for epoch[s]: 0.19203734397888184\n",
      "Epoch 43: validation data loss: 0.0013075974433933762, training data loss: 9.08185374927303e-05\n",
      "Time for epoch[s]: 0.17636847496032715\n",
      "Epoch 44: validation data loss: 0.0013059449794629936, training data loss: 8.931258463696258e-05\n",
      "Time for epoch[s]: 0.18190789222717285\n",
      "Epoch 45: validation data loss: 0.0013068367357123387, training data loss: 8.745189569039977e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2198655605316162\n",
      "Epoch 46: validation data loss: 0.001296615899969998, training data loss: 8.627738310321826e-05\n",
      "Time for epoch[s]: 0.208298921585083\n",
      "Epoch 47: validation data loss: 0.0012933304320731664, training data loss: 8.530389160326082e-05\n",
      "Time for epoch[s]: 0.21956396102905273\n",
      "Epoch 48: validation data loss: 0.0012809134781632794, training data loss: 8.287894024968692e-05\n",
      "Time for epoch[s]: 0.23254156112670898\n",
      "Epoch 49: validation data loss: 0.0012664579909686085, training data loss: 8.228843921123575e-05\n",
      "Time for epoch[s]: 0.26264381408691406\n",
      "Epoch 50: validation data loss: 0.001281207554960904, training data loss: 8.063727658088894e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2853832244873047\n",
      "Epoch 51: validation data loss: 0.0012793742358412372, training data loss: 7.933049067242504e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.27814292907714844\n",
      "Epoch 52: validation data loss: 0.0012620397626537166, training data loss: 7.754215570889651e-05\n",
      "Time for epoch[s]: 0.28075504302978516\n",
      "Epoch 53: validation data loss: 0.0012557299714110214, training data loss: 7.647049685591433e-05\n",
      "Time for epoch[s]: 0.24646568298339844\n",
      "Epoch 54: validation data loss: 0.0012533495959625941, training data loss: 7.552807495746439e-05\n",
      "Time for epoch[s]: 0.2281055450439453\n",
      "Epoch 55: validation data loss: 0.0012469810165771065, training data loss: 7.356968644547136e-05\n",
      "Time for epoch[s]: 0.21018338203430176\n",
      "Epoch 56: validation data loss: 0.0012425123012229189, training data loss: 7.289779887079648e-05\n",
      "Time for epoch[s]: 0.17654967308044434\n",
      "Epoch 57: validation data loss: 0.0012386706593918474, training data loss: 7.163356462297919e-05\n",
      "Time for epoch[s]: 0.1693115234375\n",
      "Epoch 58: validation data loss: 0.0012330303997753961, training data loss: 7.039397852878048e-05\n",
      "Time for epoch[s]: 0.15642499923706055\n",
      "Epoch 59: validation data loss: 0.0012297552742370186, training data loss: 6.976119799701047e-05\n",
      "Time for epoch[s]: 0.22176504135131836\n",
      "Epoch 60: validation data loss: 0.0012332218694904623, training data loss: 6.865748293595771e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.256894588470459\n",
      "Epoch 61: validation data loss: 0.0012360153949424013, training data loss: 6.754254097263562e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2702188491821289\n",
      "Epoch 62: validation data loss: 0.0012217621280722422, training data loss: 6.665972371896108e-05\n",
      "Time for epoch[s]: 0.22910308837890625\n",
      "Epoch 63: validation data loss: 0.0012299538203026062, training data loss: 6.599610747948085e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.19772005081176758\n",
      "Epoch 64: validation data loss: 0.0012254069929253566, training data loss: 6.488074450732366e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.1829836368560791\n",
      "Epoch 65: validation data loss: 0.0012102699987420209, training data loss: 6.394818016927536e-05\n",
      "Time for epoch[s]: 0.19719576835632324\n",
      "Epoch 66: validation data loss: 0.0012136458261916626, training data loss: 6.353235952386028e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2160642147064209\n",
      "Epoch 67: validation data loss: 0.0012115551728636162, training data loss: 6.249942072586381e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.23930597305297852\n",
      "Epoch 68: validation data loss: 0.0012136825687809078, training data loss: 6.194550918253589e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.21262335777282715\n",
      "Epoch 69: validation data loss: 0.0011957841648903068, training data loss: 6.106658307112516e-05\n",
      "Time for epoch[s]: 0.22918343544006348\n",
      "Epoch 70: validation data loss: 0.0012090134021898384, training data loss: 6.0644723713125815e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.24484992027282715\n",
      "Epoch 71: validation data loss: 0.0012049423233014807, training data loss: 5.963574414519959e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.22673583030700684\n",
      "Epoch 72: validation data loss: 0.0011860333621229755, training data loss: 5.8877315865531903e-05\n",
      "Time for epoch[s]: 0.2138352394104004\n",
      "Epoch 73: validation data loss: 0.0012034822816718114, training data loss: 5.8022268184515984e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.23137235641479492\n",
      "Epoch 74: validation data loss: 0.0011911071054467328, training data loss: 5.743738059719948e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3288083076477051\n",
      "Epoch 75: validation data loss: 0.0011791185429107108, training data loss: 5.695558914310856e-05\n",
      "Time for epoch[s]: 0.36827874183654785\n",
      "Epoch 76: validation data loss: 0.0011804415482908623, training data loss: 5.622351006285785e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.33554673194885254\n",
      "Epoch 77: validation data loss: 0.0011842248102301332, training data loss: 5.5597346599243545e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.27263760566711426\n",
      "Epoch 78: validation data loss: 0.0011865778328621224, training data loss: 5.503138314643407e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.19849681854248047\n",
      "Epoch 79: validation data loss: 0.0011764564742780712, training data loss: 5.434154103335725e-05\n",
      "Time for epoch[s]: 0.2007465362548828\n",
      "Epoch 80: validation data loss: 0.0011713802814483643, training data loss: 5.39378020657252e-05\n",
      "Time for epoch[s]: 0.21825122833251953\n",
      "Epoch 81: validation data loss: 0.0011635419984930727, training data loss: 5.338780718034805e-05\n",
      "Time for epoch[s]: 0.255138635635376\n",
      "Epoch 82: validation data loss: 0.001191474531339184, training data loss: 5.256402046849194e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.23784708976745605\n",
      "Epoch 83: validation data loss: 0.00116573076814277, training data loss: 5.2282152927085144e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.24220657348632812\n",
      "Epoch 84: validation data loss: 0.0011610961668023236, training data loss: 5.163982952838619e-05\n",
      "Time for epoch[s]: 0.1880626678466797\n",
      "Epoch 85: validation data loss: 0.001160159775111229, training data loss: 5.095884986391895e-05\n",
      "Time for epoch[s]: 0.2080385684967041\n",
      "Epoch 86: validation data loss: 0.0011565541023533094, training data loss: 5.0639244621474994e-05\n",
      "Time for epoch[s]: 0.20839858055114746\n",
      "Epoch 87: validation data loss: 0.0011650200031663729, training data loss: 4.9896189543210206e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2117633819580078\n",
      "Epoch 88: validation data loss: 0.0011660648535375725, training data loss: 4.9374487302074694e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.18125128746032715\n",
      "Epoch 89: validation data loss: 0.001172857741787009, training data loss: 4.876885120863239e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.1730203628540039\n",
      "Epoch 90: validation data loss: 0.0011569728317870397, training data loss: 4.818514584814577e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.17885136604309082\n",
      "Epoch 91: validation data loss: 0.0011612450423305982, training data loss: 4.771490024241138e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.20928549766540527\n",
      "Epoch 92: validation data loss: 0.0011530321210486705, training data loss: 4.714261740446091e-05\n",
      "Time for epoch[s]: 0.17850303649902344\n",
      "Epoch 93: validation data loss: 0.0011521266203492744, training data loss: 4.676182554464906e-05\n",
      "Time for epoch[s]: 0.16924595832824707\n",
      "Epoch 94: validation data loss: 0.0011457540945375346, training data loss: 4.604277647521398e-05\n",
      "Time for epoch[s]: 0.15971040725708008\n",
      "Epoch 95: validation data loss: 0.0011454255885729506, training data loss: 4.584577410868858e-05\n",
      "Time for epoch[s]: 0.21634817123413086\n",
      "Epoch 96: validation data loss: 0.0011522983579330792, training data loss: 4.53158260480454e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.21635174751281738\n",
      "Epoch 97: validation data loss: 0.0011439871842458369, training data loss: 4.500885957588344e-05\n",
      "Time for epoch[s]: 0.1991872787475586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-10 18:15:51,440]\u001B[0m Trial 10 finished with value: 0.0011358471357659117 and parameters: {'learning_rate': 0.2, 'ff_neurons_layer_00': 10, 'ff_neurons_layer_01': 10, 'early_stopping_epochs': 8, 'layer_activation_00': 'LeakyReLU'}. Best is trial 3 with value: 0.0011358471357659117.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98: validation data loss: 0.0011327926698885007, training data loss: 4.4318868621299256e-05\n",
      "Time for epoch[s]: 0.24115347862243652\n",
      "Epoch 99: validation data loss: 0.0011358471357659117, training data loss: 4.404742423802206e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.18630456924438477\n",
      "Final validation data loss:  0.0011358471357659117\n",
      "Initial Guess - validation data loss:  0.1280695684424274\n",
      "Epoch 0: validation data loss: 0.026301059548713302, training data loss: 0.058791334770585846\n",
      "Time for epoch[s]: 0.24640202522277832\n",
      "Epoch 1: validation data loss: 0.019513974995373592, training data loss: 0.017112139697488586\n",
      "Time for epoch[s]: 0.26499438285827637\n",
      "Epoch 2: validation data loss: 0.007752214936905256, training data loss: 0.013614544585415217\n",
      "Time for epoch[s]: 0.384152889251709\n",
      "Epoch 3: validation data loss: 0.004707333159773317, training data loss: 0.0022906606600164824\n",
      "Time for epoch[s]: 0.3207533359527588\n",
      "Epoch 4: validation data loss: 0.00394625043215817, training data loss: 0.0012456377347310383\n",
      "Time for epoch[s]: 0.31003856658935547\n",
      "Epoch 5: validation data loss: 0.0034004180398705887, training data loss: 0.0009438537027193531\n",
      "Time for epoch[s]: 0.3217611312866211\n",
      "Epoch 6: validation data loss: 0.0030304087351446284, training data loss: 0.0007554049769492999\n",
      "Time for epoch[s]: 0.2853672504425049\n",
      "Epoch 7: validation data loss: 0.002806793609166254, training data loss: 0.0006076572420390229\n",
      "Time for epoch[s]: 0.30922818183898926\n",
      "Epoch 8: validation data loss: 0.002616472440223171, training data loss: 0.0004984356769143719\n",
      "Time for epoch[s]: 0.2616922855377197\n",
      "Epoch 9: validation data loss: 0.00243860564819754, training data loss: 0.0004168272902976432\n",
      "Time for epoch[s]: 0.2506089210510254\n",
      "Epoch 10: validation data loss: 0.002274291700424125, training data loss: 0.0003560352815340643\n",
      "Time for epoch[s]: 0.24817490577697754\n",
      "Epoch 11: validation data loss: 0.0021630273834211096, training data loss: 0.00031237096683075436\n",
      "Time for epoch[s]: 0.28524136543273926\n",
      "Epoch 12: validation data loss: 0.0020947024974648813, training data loss: 0.0002799715047285437\n",
      "Time for epoch[s]: 0.34601330757141113\n",
      "Epoch 13: validation data loss: 0.0019746194147083856, training data loss: 0.0002558973349937021\n",
      "Time for epoch[s]: 0.2934291362762451\n",
      "Epoch 14: validation data loss: 0.0019323457049452552, training data loss: 0.0002367326556003257\n",
      "Time for epoch[s]: 0.22667384147644043\n",
      "Epoch 15: validation data loss: 0.001855532736538752, training data loss: 0.00022200899853553946\n",
      "Time for epoch[s]: 0.2249927520751953\n",
      "Epoch 16: validation data loss: 0.0018002513336808715, training data loss: 0.00020871467922376172\n",
      "Time for epoch[s]: 0.2053680419921875\n",
      "Epoch 17: validation data loss: 0.0017652896713448443, training data loss: 0.0001981688416711816\n",
      "Time for epoch[s]: 0.2225492000579834\n",
      "Epoch 18: validation data loss: 0.0017439067091571687, training data loss: 0.00018930190230069096\n",
      "Time for epoch[s]: 0.2637603282928467\n",
      "Epoch 19: validation data loss: 0.0016775442857176201, training data loss: 0.00018111786477641972\n",
      "Time for epoch[s]: 0.2633211612701416\n",
      "Epoch 20: validation data loss: 0.0016865451314133596, training data loss: 0.00017293494994237543\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.25367116928100586\n",
      "Epoch 21: validation data loss: 0.0016400042462022338, training data loss: 0.00016582337076261164\n",
      "Time for epoch[s]: 0.247467041015625\n",
      "Epoch 22: validation data loss: 0.0016224881013234456, training data loss: 0.00015982837505536536\n",
      "Time for epoch[s]: 0.2321469783782959\n",
      "Epoch 23: validation data loss: 0.0015908496020591422, training data loss: 0.00015340110958983366\n",
      "Time for epoch[s]: 0.2615032196044922\n",
      "Epoch 24: validation data loss: 0.001549896992504869, training data loss: 0.00014762064977867962\n",
      "Time for epoch[s]: 0.29664158821105957\n",
      "Epoch 25: validation data loss: 0.0015567444503035174, training data loss: 0.00014181826348718443\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3474464416503906\n",
      "Epoch 26: validation data loss: 0.0015347797304527944, training data loss: 0.00013700760390660533\n",
      "Time for epoch[s]: 0.3516581058502197\n",
      "Epoch 27: validation data loss: 0.00150409218383162, training data loss: 0.00013237366597402043\n",
      "Time for epoch[s]: 0.2926762104034424\n",
      "Epoch 28: validation data loss: 0.00148442999957359, training data loss: 0.00012909455148324574\n",
      "Time for epoch[s]: 0.26557421684265137\n",
      "Epoch 29: validation data loss: 0.0014765020374838075, training data loss: 0.000125125850036264\n",
      "Time for epoch[s]: 0.3122532367706299\n",
      "Epoch 30: validation data loss: 0.0014543289735436983, training data loss: 0.00012116432700255145\n",
      "Time for epoch[s]: 0.2844665050506592\n",
      "Epoch 31: validation data loss: 0.0014294851316164617, training data loss: 0.00011827920873959859\n",
      "Time for epoch[s]: 0.24478745460510254\n",
      "Epoch 32: validation data loss: 0.0014148302818542203, training data loss: 0.00011524721367718423\n",
      "Time for epoch[s]: 0.24774551391601562\n",
      "Epoch 33: validation data loss: 0.0014109038599005573, training data loss: 0.00011250494073515069\n",
      "Time for epoch[s]: 0.2569122314453125\n",
      "Epoch 34: validation data loss: 0.0014060210419572106, training data loss: 0.00010961470232434469\n",
      "Time for epoch[s]: 0.22883057594299316\n",
      "Epoch 35: validation data loss: 0.001377213082901419, training data loss: 0.0001073996855379784\n",
      "Time for epoch[s]: 0.22995448112487793\n",
      "Epoch 36: validation data loss: 0.0013803121161787477, training data loss: 0.00010499803000661336\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.23301219940185547\n",
      "Epoch 37: validation data loss: 0.0013538348620340704, training data loss: 0.00010281568180480503\n",
      "Time for epoch[s]: 0.19244623184204102\n",
      "Epoch 38: validation data loss: 0.0013488751568206368, training data loss: 0.00010051696505992924\n",
      "Time for epoch[s]: 0.19212794303894043\n",
      "Epoch 39: validation data loss: 0.0013412524303889166, training data loss: 9.855326077981627e-05\n",
      "Time for epoch[s]: 0.2209184169769287\n",
      "Epoch 40: validation data loss: 0.0013389090696970622, training data loss: 9.664106416647838e-05\n",
      "Time for epoch[s]: 0.21352076530456543\n",
      "Epoch 41: validation data loss: 0.001319207012925518, training data loss: 9.477300118637956e-05\n",
      "Time for epoch[s]: 0.2155294418334961\n",
      "Epoch 42: validation data loss: 0.0013137463837453764, training data loss: 9.27602132297542e-05\n",
      "Time for epoch[s]: 0.21845030784606934\n",
      "Epoch 43: validation data loss: 0.0013075974433933762, training data loss: 9.08185374927303e-05\n",
      "Time for epoch[s]: 0.22045087814331055\n",
      "Epoch 44: validation data loss: 0.0013059449794629936, training data loss: 8.931258463696258e-05\n",
      "Time for epoch[s]: 0.228165864944458\n",
      "Epoch 45: validation data loss: 0.0013068367357123387, training data loss: 8.745189569039977e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.22831964492797852\n",
      "Epoch 46: validation data loss: 0.001296615899969998, training data loss: 8.627738310321826e-05\n",
      "Time for epoch[s]: 0.24397039413452148\n",
      "Epoch 47: validation data loss: 0.0012933304320731664, training data loss: 8.530389160326082e-05\n",
      "Time for epoch[s]: 0.20040273666381836\n",
      "Epoch 48: validation data loss: 0.0012809134781632794, training data loss: 8.287894024968692e-05\n",
      "Time for epoch[s]: 0.1997358798980713\n",
      "Epoch 49: validation data loss: 0.0012664579909686085, training data loss: 8.228843921123575e-05\n",
      "Time for epoch[s]: 0.2702760696411133\n",
      "Epoch 50: validation data loss: 0.001281207554960904, training data loss: 8.063727658088894e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.24277997016906738\n",
      "Epoch 51: validation data loss: 0.0012793742358412372, training data loss: 7.933049067242504e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.21886563301086426\n",
      "Epoch 52: validation data loss: 0.0012620397626537166, training data loss: 7.754215570889651e-05\n",
      "Time for epoch[s]: 0.21439647674560547\n",
      "Epoch 53: validation data loss: 0.0012557299714110214, training data loss: 7.647049685591433e-05\n",
      "Time for epoch[s]: 0.17680597305297852\n",
      "Epoch 54: validation data loss: 0.0012533495959625941, training data loss: 7.552807495746439e-05\n",
      "Time for epoch[s]: 0.1776125431060791\n",
      "Epoch 55: validation data loss: 0.0012469810165771065, training data loss: 7.356968644547136e-05\n",
      "Time for epoch[s]: 0.17693257331848145\n",
      "Epoch 56: validation data loss: 0.0012425123012229189, training data loss: 7.289779887079648e-05\n",
      "Time for epoch[s]: 0.20517396926879883\n",
      "Epoch 57: validation data loss: 0.0012386706593918474, training data loss: 7.163356462297919e-05\n",
      "Time for epoch[s]: 0.2372267246246338\n",
      "Epoch 58: validation data loss: 0.0012330303997753961, training data loss: 7.039397852878048e-05\n",
      "Time for epoch[s]: 0.28824687004089355\n",
      "Epoch 59: validation data loss: 0.0012297552742370186, training data loss: 6.976119799701047e-05\n",
      "Time for epoch[s]: 0.27649950981140137\n",
      "Epoch 60: validation data loss: 0.0012332218694904623, training data loss: 6.865748293595771e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.20601224899291992\n",
      "Epoch 61: validation data loss: 0.0012360153949424013, training data loss: 6.754254097263562e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2143871784210205\n",
      "Epoch 62: validation data loss: 0.0012217621280722422, training data loss: 6.665972371896108e-05\n",
      "Time for epoch[s]: 0.22252368927001953\n",
      "Epoch 63: validation data loss: 0.0012299538203026062, training data loss: 6.599610747948085e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.23436474800109863\n",
      "Epoch 64: validation data loss: 0.0012254069929253566, training data loss: 6.488074450732366e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2487647533416748\n",
      "Epoch 65: validation data loss: 0.0012102699987420209, training data loss: 6.394818016927536e-05\n",
      "Time for epoch[s]: 0.23766255378723145\n",
      "Epoch 66: validation data loss: 0.0012136458261916626, training data loss: 6.353235952386028e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.22214484214782715\n",
      "Epoch 67: validation data loss: 0.0012115551728636162, training data loss: 6.249942072586381e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.20336604118347168\n",
      "Epoch 68: validation data loss: 0.0012136825687809078, training data loss: 6.194550918253589e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.18483328819274902\n",
      "Epoch 69: validation data loss: 0.0011957841648903068, training data loss: 6.106658307112516e-05\n",
      "Time for epoch[s]: 0.191633939743042\n",
      "Epoch 70: validation data loss: 0.0012090134021898384, training data loss: 6.0644723713125815e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.21527957916259766\n",
      "Epoch 71: validation data loss: 0.0012049423233014807, training data loss: 5.963574414519959e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.22816133499145508\n",
      "Epoch 72: validation data loss: 0.0011860333621229755, training data loss: 5.8877315865531903e-05\n",
      "Time for epoch[s]: 0.2304391860961914\n",
      "Epoch 73: validation data loss: 0.0012034822816718114, training data loss: 5.8022268184515984e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.22899150848388672\n",
      "Epoch 74: validation data loss: 0.0011911071054467328, training data loss: 5.743738059719948e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2787158489227295\n",
      "Epoch 75: validation data loss: 0.0011791185429107108, training data loss: 5.695558914310856e-05\n",
      "Time for epoch[s]: 0.27734827995300293\n",
      "Epoch 76: validation data loss: 0.0011804415482908623, training data loss: 5.622351006285785e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3465902805328369\n",
      "Epoch 77: validation data loss: 0.0011842248102301332, training data loss: 5.5597346599243545e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.28675174713134766\n",
      "Epoch 78: validation data loss: 0.0011865778328621224, training data loss: 5.503138314643407e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.23971915245056152\n",
      "Epoch 79: validation data loss: 0.0011764564742780712, training data loss: 5.434154103335725e-05\n",
      "Time for epoch[s]: 0.24811983108520508\n",
      "Epoch 80: validation data loss: 0.0011713802814483643, training data loss: 5.39378020657252e-05\n",
      "Time for epoch[s]: 0.23497796058654785\n",
      "Epoch 81: validation data loss: 0.0011635419984930727, training data loss: 5.338780718034805e-05\n",
      "Time for epoch[s]: 0.18916821479797363\n",
      "Epoch 82: validation data loss: 0.001191474531339184, training data loss: 5.256402046849194e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.20576024055480957\n",
      "Epoch 83: validation data loss: 0.00116573076814277, training data loss: 5.2282152927085144e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.20804190635681152\n",
      "Epoch 84: validation data loss: 0.0011610961668023236, training data loss: 5.163982952838619e-05\n",
      "Time for epoch[s]: 0.19226574897766113\n",
      "Epoch 85: validation data loss: 0.001160159775111229, training data loss: 5.095884986391895e-05\n",
      "Time for epoch[s]: 0.21164202690124512\n",
      "Epoch 86: validation data loss: 0.0011565541023533094, training data loss: 5.0639244621474994e-05\n",
      "Time for epoch[s]: 0.21561884880065918\n",
      "Epoch 87: validation data loss: 0.0011650200031663729, training data loss: 4.9896189543210206e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.19512057304382324\n",
      "Epoch 88: validation data loss: 0.0011660648535375725, training data loss: 4.9374487302074694e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.21548175811767578\n",
      "Epoch 89: validation data loss: 0.001172857741787009, training data loss: 4.876885120863239e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.23278498649597168\n",
      "Epoch 90: validation data loss: 0.0011569728317870397, training data loss: 4.818514584814577e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2227921485900879\n",
      "Epoch 91: validation data loss: 0.0011612450423305982, training data loss: 4.771490024241138e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.24954962730407715\n",
      "Epoch 92: validation data loss: 0.0011530321210486705, training data loss: 4.714261740446091e-05\n",
      "Time for epoch[s]: 0.23442482948303223\n",
      "Epoch 93: validation data loss: 0.0011521266203492744, training data loss: 4.676182554464906e-05\n",
      "Time for epoch[s]: 0.2551858425140381\n",
      "Epoch 94: validation data loss: 0.0011457540945375346, training data loss: 4.604277647521398e-05\n",
      "Time for epoch[s]: 0.2302074432373047\n",
      "Epoch 95: validation data loss: 0.0011454255885729506, training data loss: 4.584577410868858e-05\n",
      "Time for epoch[s]: 0.19000601768493652\n",
      "Epoch 96: validation data loss: 0.0011522983579330792, training data loss: 4.53158260480454e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.21031737327575684\n",
      "Epoch 97: validation data loss: 0.0011439871842458369, training data loss: 4.500885957588344e-05\n",
      "Time for epoch[s]: 0.22561097145080566\n",
      "Epoch 98: validation data loss: 0.0011327926698885007, training data loss: 4.4318868621299256e-05\n",
      "Time for epoch[s]: 0.21318531036376953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-10 18:16:15,843]\u001B[0m Trial 11 finished with value: 0.0011358471357659117 and parameters: {'learning_rate': 0.2, 'ff_neurons_layer_00': 10, 'ff_neurons_layer_01': 10, 'early_stopping_epochs': 8, 'layer_activation_00': 'LeakyReLU'}. Best is trial 3 with value: 0.0011358471357659117.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: validation data loss: 0.0011358471357659117, training data loss: 4.404742423802206e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.26014041900634766\n",
      "Final validation data loss:  0.0011358471357659117\n",
      "Initial Guess - validation data loss:  0.03549702091304135\n",
      "Epoch 0: validation data loss: 0.007443305564253298, training data loss: 0.007697469567599362\n",
      "Time for epoch[s]: 0.2884387969970703\n",
      "Epoch 1: validation data loss: 0.0071638642925105685, training data loss: 0.0036426956795121983\n",
      "Time for epoch[s]: 0.3213956356048584\n",
      "Epoch 2: validation data loss: 0.007025104135138804, training data loss: 0.0035155352936487765\n",
      "Time for epoch[s]: 0.33808207511901855\n",
      "Epoch 3: validation data loss: 0.006879740654061374, training data loss: 0.0034213610435729703\n",
      "Time for epoch[s]: 0.34950709342956543\n",
      "Epoch 4: validation data loss: 0.006718869622983889, training data loss: 0.0033213003585327704\n",
      "Time for epoch[s]: 0.28812074661254883\n",
      "Epoch 5: validation data loss: 0.006522619016638629, training data loss: 0.003205367963607997\n",
      "Time for epoch[s]: 0.2928507328033447\n",
      "Epoch 6: validation data loss: 0.006304848139688849, training data loss: 0.0030746800170097176\n",
      "Time for epoch[s]: 0.2952859401702881\n",
      "Epoch 7: validation data loss: 0.006058989050181489, training data loss: 0.0029296107488135767\n",
      "Time for epoch[s]: 0.2634310722351074\n",
      "Epoch 8: validation data loss: 0.005788205965468873, training data loss: 0.0027698811875086398\n",
      "Time for epoch[s]: 0.25748658180236816\n",
      "Epoch 9: validation data loss: 0.005499355324871464, training data loss: 0.002600625497565422\n",
      "Time for epoch[s]: 0.21006035804748535\n",
      "Epoch 10: validation data loss: 0.005202898151798335, training data loss: 0.0024287782303274493\n",
      "Time for epoch[s]: 0.22391676902770996\n",
      "Epoch 11: validation data loss: 0.004915687591517897, training data loss: 0.002263580010906202\n",
      "Time for epoch[s]: 0.2379131317138672\n",
      "Epoch 12: validation data loss: 0.004639941263416586, training data loss: 0.0021087559935164778\n",
      "Time for epoch[s]: 0.2392103672027588\n",
      "Epoch 13: validation data loss: 0.004395792745564082, training data loss: 0.0019706848277348905\n",
      "Time for epoch[s]: 0.22881245613098145\n",
      "Epoch 14: validation data loss: 0.004181529016799578, training data loss: 0.0018515913453820633\n",
      "Time for epoch[s]: 0.23787689208984375\n",
      "Epoch 15: validation data loss: 0.003995666220852229, training data loss: 0.0017517237630608963\n",
      "Time for epoch[s]: 0.23196125030517578\n",
      "Epoch 16: validation data loss: 0.003842398184075203, training data loss: 0.0016677107440826555\n",
      "Time for epoch[s]: 0.22798728942871094\n",
      "Epoch 17: validation data loss: 0.003710637898205622, training data loss: 0.0015981099376939747\n",
      "Time for epoch[s]: 0.2473585605621338\n",
      "Epoch 18: validation data loss: 0.003597069004354956, training data loss: 0.00153721453936677\n",
      "Time for epoch[s]: 0.2341444492340088\n",
      "Epoch 19: validation data loss: 0.003498791287478791, training data loss: 0.0014853131825521113\n",
      "Time for epoch[s]: 0.23687410354614258\n",
      "Epoch 20: validation data loss: 0.003413145128450437, training data loss: 0.0014393649416971424\n",
      "Time for epoch[s]: 0.2700188159942627\n",
      "Epoch 21: validation data loss: 0.0033339044274804798, training data loss: 0.0013983055064667306\n",
      "Time for epoch[s]: 0.38960957527160645\n",
      "Epoch 22: validation data loss: 0.0032672386735541634, training data loss: 0.0013612557763922704\n",
      "Time for epoch[s]: 0.37534546852111816\n",
      "Epoch 23: validation data loss: 0.0032018790506336786, training data loss: 0.0013270594485818522\n",
      "Time for epoch[s]: 0.33835506439208984\n",
      "Epoch 24: validation data loss: 0.0031411767550255064, training data loss: 0.0012961205554335084\n",
      "Time for epoch[s]: 0.3436295986175537\n",
      "Epoch 25: validation data loss: 0.003082800945734869, training data loss: 0.001266134111848596\n",
      "Time for epoch[s]: 0.30594396591186523\n",
      "Epoch 26: validation data loss: 0.003029640406778414, training data loss: 0.0012382064780143843\n",
      "Time for epoch[s]: 0.35165834426879883\n",
      "Epoch 27: validation data loss: 0.0029751329117169664, training data loss: 0.0012114382769963512\n",
      "Time for epoch[s]: 0.24822664260864258\n",
      "Epoch 28: validation data loss: 0.002927757289311657, training data loss: 0.001187705966435611\n",
      "Time for epoch[s]: 0.21934962272644043\n",
      "Epoch 29: validation data loss: 0.0028829629018426485, training data loss: 0.0011623216001954797\n",
      "Time for epoch[s]: 0.2137281894683838\n",
      "Epoch 30: validation data loss: 0.00283679352503389, training data loss: 0.001139737223381321\n",
      "Time for epoch[s]: 0.2352454662322998\n",
      "Epoch 31: validation data loss: 0.002793749445649587, training data loss: 0.0011163179052474837\n",
      "Time for epoch[s]: 0.21230149269104004\n",
      "Epoch 32: validation data loss: 0.002748327984657462, training data loss: 0.0010934054307197327\n",
      "Time for epoch[s]: 0.18869638442993164\n",
      "Epoch 33: validation data loss: 0.002708469896011701, training data loss: 0.0010715162373024579\n",
      "Time for epoch[s]: 0.19077730178833008\n",
      "Epoch 34: validation data loss: 0.0026669551248419774, training data loss: 0.0010516862618868753\n",
      "Time for epoch[s]: 0.20203375816345215\n",
      "Epoch 35: validation data loss: 0.002625709799326718, training data loss: 0.0010301848388698004\n",
      "Time for epoch[s]: 0.2169032096862793\n",
      "Epoch 36: validation data loss: 0.002588147442090457, training data loss: 0.0010093424009950194\n",
      "Time for epoch[s]: 0.2666008472442627\n",
      "Epoch 37: validation data loss: 0.0025500377563581074, training data loss: 0.0009896366019227188\n",
      "Time for epoch[s]: 0.29352903366088867\n",
      "Epoch 38: validation data loss: 0.0025177840228494445, training data loss: 0.0009698837859445511\n",
      "Time for epoch[s]: 0.2554194927215576\n",
      "Epoch 39: validation data loss: 0.0024796645390932964, training data loss: 0.0009507334123463391\n",
      "Time for epoch[s]: 0.2021024227142334\n",
      "Epoch 40: validation data loss: 0.0024405513724235638, training data loss: 0.0009316050843016742\n",
      "Time for epoch[s]: 0.23657631874084473\n",
      "Epoch 41: validation data loss: 0.002408577154760491, training data loss: 0.0009138485476306585\n",
      "Time for epoch[s]: 0.2583122253417969\n",
      "Epoch 42: validation data loss: 0.0023774112740608112, training data loss: 0.0008962827322145575\n",
      "Time for epoch[s]: 0.2373809814453125\n",
      "Epoch 43: validation data loss: 0.0023423139363119047, training data loss: 0.0008785536844436436\n",
      "Time for epoch[s]: 0.22099947929382324\n",
      "Epoch 44: validation data loss: 0.0023118069727126865, training data loss: 0.0008607694547470302\n",
      "Time for epoch[s]: 0.20542073249816895\n",
      "Epoch 45: validation data loss: 0.00228000313179678, training data loss: 0.0008456744423739987\n",
      "Time for epoch[s]: 0.20955944061279297\n",
      "Epoch 46: validation data loss: 0.0022467286347254227, training data loss: 0.0008290993160308768\n",
      "Time for epoch[s]: 0.21635890007019043\n",
      "Epoch 47: validation data loss: 0.002218099353520293, training data loss: 0.000812790641501614\n",
      "Time for epoch[s]: 0.2089688777923584\n",
      "Epoch 48: validation data loss: 0.0021856789175233884, training data loss: 0.0007977320995504998\n",
      "Time for epoch[s]: 0.26495957374572754\n",
      "Epoch 49: validation data loss: 0.0021592476596570993, training data loss: 0.0007830562929040221\n",
      "Time for epoch[s]: 0.33386921882629395\n",
      "Epoch 50: validation data loss: 0.002130861696042971, training data loss: 0.0007716652737360567\n",
      "Time for epoch[s]: 0.3176572322845459\n",
      "Epoch 51: validation data loss: 0.0021004280815385794, training data loss: 0.0007542797011327526\n",
      "Time for epoch[s]: 0.29962682723999023\n",
      "Epoch 52: validation data loss: 0.002074578989586329, training data loss: 0.0007408306206742379\n",
      "Time for epoch[s]: 0.25612902641296387\n",
      "Epoch 53: validation data loss: 0.0020474316594807523, training data loss: 0.0007281929390615524\n",
      "Time for epoch[s]: 0.2088019847869873\n",
      "Epoch 54: validation data loss: 0.0020204914487115867, training data loss: 0.0007148449687652936\n",
      "Time for epoch[s]: 0.22801494598388672\n",
      "Epoch 55: validation data loss: 0.0019971381311547267, training data loss: 0.0007031386437481397\n",
      "Time for epoch[s]: 0.24079680442810059\n",
      "Epoch 56: validation data loss: 0.001969922214882559, training data loss: 0.0006910336616376764\n",
      "Time for epoch[s]: 0.21311426162719727\n",
      "Epoch 57: validation data loss: 0.0019458608540225792, training data loss: 0.0006804113790869168\n",
      "Time for epoch[s]: 0.2385399341583252\n",
      "Epoch 58: validation data loss: 0.0019225034539558027, training data loss: 0.0006685305948126806\n",
      "Time for epoch[s]: 0.2593381404876709\n",
      "Epoch 59: validation data loss: 0.001898168292764115, training data loss: 0.000657905318421316\n",
      "Time for epoch[s]: 0.2449946403503418\n",
      "Epoch 60: validation data loss: 0.0018783287914920616, training data loss: 0.0006471685214674091\n",
      "Time for epoch[s]: 0.2087092399597168\n",
      "Epoch 61: validation data loss: 0.001854761958666588, training data loss: 0.0006384598473980003\n",
      "Time for epoch[s]: 0.17737460136413574\n",
      "Epoch 62: validation data loss: 0.0018338139198686434, training data loss: 0.0006281396025392018\n",
      "Time for epoch[s]: 0.16637873649597168\n",
      "Epoch 63: validation data loss: 0.0018127791957768132, training data loss: 0.0006188280097970135\n",
      "Time for epoch[s]: 0.17922043800354004\n",
      "Epoch 64: validation data loss: 0.001791358946665237, training data loss: 0.0006100429247503412\n",
      "Time for epoch[s]: 0.2029130458831787\n",
      "Epoch 65: validation data loss: 0.001772913894696867, training data loss: 0.0006021755198909812\n",
      "Time for epoch[s]: 0.20631957054138184\n",
      "Epoch 66: validation data loss: 0.0017532319783075759, training data loss: 0.0005938301211622753\n",
      "Time for epoch[s]: 0.19691085815429688\n",
      "Epoch 67: validation data loss: 0.0017337686122824612, training data loss: 0.000585280664979595\n",
      "Time for epoch[s]: 0.18470144271850586\n",
      "Epoch 68: validation data loss: 0.0017139368677792483, training data loss: 0.0005817103739742819\n",
      "Time for epoch[s]: 0.3256978988647461\n",
      "Epoch 69: validation data loss: 0.0016958697201454476, training data loss: 0.0005702518573090366\n",
      "Time for epoch[s]: 0.3048129081726074\n",
      "Epoch 70: validation data loss: 0.001679880978309945, training data loss: 0.000566069291879053\n",
      "Time for epoch[s]: 0.2609086036682129\n",
      "Epoch 71: validation data loss: 0.0016627348449132213, training data loss: 0.0005560039997645165\n",
      "Time for epoch[s]: 0.20427274703979492\n",
      "Epoch 72: validation data loss: 0.0016444592171063707, training data loss: 0.0005488428010788138\n",
      "Time for epoch[s]: 0.18619012832641602\n",
      "Epoch 73: validation data loss: 0.0016276930292991743, training data loss: 0.0005426583088696275\n",
      "Time for epoch[s]: 0.17541766166687012\n",
      "Epoch 74: validation data loss: 0.0016128182683361175, training data loss: 0.0005371933930540738\n",
      "Time for epoch[s]: 0.29219937324523926\n",
      "Epoch 75: validation data loss: 0.0015975208315130783, training data loss: 0.0005302034896802685\n",
      "Time for epoch[s]: 0.2585470676422119\n",
      "Epoch 76: validation data loss: 0.0015805022085094017, training data loss: 0.0005248145765909865\n",
      "Time for epoch[s]: 0.250014066696167\n",
      "Epoch 77: validation data loss: 0.0015662934409973284, training data loss: 0.0005187919899208905\n",
      "Time for epoch[s]: 0.22830748558044434\n",
      "Epoch 78: validation data loss: 0.0015518387703046406, training data loss: 0.0005132076947112062\n",
      "Time for epoch[s]: 0.20905804634094238\n",
      "Epoch 79: validation data loss: 0.001537293603975479, training data loss: 0.0005076976331401633\n",
      "Time for epoch[s]: 0.19889497756958008\n",
      "Epoch 80: validation data loss: 0.001521077602421312, training data loss: 0.0005026462075372809\n",
      "Time for epoch[s]: 0.24686574935913086\n",
      "Epoch 81: validation data loss: 0.0015106397132350973, training data loss: 0.0004983626340077892\n",
      "Time for epoch[s]: 0.2945408821105957\n",
      "Epoch 82: validation data loss: 0.0014961507494591143, training data loss: 0.0004923098367643139\n",
      "Time for epoch[s]: 0.2560744285583496\n",
      "Epoch 83: validation data loss: 0.0014821382145903427, training data loss: 0.00048781138712957026\n",
      "Time for epoch[s]: 0.26062464714050293\n",
      "Epoch 84: validation data loss: 0.0014681611975578413, training data loss: 0.0004827522387787632\n",
      "Time for epoch[s]: 0.20510506629943848\n",
      "Epoch 85: validation data loss: 0.0014575129230272824, training data loss: 0.00047828746985082755\n",
      "Time for epoch[s]: 0.1848769187927246\n",
      "Epoch 86: validation data loss: 0.001447451849506326, training data loss: 0.0004736040825168836\n",
      "Time for epoch[s]: 0.17428159713745117\n",
      "Epoch 87: validation data loss: 0.0014320620118755184, training data loss: 0.00046935007316336783\n",
      "Time for epoch[s]: 0.22294116020202637\n",
      "Epoch 88: validation data loss: 0.0014204339349650902, training data loss: 0.0004653590795111983\n",
      "Time for epoch[s]: 0.21448087692260742\n",
      "Epoch 89: validation data loss: 0.0014109265858724238, training data loss: 0.000461349918689902\n",
      "Time for epoch[s]: 0.18950486183166504\n",
      "Epoch 90: validation data loss: 0.0013987496835456048, training data loss: 0.00045703650881710663\n",
      "Time for epoch[s]: 0.1734910011291504\n",
      "Epoch 91: validation data loss: 0.0013855370484530654, training data loss: 0.00045313066951760417\n",
      "Time for epoch[s]: 0.17304039001464844\n",
      "Epoch 92: validation data loss: 0.0013774623881736303, training data loss: 0.0004495649373150307\n",
      "Time for epoch[s]: 0.23110461235046387\n",
      "Epoch 93: validation data loss: 0.0013653280800336029, training data loss: 0.0004467215712211992\n",
      "Time for epoch[s]: 0.22246146202087402\n",
      "Epoch 94: validation data loss: 0.0013506394815227213, training data loss: 0.0004415577745328755\n",
      "Time for epoch[s]: 0.21112442016601562\n",
      "Epoch 95: validation data loss: 0.001347849494246043, training data loss: 0.0004387247848184141\n",
      "Time for epoch[s]: 0.1875624656677246\n",
      "Epoch 96: validation data loss: 0.0013399551448212367, training data loss: 0.0004350452186310128\n",
      "Time for epoch[s]: 0.18083524703979492\n",
      "Epoch 97: validation data loss: 0.0013284100789457695, training data loss: 0.000432307453460345\n",
      "Time for epoch[s]: 0.26027607917785645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-10 18:16:40,116]\u001B[0m Trial 12 finished with value: 0.0013101344511389189 and parameters: {'learning_rate': 0.5, 'ff_neurons_layer_00': 10, 'ff_neurons_layer_01': 10, 'early_stopping_epochs': 8, 'layer_activation_00': 'Sigmoid'}. Best is trial 3 with value: 0.0011358471357659117.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98: validation data loss: 0.0013185415638091902, training data loss: 0.0004284260071575914\n",
      "Time for epoch[s]: 0.2132401466369629\n",
      "Epoch 99: validation data loss: 0.0013101344511389189, training data loss: 0.00042503252285256234\n",
      "Time for epoch[s]: 0.19807982444763184\n",
      "Final validation data loss:  0.0013101344511389189\n",
      "Initial Guess - validation data loss:  0.12583412971670768\n",
      "Epoch 0: validation data loss: 0.04873248757837025, training data loss: 0.053158407341944026\n",
      "Time for epoch[s]: 0.3576481342315674\n",
      "Epoch 1: validation data loss: 0.047858508210203966, training data loss: 0.04463234121940996\n",
      "Time for epoch[s]: 0.4627397060394287\n",
      "Epoch 2: validation data loss: 0.047530753427444526, training data loss: 0.044215402646696184\n",
      "Time for epoch[s]: 0.6609609127044678\n",
      "Epoch 3: validation data loss: 0.04721094593065515, training data loss: 0.04397757630369979\n",
      "Time for epoch[s]: 0.5270845890045166\n",
      "Epoch 4: validation data loss: 0.046956550040745844, training data loss: 0.04378965456191808\n",
      "Time for epoch[s]: 0.3745720386505127\n",
      "Epoch 5: validation data loss: 0.028033289190841047, training data loss: 0.0331163079771277\n",
      "Time for epoch[s]: 0.461362361907959\n",
      "Epoch 6: validation data loss: 0.02772103923640839, training data loss: 0.024995975842758946\n",
      "Time for epoch[s]: 0.5023846626281738\n",
      "Epoch 7: validation data loss: 0.027599393504939666, training data loss: 0.024800359386287323\n",
      "Time for epoch[s]: 0.4457356929779053\n",
      "Epoch 8: validation data loss: 0.004884421553241608, training data loss: 0.0227335011033707\n",
      "Time for epoch[s]: 0.41987037658691406\n",
      "Epoch 9: validation data loss: 0.0025155380980609213, training data loss: 0.0008584548077082525\n",
      "Time for epoch[s]: 0.419018030166626\n",
      "Epoch 10: validation data loss: 0.002452774407112435, training data loss: 0.0005001173307906548\n",
      "Time for epoch[s]: 0.36574840545654297\n",
      "Epoch 11: validation data loss: 0.002378144220674419, training data loss: 0.00040624351942375914\n",
      "Time for epoch[s]: 0.3113226890563965\n",
      "Epoch 12: validation data loss: 0.0022637540619122928, training data loss: 0.0003413314416528292\n",
      "Time for epoch[s]: 0.3873937129974365\n",
      "Epoch 13: validation data loss: 0.0023171915311247246, training data loss: 0.0002931708546533976\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3688056468963623\n",
      "Epoch 14: validation data loss: 0.0023124335019011474, training data loss: 0.00025468420016166825\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.28377604484558105\n",
      "Epoch 15: validation data loss: 0.002312558426704581, training data loss: 0.00022545191523146957\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.30959081649780273\n",
      "Epoch 16: validation data loss: 0.002249645996311484, training data loss: 0.00020113764288218599\n",
      "Time for epoch[s]: 0.3344752788543701\n",
      "Epoch 17: validation data loss: 0.002244264295656387, training data loss: 0.000181665108220218\n",
      "Time for epoch[s]: 0.3366689682006836\n",
      "Epoch 18: validation data loss: 0.0022579402956244064, training data loss: 0.00016520942931305872\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4695892333984375\n",
      "Epoch 19: validation data loss: 0.002243276736507677, training data loss: 0.00015199179314587214\n",
      "Time for epoch[s]: 0.36649131774902344\n",
      "Epoch 20: validation data loss: 0.002260828943557391, training data loss: 0.00013958104805314922\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.30776023864746094\n",
      "Epoch 21: validation data loss: 0.00225360453400982, training data loss: 0.00013007215951403527\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.29582691192626953\n",
      "Epoch 22: validation data loss: 0.0022367315205264852, training data loss: 0.00012142621150844173\n",
      "Time for epoch[s]: 0.32697057723999023\n",
      "Epoch 23: validation data loss: 0.002236852771070994, training data loss: 0.00011413248434458694\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3102152347564697\n",
      "Epoch 24: validation data loss: 0.002262283950091497, training data loss: 0.00010723292351313377\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.29846882820129395\n",
      "Epoch 25: validation data loss: 0.0021970678954363956, training data loss: 0.00010073782034116248\n",
      "Time for epoch[s]: 0.4089949131011963\n",
      "Epoch 26: validation data loss: 0.0022440627557501944, training data loss: 9.574268234374861e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.33598875999450684\n",
      "Epoch 27: validation data loss: 0.0022652074354424325, training data loss: 9.078876919125858e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.30202341079711914\n",
      "Epoch 28: validation data loss: 0.002228042170337346, training data loss: 8.695548800028623e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3149833679199219\n",
      "Epoch 29: validation data loss: 0.0022383873865484648, training data loss: 8.31674461223219e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3701653480529785\n",
      "Epoch 30: validation data loss: 0.002246328140502651, training data loss: 7.985578212019515e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.31386733055114746\n",
      "Epoch 31: validation data loss: 0.002233256352002218, training data loss: 7.68584033397779e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2962832450866699\n",
      "Epoch 32: validation data loss: 0.00224651198953254, training data loss: 7.40213396206294e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.33843207359313965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-10 18:16:53,014]\u001B[0m Trial 13 finished with value: 0.002215350463510104 and parameters: {'learning_rate': 0.1, 'ff_neurons_layer_00': 200, 'ff_neurons_layer_01': 10, 'early_stopping_epochs': 8, 'layer_activation_00': 'LeakyReLU'}. Best is trial 3 with value: 0.0011358471357659117.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: validation data loss: 0.002215350463510104, training data loss: 7.152005383685299e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Stopping the training, validation accuracy has not improved for 8 epochs.\n",
      "Final validation data loss:  0.002215350463510104\n",
      "Initial Guess - validation data loss:  0.1274287602672838\n",
      "Epoch 0: validation data loss: 0.04322640754316495, training data loss: 0.0699090304440015\n",
      "Time for epoch[s]: 0.3641016483306885\n",
      "Epoch 1: validation data loss: 0.036454901847665165, training data loss: 0.03378032440464246\n",
      "Time for epoch[s]: 0.35767507553100586\n",
      "Epoch 2: validation data loss: 0.03553532247673975, training data loss: 0.03209539744407619\n",
      "Time for epoch[s]: 0.286327600479126\n",
      "Epoch 3: validation data loss: 0.03361558696450708, training data loss: 0.031105461730260284\n",
      "Time for epoch[s]: 0.30263710021972656\n",
      "Epoch 4: validation data loss: 0.021796770836120326, training data loss: 0.022263585704646698\n",
      "Time for epoch[s]: 0.39817333221435547\n",
      "Epoch 5: validation data loss: 0.020949749097432177, training data loss: 0.01805545205939306\n",
      "Time for epoch[s]: 0.4401981830596924\n",
      "Epoch 6: validation data loss: 0.020424773159636755, training data loss: 0.01777212804855277\n",
      "Time for epoch[s]: 0.35431790351867676\n",
      "Epoch 7: validation data loss: 0.02007505338485927, training data loss: 0.017614988431538622\n",
      "Time for epoch[s]: 0.2622804641723633\n",
      "Epoch 8: validation data loss: 0.019811096801061065, training data loss: 0.017511031399034475\n",
      "Time for epoch[s]: 0.23339319229125977\n",
      "Epoch 9: validation data loss: 0.01962592503795885, training data loss: 0.017433678178482404\n",
      "Time for epoch[s]: 0.18872833251953125\n",
      "Epoch 10: validation data loss: 0.019466977141219186, training data loss: 0.017371065540400816\n",
      "Time for epoch[s]: 0.17404460906982422\n",
      "Epoch 11: validation data loss: 0.019360953814362827, training data loss: 0.017321359077000727\n",
      "Time for epoch[s]: 0.21207404136657715\n",
      "Epoch 12: validation data loss: 0.01928084081710746, training data loss: 0.01727391813443676\n",
      "Time for epoch[s]: 0.19861316680908203\n",
      "Epoch 13: validation data loss: 0.019161712089085688, training data loss: 0.017233769098917644\n",
      "Time for epoch[s]: 0.1828160285949707\n",
      "Epoch 14: validation data loss: 0.019106860574521975, training data loss: 0.017195919332983287\n",
      "Time for epoch[s]: 0.22197532653808594\n",
      "Epoch 15: validation data loss: 0.019030285752527246, training data loss: 0.017162380697520355\n",
      "Time for epoch[s]: 0.2070765495300293\n",
      "Epoch 16: validation data loss: 0.01896370926948443, training data loss: 0.017133105291079167\n",
      "Time for epoch[s]: 0.19624638557434082\n",
      "Epoch 17: validation data loss: 0.018936906231048444, training data loss: 0.017105686065813176\n",
      "Time for epoch[s]: 0.18625426292419434\n",
      "Epoch 18: validation data loss: 0.01891232190066821, training data loss: 0.01708690216552177\n",
      "Time for epoch[s]: 0.21329021453857422\n",
      "Epoch 19: validation data loss: 0.01884421257123555, training data loss: 0.017070107264061496\n",
      "Time for epoch[s]: 0.21348905563354492\n",
      "Epoch 20: validation data loss: 0.01885559134287377, training data loss: 0.01705477226814723\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.18532991409301758\n",
      "Epoch 21: validation data loss: 0.01881026568478101, training data loss: 0.017044499584528954\n",
      "Time for epoch[s]: 0.1943652629852295\n",
      "Epoch 22: validation data loss: 0.018801179650711687, training data loss: 0.01703418335413824\n",
      "Time for epoch[s]: 0.1981792449951172\n",
      "Epoch 23: validation data loss: 0.018772443135579426, training data loss: 0.017025700442867192\n",
      "Time for epoch[s]: 0.1865241527557373\n",
      "Epoch 24: validation data loss: 0.018737418466507026, training data loss: 0.01701618547308935\n",
      "Time for epoch[s]: 0.23465275764465332\n",
      "Epoch 25: validation data loss: 0.018740057400916808, training data loss: 0.017011132958817156\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.29909491539001465\n",
      "Epoch 26: validation data loss: 0.018721406318281338, training data loss: 0.01700690238987474\n",
      "Time for epoch[s]: 0.2689785957336426\n",
      "Epoch 27: validation data loss: 0.018709903438341672, training data loss: 0.01699981406399104\n",
      "Time for epoch[s]: 0.22858285903930664\n",
      "Epoch 28: validation data loss: 0.018691195744902033, training data loss: 0.016993307087519397\n",
      "Time for epoch[s]: 0.24163174629211426\n",
      "Epoch 29: validation data loss: 0.01868989805108336, training data loss: 0.016990636581699598\n",
      "Time for epoch[s]: 0.2608482837677002\n",
      "Epoch 30: validation data loss: 0.01865854655226616, training data loss: 0.016984326654373237\n",
      "Time for epoch[s]: 0.23137402534484863\n",
      "Epoch 31: validation data loss: 0.018649284153768462, training data loss: 0.016984041423013765\n",
      "Time for epoch[s]: 0.1883680820465088\n",
      "Epoch 32: validation data loss: 0.01863077895282066, training data loss: 0.0169809376268082\n",
      "Time for epoch[s]: 0.2064526081085205\n",
      "Epoch 33: validation data loss: 0.01861453818403967, training data loss: 0.016978292160382553\n",
      "Time for epoch[s]: 0.20418119430541992\n",
      "Epoch 34: validation data loss: 0.018624958926684237, training data loss: 0.01697408663083429\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.23583436012268066\n",
      "Epoch 35: validation data loss: 0.01859706939627591, training data loss: 0.016970379711830453\n",
      "Time for epoch[s]: 0.20606756210327148\n",
      "Epoch 36: validation data loss: 0.018595871860033846, training data loss: 0.016969481559648905\n",
      "Time for epoch[s]: 0.1807246208190918\n",
      "Epoch 37: validation data loss: 0.018580140588490387, training data loss: 0.016968500668599725\n",
      "Time for epoch[s]: 0.17577862739562988\n",
      "Epoch 38: validation data loss: 0.018583045158212044, training data loss: 0.016963419848925448\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.17177844047546387\n",
      "Epoch 39: validation data loss: 0.018572067016880262, training data loss: 0.01696147330819744\n",
      "Time for epoch[s]: 0.21831321716308594\n",
      "Epoch 40: validation data loss: 0.018581203129737888, training data loss: 0.01696080704257913\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.23085641860961914\n",
      "Epoch 41: validation data loss: 0.01855299570789076, training data loss: 0.016958444630174332\n",
      "Time for epoch[s]: 0.20043206214904785\n",
      "Epoch 42: validation data loss: 0.01856708526611328, training data loss: 0.016957376645580273\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.1886298656463623\n",
      "Epoch 43: validation data loss: 0.018556703715563908, training data loss: 0.016955051247932052\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2295234203338623\n",
      "Epoch 44: validation data loss: 0.018560283260258364, training data loss: 0.016955215636998006\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.22065329551696777\n",
      "Epoch 45: validation data loss: 0.018551029571115155, training data loss: 0.016952292559898063\n",
      "Time for epoch[s]: 0.20310354232788086\n",
      "Epoch 46: validation data loss: 0.018551447620130564, training data loss: 0.016952142323533148\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.1893925666809082\n",
      "Epoch 47: validation data loss: 0.018556115834135987, training data loss: 0.016948944901766842\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.18490314483642578\n",
      "Epoch 48: validation data loss: 0.018543574363673657, training data loss: 0.016947011425070567\n",
      "Time for epoch[s]: 0.21374845504760742\n",
      "Epoch 49: validation data loss: 0.018543263004250722, training data loss: 0.016947769138911\n",
      "Time for epoch[s]: 0.24893999099731445\n",
      "Epoch 50: validation data loss: 0.018561328382796893, training data loss: 0.016947148597403747\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.20152997970581055\n",
      "Epoch 51: validation data loss: 0.018552788860721675, training data loss: 0.016945390396466538\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.1833512783050537\n",
      "Epoch 52: validation data loss: 0.018538607854277033, training data loss: 0.016943415550336446\n",
      "Time for epoch[s]: 0.19077563285827637\n",
      "Epoch 53: validation data loss: 0.018534947748053563, training data loss: 0.01694189903398627\n",
      "Time for epoch[s]: 0.2076585292816162\n",
      "Epoch 54: validation data loss: 0.018529325859731736, training data loss: 0.016942536994202496\n",
      "Time for epoch[s]: 0.25143885612487793\n",
      "Epoch 55: validation data loss: 0.018524278788806095, training data loss: 0.016939733670726757\n",
      "Time for epoch[s]: 0.23870849609375\n",
      "Epoch 56: validation data loss: 0.01853085435144433, training data loss: 0.016940134301033194\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2386455535888672\n",
      "Epoch 57: validation data loss: 0.01853111127740172, training data loss: 0.016938762577701376\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.18378019332885742\n",
      "Epoch 58: validation data loss: 0.01852211886889314, training data loss: 0.016936751805484023\n",
      "Time for epoch[s]: 0.18291354179382324\n",
      "Epoch 59: validation data loss: 0.01852578115245523, training data loss: 0.016939265542923042\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.22271156311035156\n",
      "Epoch 60: validation data loss: 0.018531455296903986, training data loss: 0.016935652249479946\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3667318820953369\n",
      "Epoch 61: validation data loss: 0.018541057360226706, training data loss: 0.016934541806782763\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3258702754974365\n",
      "Epoch 62: validation data loss: 0.018535189432640597, training data loss: 0.016935053481358917\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2475261688232422\n",
      "Epoch 63: validation data loss: 0.01853919355836633, training data loss: 0.01693319730018372\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.19439291954040527\n",
      "Epoch 64: validation data loss: 0.01853572288060297, training data loss: 0.016933405236022114\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.18630218505859375\n",
      "Epoch 65: validation data loss: 0.01852386291712931, training data loss: 0.016932093389502398\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.20367693901062012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-10 18:17:08,599]\u001B[0m Trial 14 finished with value: 0.018540384562592528 and parameters: {'learning_rate': 0.2, 'ff_neurons_layer_00': 10, 'ff_neurons_layer_01': 10, 'early_stopping_epochs': 8, 'layer_activation_00': 'ReLU'}. Best is trial 3 with value: 0.0011358471357659117.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66: validation data loss: 0.018540384562592528, training data loss: 0.01693285872402801\n",
      "Validation accuracy has not improved enough.\n",
      "Stopping the training, validation accuracy has not improved for 8 epochs.\n",
      "Final validation data loss:  0.018540384562592528\n",
      "Initial Guess - validation data loss:  0.12288565613907766\n",
      "Epoch 0: validation data loss: 0.0026069634581265383, training data loss: 0.004722176077159028\n",
      "Time for epoch[s]: 0.2542765140533447\n",
      "Epoch 1: validation data loss: 0.002033612771665669, training data loss: 0.0004890525381859035\n",
      "Time for epoch[s]: 0.27993178367614746\n",
      "Epoch 2: validation data loss: 0.0017685109077523289, training data loss: 0.0003296653633792651\n",
      "Time for epoch[s]: 0.27925944328308105\n",
      "Epoch 3: validation data loss: 0.0017220768482173415, training data loss: 0.0002538146935913661\n",
      "Time for epoch[s]: 0.27964234352111816\n",
      "Epoch 4: validation data loss: 0.0016750494639078777, training data loss: 0.00020734605179529756\n",
      "Time for epoch[s]: 0.3341085910797119\n",
      "Epoch 5: validation data loss: 0.001688039874377316, training data loss: 0.000179956203590245\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.34688711166381836\n",
      "Epoch 6: validation data loss: 0.0016412340342726337, training data loss: 0.0001586917362528849\n",
      "Time for epoch[s]: 0.34810304641723633\n",
      "Epoch 7: validation data loss: 0.0017020977251061566, training data loss: 0.00014261214156129044\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2746577262878418\n",
      "Epoch 8: validation data loss: 0.0017735004152881501, training data loss: 0.0001332256942987442\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.39195871353149414\n",
      "Epoch 9: validation data loss: 0.0016532108936135627, training data loss: 0.00012270165564807038\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4048926830291748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-10 18:17:12,234]\u001B[0m Trial 15 finished with value: 0.0016805884772784089 and parameters: {'learning_rate': 0.2, 'ff_neurons_layer_00': 10, 'ff_neurons_layer_01': 200, 'early_stopping_epochs': 4, 'layer_activation_00': 'LeakyReLU'}. Best is trial 3 with value: 0.0011358471357659117.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: validation data loss: 0.0016805884772784089, training data loss: 0.00011435287183822562\n",
      "Validation accuracy has not improved enough.\n",
      "Stopping the training, validation accuracy has not improved for 4 epochs.\n",
      "Final validation data loss:  0.0016805884772784089\n",
      "Initial Guess - validation data loss:  0.09215880964444653\n",
      "Epoch 0: validation data loss: 0.00593201156076231, training data loss: 0.010470476324699786\n",
      "Time for epoch[s]: 0.394162654876709\n",
      "Epoch 1: validation data loss: 0.004574376698498312, training data loss: 0.0012840194517074654\n",
      "Time for epoch[s]: 0.40748143196105957\n",
      "Epoch 2: validation data loss: 0.003997712919156845, training data loss: 0.0007375256247716407\n",
      "Time for epoch[s]: 0.34046101570129395\n",
      "Epoch 3: validation data loss: 0.003648150184927466, training data loss: 0.0005437841290208302\n",
      "Time for epoch[s]: 0.3556938171386719\n",
      "Epoch 4: validation data loss: 0.0037175729938838035, training data loss: 0.00044662855803694355\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3426997661590576\n",
      "Epoch 5: validation data loss: 0.0035641299535150396, training data loss: 0.00038548548744149404\n",
      "Time for epoch[s]: 0.31565189361572266\n",
      "Epoch 6: validation data loss: 0.003380246902709682, training data loss: 0.00034250485706547077\n",
      "Time for epoch[s]: 0.3581864833831787\n",
      "Epoch 7: validation data loss: 0.0035537290790853978, training data loss: 0.00030732811449869584\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3528909683227539\n",
      "Epoch 8: validation data loss: 0.003434100379682567, training data loss: 0.0002778754080539425\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.36086130142211914\n",
      "Epoch 9: validation data loss: 0.0034474578077934646, training data loss: 0.0002517438064154969\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3398571014404297\n",
      "Epoch 10: validation data loss: 0.0035150990094224067, training data loss: 0.00022902873825264848\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4194355010986328\n",
      "Epoch 11: validation data loss: 0.003492452510415691, training data loss: 0.00021075380787457506\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.4144275188446045\n",
      "Epoch 12: validation data loss: 0.0035142855012797874, training data loss: 0.0001937570433094077\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3049805164337158\n",
      "Epoch 13: validation data loss: 0.0034366459606989335, training data loss: 0.0001778233132950247\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3111083507537842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-10 18:17:17,871]\u001B[0m Trial 16 finished with value: 0.0034197612440205055 and parameters: {'learning_rate': 0.2, 'ff_neurons_layer_00': 100, 'ff_neurons_layer_01': 10, 'early_stopping_epochs': 8, 'layer_activation_00': 'LeakyReLU'}. Best is trial 3 with value: 0.0011358471357659117.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: validation data loss: 0.0034197612440205055, training data loss: 0.00016407794467934736\n",
      "Validation accuracy has not improved enough.\n",
      "Stopping the training, validation accuracy has not improved for 8 epochs.\n",
      "Final validation data loss:  0.0034197612440205055\n",
      "Initial Guess - validation data loss:  0.050907988526505424\n",
      "Epoch 0: validation data loss: 0.008378724529318614, training data loss: 0.014638773382526555\n",
      "Time for epoch[s]: 0.40041518211364746\n",
      "Epoch 1: validation data loss: 0.00738954163033124, training data loss: 0.003995081877599568\n",
      "Time for epoch[s]: 0.479785680770874\n",
      "Epoch 2: validation data loss: 0.007271409579063659, training data loss: 0.0036710185003062906\n",
      "Time for epoch[s]: 0.4095163345336914\n",
      "Epoch 3: validation data loss: 0.007211968234684914, training data loss: 0.0036123721566918778\n",
      "Time for epoch[s]: 0.41516876220703125\n",
      "Epoch 4: validation data loss: 0.007156374247651122, training data loss: 0.0035694434762545374\n",
      "Time for epoch[s]: 0.39238977432250977\n",
      "Epoch 5: validation data loss: 0.007096322159788925, training data loss: 0.0035329980937313273\n",
      "Time for epoch[s]: 0.3845086097717285\n",
      "Epoch 6: validation data loss: 0.007036936881879693, training data loss: 0.0034860788959346404\n",
      "Time for epoch[s]: 0.31360912322998047\n",
      "Epoch 7: validation data loss: 0.006970095852194312, training data loss: 0.0034375816719717086\n",
      "Time for epoch[s]: 0.26473498344421387\n",
      "Epoch 8: validation data loss: 0.006897243734908431, training data loss: 0.003386470008658492\n",
      "Time for epoch[s]: 0.2951943874359131\n",
      "Epoch 9: validation data loss: 0.006815695327166553, training data loss: 0.0033306479454040527\n",
      "Time for epoch[s]: 0.24434781074523926\n",
      "Epoch 10: validation data loss: 0.006725741848009362, training data loss: 0.0032702488441989845\n",
      "Time for epoch[s]: 0.2534201145172119\n",
      "Epoch 11: validation data loss: 0.006623887580279346, training data loss: 0.003203887645512411\n",
      "Time for epoch[s]: 0.24496006965637207\n",
      "Epoch 12: validation data loss: 0.0065138644823744965, training data loss: 0.003132205303401163\n",
      "Time for epoch[s]: 0.24959778785705566\n",
      "Epoch 13: validation data loss: 0.006393503925027368, training data loss: 0.0030565819783842183\n",
      "Time for epoch[s]: 0.2362527847290039\n",
      "Epoch 14: validation data loss: 0.0062594391983938, training data loss: 0.0029702630217216873\n",
      "Time for epoch[s]: 0.2576456069946289\n",
      "Epoch 15: validation data loss: 0.006118505512742691, training data loss: 0.0028846454402627467\n",
      "Time for epoch[s]: 0.2285914421081543\n",
      "Epoch 16: validation data loss: 0.005965244280148859, training data loss: 0.0027817169280901347\n",
      "Time for epoch[s]: 0.24048829078674316\n",
      "Epoch 17: validation data loss: 0.005806810779658627, training data loss: 0.0026824349137746038\n",
      "Time for epoch[s]: 0.22795510292053223\n",
      "Epoch 18: validation data loss: 0.005643097233010209, training data loss: 0.002580144906152873\n",
      "Time for epoch[s]: 0.2999734878540039\n",
      "Epoch 19: validation data loss: 0.0054753420015448305, training data loss: 0.0024782062665512574\n",
      "Time for epoch[s]: 0.2978856563568115\n",
      "Epoch 20: validation data loss: 0.005309571958567998, training data loss: 0.002378397064121891\n",
      "Time for epoch[s]: 0.31421446800231934\n",
      "Epoch 21: validation data loss: 0.005143204780474101, training data loss: 0.002276102021404597\n",
      "Time for epoch[s]: 0.3580305576324463\n",
      "Epoch 22: validation data loss: 0.00497994847493629, training data loss: 0.002179121590096112\n",
      "Time for epoch[s]: 0.41602325439453125\n",
      "Epoch 23: validation data loss: 0.004825452691343822, training data loss: 0.002086004437921254\n",
      "Time for epoch[s]: 0.5002810955047607\n",
      "Epoch 24: validation data loss: 0.004674354644670879, training data loss: 0.0019977815347175074\n",
      "Time for epoch[s]: 0.537508487701416\n",
      "Epoch 25: validation data loss: 0.004536009814641247, training data loss: 0.0019137140822737184\n",
      "Time for epoch[s]: 0.5137455463409424\n",
      "Epoch 26: validation data loss: 0.004403238699316434, training data loss: 0.0018362209677151894\n",
      "Time for epoch[s]: 0.42755627632141113\n",
      "Epoch 27: validation data loss: 0.004283055322899666, training data loss: 0.0017636905522106989\n",
      "Time for epoch[s]: 0.3337736129760742\n",
      "Epoch 28: validation data loss: 0.004169526437646178, training data loss: 0.0016970173136828697\n",
      "Time for epoch[s]: 0.3206934928894043\n",
      "Epoch 29: validation data loss: 0.004064304915737344, training data loss: 0.0016353488240612151\n",
      "Time for epoch[s]: 0.33510756492614746\n",
      "Epoch 30: validation data loss: 0.003972894252707425, training data loss: 0.001578950038239292\n",
      "Time for epoch[s]: 0.33089375495910645\n",
      "Epoch 31: validation data loss: 0.003886354840509423, training data loss: 0.0015265184994701926\n",
      "Time for epoch[s]: 0.33943963050842285\n",
      "Epoch 32: validation data loss: 0.003813459720785759, training data loss: 0.0014784405492756464\n",
      "Time for epoch[s]: 0.30803918838500977\n",
      "Epoch 33: validation data loss: 0.0037399810743114176, training data loss: 0.0014354409692494293\n",
      "Time for epoch[s]: 0.28597426414489746\n",
      "Epoch 34: validation data loss: 0.0036743228838324003, training data loss: 0.0013952183124681587\n",
      "Time for epoch[s]: 0.3709723949432373\n",
      "Epoch 35: validation data loss: 0.0036137229231394587, training data loss: 0.0013578591281420563\n",
      "Time for epoch[s]: 0.39792609214782715\n",
      "Epoch 36: validation data loss: 0.003561085489786923, training data loss: 0.0013230452526649928\n",
      "Time for epoch[s]: 0.4971201419830322\n",
      "Epoch 37: validation data loss: 0.0035102745713708606, training data loss: 0.0012913617097079482\n",
      "Time for epoch[s]: 0.7169098854064941\n",
      "Epoch 38: validation data loss: 0.0034677078734794164, training data loss: 0.001263741761037748\n",
      "Time for epoch[s]: 0.47618937492370605\n",
      "Epoch 39: validation data loss: 0.0034268756979676687, training data loss: 0.001235741730694357\n",
      "Time for epoch[s]: 0.5093495845794678\n",
      "Epoch 40: validation data loss: 0.0033859659547675145, training data loss: 0.0012109874318179475\n",
      "Time for epoch[s]: 0.45209836959838867\n",
      "Epoch 41: validation data loss: 0.0033497715105204823, training data loss: 0.0011909265224247763\n",
      "Time for epoch[s]: 0.3732025623321533\n",
      "Epoch 42: validation data loss: 0.0033210325458822728, training data loss: 0.0011637797366538549\n",
      "Time for epoch[s]: 0.32612037658691406\n",
      "Epoch 43: validation data loss: 0.0032885654331886605, training data loss: 0.0011441913097416428\n",
      "Time for epoch[s]: 0.3308603763580322\n",
      "Epoch 44: validation data loss: 0.0032614243629316216, training data loss: 0.0011239851310372896\n",
      "Time for epoch[s]: 0.34870004653930664\n",
      "Epoch 45: validation data loss: 0.0032345306927755, training data loss: 0.0011054033149867297\n",
      "Time for epoch[s]: 0.5802493095397949\n",
      "Epoch 46: validation data loss: 0.003204664955400441, training data loss: 0.0010873133733392305\n",
      "Time for epoch[s]: 0.5497429370880127\n",
      "Epoch 47: validation data loss: 0.003181857333335702, training data loss: 0.001072039342906377\n",
      "Time for epoch[s]: 0.4861304759979248\n",
      "Epoch 48: validation data loss: 0.0031597004633515937, training data loss: 0.0010561532081534329\n",
      "Time for epoch[s]: 0.4431908130645752\n",
      "Epoch 49: validation data loss: 0.003137517465304022, training data loss: 0.0010404034292316872\n",
      "Time for epoch[s]: 0.5200009346008301\n",
      "Epoch 50: validation data loss: 0.0031152901039820284, training data loss: 0.0010262261650878\n",
      "Time for epoch[s]: 0.5043461322784424\n",
      "Epoch 51: validation data loss: 0.003093710228732732, training data loss: 0.0010123075279470993\n",
      "Time for epoch[s]: 0.7566606998443604\n",
      "Epoch 52: validation data loss: 0.0030761129779902766, training data loss: 0.0009997018940372554\n",
      "Time for epoch[s]: 0.5223500728607178\n",
      "Epoch 53: validation data loss: 0.0030559257829570335, training data loss: 0.000987228465406862\n",
      "Time for epoch[s]: 0.46819376945495605\n",
      "Epoch 54: validation data loss: 0.0030398504919113088, training data loss: 0.0009746920980819284\n",
      "Time for epoch[s]: 0.41001129150390625\n",
      "Epoch 55: validation data loss: 0.00301935547563039, training data loss: 0.0009640778444673372\n",
      "Time for epoch[s]: 0.3579883575439453\n",
      "Epoch 56: validation data loss: 0.0029995729934134984, training data loss: 0.000951964493211546\n",
      "Time for epoch[s]: 0.32929039001464844\n",
      "Epoch 57: validation data loss: 0.0029848318666083626, training data loss: 0.0009405081538849225\n",
      "Time for epoch[s]: 0.29210686683654785\n",
      "Epoch 58: validation data loss: 0.002965555343453743, training data loss: 0.0009301545685284758\n",
      "Time for epoch[s]: 0.2986302375793457\n",
      "Epoch 59: validation data loss: 0.0029482044041428935, training data loss: 0.0009203269465328896\n",
      "Time for epoch[s]: 0.2762930393218994\n",
      "Epoch 60: validation data loss: 0.0029362478212678813, training data loss: 0.000910920911727975\n",
      "Time for epoch[s]: 0.29691123962402344\n",
      "Epoch 61: validation data loss: 0.002918711263839513, training data loss: 0.0009015950982429121\n",
      "Time for epoch[s]: 0.2787902355194092\n",
      "Epoch 62: validation data loss: 0.002903336259328067, training data loss: 0.0008920796521722454\n",
      "Time for epoch[s]: 0.29648447036743164\n",
      "Epoch 63: validation data loss: 0.0028862808937351455, training data loss: 0.0008830715940423208\n",
      "Time for epoch[s]: 0.315690279006958\n",
      "Epoch 64: validation data loss: 0.0028728053450040075, training data loss: 0.000875123967863109\n",
      "Time for epoch[s]: 0.29404664039611816\n",
      "Epoch 65: validation data loss: 0.002856811431989278, training data loss: 0.0008666409205083978\n",
      "Time for epoch[s]: 0.3107011318206787\n",
      "Epoch 66: validation data loss: 0.0028414230912787728, training data loss: 0.0008587814085015424\n",
      "Time for epoch[s]: 0.3632850646972656\n",
      "Epoch 67: validation data loss: 0.002825837701422983, training data loss: 0.0008516498760545635\n",
      "Time for epoch[s]: 0.48419976234436035\n",
      "Epoch 68: validation data loss: 0.002811589741815715, training data loss: 0.0008440893398572321\n",
      "Time for epoch[s]: 0.6489152908325195\n",
      "Epoch 69: validation data loss: 0.0028008421262105307, training data loss: 0.0008367012216620249\n",
      "Time for epoch[s]: 0.5732884407043457\n",
      "Epoch 70: validation data loss: 0.0027884870903677047, training data loss: 0.0008299988970908945\n",
      "Time for epoch[s]: 0.41179823875427246\n",
      "Epoch 71: validation data loss: 0.0027693708737691245, training data loss: 0.0008234079569986422\n",
      "Time for epoch[s]: 0.34363436698913574\n",
      "Epoch 72: validation data loss: 0.0027591262233856062, training data loss: 0.0008171205515186536\n",
      "Time for epoch[s]: 0.3096911907196045\n",
      "Epoch 73: validation data loss: 0.0027437797964435734, training data loss: 0.0008103538593745123\n",
      "Time for epoch[s]: 0.2940042018890381\n",
      "Epoch 74: validation data loss: 0.0027372450589045, training data loss: 0.000808520063962022\n",
      "Time for epoch[s]: 0.2846338748931885\n",
      "Epoch 75: validation data loss: 0.002724247980335532, training data loss: 0.0008001254709888267\n",
      "Time for epoch[s]: 0.30678796768188477\n",
      "Epoch 76: validation data loss: 0.0027102710993866944, training data loss: 0.0007973111927781475\n",
      "Time for epoch[s]: 0.283583402633667\n",
      "Epoch 77: validation data loss: 0.0026947276232993766, training data loss: 0.0007876710260295432\n",
      "Time for epoch[s]: 0.2925746440887451\n",
      "Epoch 78: validation data loss: 0.002682531533175952, training data loss: 0.0007823276329258261\n",
      "Time for epoch[s]: 0.3081176280975342\n",
      "Epoch 79: validation data loss: 0.002670354222598141, training data loss: 0.000777253753518405\n",
      "Time for epoch[s]: 0.29442596435546875\n",
      "Epoch 80: validation data loss: 0.002661410804208555, training data loss: 0.0007739290254845467\n",
      "Time for epoch[s]: 0.3513467311859131\n",
      "Epoch 81: validation data loss: 0.0026478266607136485, training data loss: 0.0007676286920564904\n",
      "Time for epoch[s]: 0.5312161445617676\n",
      "Epoch 82: validation data loss: 0.002637988900485104, training data loss: 0.0007634717443762304\n",
      "Time for epoch[s]: 0.4703688621520996\n",
      "Epoch 83: validation data loss: 0.0026269751052333886, training data loss: 0.0007581846218675239\n",
      "Time for epoch[s]: 0.4391138553619385\n",
      "Epoch 84: validation data loss: 0.0026123907468090317, training data loss: 0.0007535494081505902\n",
      "Time for epoch[s]: 0.455005407333374\n",
      "Epoch 85: validation data loss: 0.0026047246096885367, training data loss: 0.0007496035942748257\n",
      "Time for epoch[s]: 0.3296327590942383\n",
      "Epoch 86: validation data loss: 0.002593462325666593, training data loss: 0.000745379013013622\n",
      "Time for epoch[s]: 0.35469889640808105\n",
      "Epoch 87: validation data loss: 0.0025816026343602567, training data loss: 0.0007406642583951558\n",
      "Time for epoch[s]: 0.3200492858886719\n",
      "Epoch 88: validation data loss: 0.0025714404506770442, training data loss: 0.0007365354119914852\n",
      "Time for epoch[s]: 0.2800474166870117\n",
      "Epoch 89: validation data loss: 0.002560932886654928, training data loss: 0.0007326070848665281\n",
      "Time for epoch[s]: 0.297166109085083\n",
      "Epoch 90: validation data loss: 0.002551615510357025, training data loss: 0.0007289660983978341\n",
      "Time for epoch[s]: 0.37253665924072266\n",
      "Epoch 91: validation data loss: 0.0025410113269335602, training data loss: 0.000724938838449243\n",
      "Time for epoch[s]: 0.3648855686187744\n",
      "Epoch 92: validation data loss: 0.00253190215864138, training data loss: 0.0007230113494341776\n",
      "Time for epoch[s]: 0.4004781246185303\n",
      "Epoch 93: validation data loss: 0.0025216722052935595, training data loss: 0.0007171679169075674\n",
      "Time for epoch[s]: 0.31929469108581543\n",
      "Epoch 94: validation data loss: 0.0025099447873085058, training data loss: 0.0007147894466304343\n",
      "Time for epoch[s]: 0.2774832248687744\n",
      "Epoch 95: validation data loss: 0.002501963480422486, training data loss: 0.0007101178985752471\n",
      "Time for epoch[s]: 0.3105792999267578\n",
      "Epoch 96: validation data loss: 0.002492982230774344, training data loss: 0.0007061189440287412\n",
      "Time for epoch[s]: 0.28094029426574707\n",
      "Epoch 97: validation data loss: 0.0024787190297967223, training data loss: 0.0007028896786850881\n",
      "Time for epoch[s]: 0.2879300117492676\n",
      "Epoch 98: validation data loss: 0.002472442579051675, training data loss: 0.0007010180520140417\n",
      "Time for epoch[s]: 0.30449390411376953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-10 18:17:55,113]\u001B[0m Trial 17 finished with value: 0.002465070382645141 and parameters: {'learning_rate': 0.2, 'ff_neurons_layer_00': 200, 'ff_neurons_layer_01': 10, 'early_stopping_epochs': 8, 'layer_activation_00': 'Sigmoid'}. Best is trial 3 with value: 0.0011358471357659117.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: validation data loss: 0.002465070382645141, training data loss: 0.0006960956337245087\n",
      "Time for epoch[s]: 0.2628798484802246\n",
      "Final validation data loss:  0.002465070382645141\n",
      "Initial Guess - validation data loss:  0.050718111534641216\n",
      "Epoch 0: validation data loss: 0.007239376573257795, training data loss: 0.0047378959176747224\n",
      "Time for epoch[s]: 0.31429052352905273\n",
      "Epoch 1: validation data loss: 0.007131197136831066, training data loss: 0.00362804164625194\n",
      "Time for epoch[s]: 0.3524291515350342\n",
      "Epoch 2: validation data loss: 0.0070025338429838555, training data loss: 0.003540070633910018\n",
      "Time for epoch[s]: 0.32512617111206055\n",
      "Epoch 3: validation data loss: 0.006855777409523045, training data loss: 0.0034424156358797255\n",
      "Time for epoch[s]: 0.3421461582183838\n",
      "Epoch 4: validation data loss: 0.006676058790999461, training data loss: 0.003327627976735433\n",
      "Time for epoch[s]: 0.35167932510375977\n",
      "Epoch 5: validation data loss: 0.006470753721994896, training data loss: 0.0032046004517437662\n",
      "Time for epoch[s]: 0.40936279296875\n",
      "Epoch 6: validation data loss: 0.006244350241743811, training data loss: 0.0030583461670026387\n",
      "Time for epoch[s]: 0.44376373291015625\n",
      "Epoch 7: validation data loss: 0.006009450786189946, training data loss: 0.002907086997271673\n",
      "Time for epoch[s]: 0.33390259742736816\n",
      "Epoch 8: validation data loss: 0.005767042778398348, training data loss: 0.0027527839081472457\n",
      "Time for epoch[s]: 0.23947787284851074\n",
      "Epoch 9: validation data loss: 0.005521922895353134, training data loss: 0.0026042202836302317\n",
      "Time for epoch[s]: 0.23954153060913086\n",
      "Epoch 10: validation data loss: 0.005281260568801671, training data loss: 0.0024545930291964037\n",
      "Time for epoch[s]: 0.18443989753723145\n",
      "Epoch 11: validation data loss: 0.005058686482851908, training data loss: 0.0023199099383942067\n",
      "Time for epoch[s]: 0.1799933910369873\n",
      "Epoch 12: validation data loss: 0.004859858451912937, training data loss: 0.00219828693289735\n",
      "Time for epoch[s]: 0.17513370513916016\n",
      "Epoch 13: validation data loss: 0.0046669914297861595, training data loss: 0.0020867901849964438\n",
      "Time for epoch[s]: 0.19618654251098633\n",
      "Epoch 14: validation data loss: 0.004495920928101561, training data loss: 0.001990793910745072\n",
      "Time for epoch[s]: 0.19239330291748047\n",
      "Epoch 15: validation data loss: 0.004329912738712955, training data loss: 0.001903304498489589\n",
      "Time for epoch[s]: 0.1973884105682373\n",
      "Epoch 16: validation data loss: 0.004195618575022101, training data loss: 0.0018311820618093831\n",
      "Time for epoch[s]: 0.2373065948486328\n",
      "Epoch 17: validation data loss: 0.00408667053806183, training data loss: 0.0017625682702347568\n",
      "Time for epoch[s]: 0.3115696907043457\n",
      "Epoch 18: validation data loss: 0.0039687692846881745, training data loss: 0.0017031390373020957\n",
      "Time for epoch[s]: 0.22987079620361328\n",
      "Epoch 19: validation data loss: 0.0038600498682832066, training data loss: 0.0016512549631127484\n",
      "Time for epoch[s]: 0.23170256614685059\n",
      "Epoch 20: validation data loss: 0.003758349647260692, training data loss: 0.0015998018658868799\n",
      "Time for epoch[s]: 0.30352258682250977\n",
      "Epoch 21: validation data loss: 0.0037005325974939075, training data loss: 0.0015538496786056588\n",
      "Time for epoch[s]: 0.2110753059387207\n",
      "Epoch 22: validation data loss: 0.0036000903338602144, training data loss: 0.001510511794591059\n",
      "Time for epoch[s]: 0.19373202323913574\n",
      "Epoch 23: validation data loss: 0.0035318721374964605, training data loss: 0.0014680048374280537\n",
      "Time for epoch[s]: 0.1737842559814453\n",
      "Epoch 24: validation data loss: 0.003474122313059628, training data loss: 0.0014282953521432397\n",
      "Time for epoch[s]: 0.1836717128753662\n",
      "Epoch 25: validation data loss: 0.003396745141782717, training data loss: 0.0013881505352177032\n",
      "Time for epoch[s]: 0.23702788352966309\n",
      "Epoch 26: validation data loss: 0.003340696635311597, training data loss: 0.0013514393812989536\n",
      "Time for epoch[s]: 0.2575802803039551\n",
      "Epoch 27: validation data loss: 0.0032844851006111598, training data loss: 0.0013139321379465599\n",
      "Time for epoch[s]: 0.22734665870666504\n",
      "Epoch 28: validation data loss: 0.0032317458766780486, training data loss: 0.0012794309827290713\n",
      "Time for epoch[s]: 0.21465492248535156\n",
      "Epoch 29: validation data loss: 0.0031732957657069375, training data loss: 0.0012486500827144813\n",
      "Time for epoch[s]: 0.24353837966918945\n",
      "Epoch 30: validation data loss: 0.0031199692046805605, training data loss: 0.0012120156799821549\n",
      "Time for epoch[s]: 0.20219731330871582\n",
      "Epoch 31: validation data loss: 0.003068641440509117, training data loss: 0.0011806467624559795\n",
      "Time for epoch[s]: 0.17733526229858398\n",
      "Epoch 32: validation data loss: 0.0030148511063562682, training data loss: 0.0011508795496535627\n",
      "Time for epoch[s]: 0.17903518676757812\n",
      "Epoch 33: validation data loss: 0.002974350158482382, training data loss: 0.001121761047676818\n",
      "Time for epoch[s]: 0.1734941005706787\n",
      "Epoch 34: validation data loss: 0.0029172099888596903, training data loss: 0.0010943199809827762\n",
      "Time for epoch[s]: 0.2089543342590332\n",
      "Epoch 35: validation data loss: 0.0028715716105073555, training data loss: 0.0010685897581109174\n",
      "Time for epoch[s]: 0.2099137306213379\n",
      "Epoch 36: validation data loss: 0.0028263436604852544, training data loss: 0.0010430666545754698\n",
      "Time for epoch[s]: 0.28627920150756836\n",
      "Epoch 37: validation data loss: 0.0027855057694596242, training data loss: 0.0010199018128930705\n",
      "Time for epoch[s]: 0.22645807266235352\n",
      "Epoch 38: validation data loss: 0.0027402103219402434, training data loss: 0.0009967049915496618\n",
      "Time for epoch[s]: 0.18187904357910156\n",
      "Epoch 39: validation data loss: 0.002697307225231711, training data loss: 0.0009758621454238892\n",
      "Time for epoch[s]: 0.1751561164855957\n",
      "Epoch 40: validation data loss: 0.0026676469741890962, training data loss: 0.0009559509961028078\n",
      "Time for epoch[s]: 0.1648120880126953\n",
      "Epoch 41: validation data loss: 0.0026377434055554812, training data loss: 0.0009377955301711549\n",
      "Time for epoch[s]: 0.20827603340148926\n",
      "Epoch 42: validation data loss: 0.0025962637439710364, training data loss: 0.0009193607525194073\n",
      "Time for epoch[s]: 0.224013090133667\n",
      "Epoch 43: validation data loss: 0.0025586453202652605, training data loss: 0.0009028206677197322\n",
      "Time for epoch[s]: 0.20988225936889648\n",
      "Epoch 44: validation data loss: 0.0025242760301180628, training data loss: 0.0008895842179860154\n",
      "Time for epoch[s]: 0.1996767520904541\n",
      "Epoch 45: validation data loss: 0.002495410235505126, training data loss: 0.0008717428331505762\n",
      "Time for epoch[s]: 0.18681883811950684\n",
      "Epoch 46: validation data loss: 0.002464775625429197, training data loss: 0.0008565529024219949\n",
      "Time for epoch[s]: 0.21920514106750488\n",
      "Epoch 47: validation data loss: 0.0024369040580645, training data loss: 0.00084380519716707\n",
      "Time for epoch[s]: 0.26402831077575684\n",
      "Epoch 48: validation data loss: 0.0024152155880514344, training data loss: 0.0008308588097628937\n",
      "Time for epoch[s]: 0.3151400089263916\n",
      "Epoch 49: validation data loss: 0.0023978940428119816, training data loss: 0.0008200137582543778\n",
      "Time for epoch[s]: 0.265566349029541\n",
      "Epoch 50: validation data loss: 0.0023582166732718413, training data loss: 0.0008067162749974151\n",
      "Time for epoch[s]: 0.22445273399353027\n",
      "Epoch 51: validation data loss: 0.0023462728822612328, training data loss: 0.0008010599575086271\n",
      "Time for epoch[s]: 0.19477033615112305\n",
      "Epoch 52: validation data loss: 0.002313977507151425, training data loss: 0.0007859237117854428\n",
      "Time for epoch[s]: 0.18867707252502441\n",
      "Epoch 53: validation data loss: 0.002291283378862355, training data loss: 0.0007748616068330529\n",
      "Time for epoch[s]: 0.2197718620300293\n",
      "Epoch 54: validation data loss: 0.0022612497142460793, training data loss: 0.0007650124155767432\n",
      "Time for epoch[s]: 0.24225759506225586\n",
      "Epoch 55: validation data loss: 0.002236593123440329, training data loss: 0.0007557700894194651\n",
      "Time for epoch[s]: 0.25706982612609863\n",
      "Epoch 56: validation data loss: 0.0022249681764541698, training data loss: 0.0007466557499480574\n",
      "Time for epoch[s]: 0.23357748985290527\n",
      "Epoch 57: validation data loss: 0.002203735858882399, training data loss: 0.0007376841622400501\n",
      "Time for epoch[s]: 0.2499687671661377\n",
      "Epoch 58: validation data loss: 0.002191411441863944, training data loss: 0.0007294572243407437\n",
      "Time for epoch[s]: 0.19218945503234863\n",
      "Epoch 59: validation data loss: 0.0021584032605227814, training data loss: 0.000721359906131274\n",
      "Time for epoch[s]: 0.20219087600708008\n",
      "Epoch 60: validation data loss: 0.0021388039469174597, training data loss: 0.0007133348483473198\n",
      "Time for epoch[s]: 0.20316863059997559\n",
      "Epoch 61: validation data loss: 0.0021344998927965556, training data loss: 0.0007064397612663165\n",
      "Time for epoch[s]: 0.24233365058898926\n",
      "Epoch 62: validation data loss: 0.0021060041096656835, training data loss: 0.0006980804820038957\n",
      "Time for epoch[s]: 0.198073148727417\n",
      "Epoch 63: validation data loss: 0.0020887429855729893, training data loss: 0.0006910457730837609\n",
      "Time for epoch[s]: 0.187164306640625\n",
      "Epoch 64: validation data loss: 0.0020681663190937477, training data loss: 0.0006846367768501037\n",
      "Time for epoch[s]: 0.25640368461608887\n",
      "Epoch 65: validation data loss: 0.002057673724274657, training data loss: 0.0006772663495311998\n",
      "Time for epoch[s]: 0.23563814163208008\n",
      "Epoch 66: validation data loss: 0.0020355862569591226, training data loss: 0.0006707104509823943\n",
      "Time for epoch[s]: 0.270892858505249\n",
      "Epoch 67: validation data loss: 0.0020182731489068297, training data loss: 0.0006640991663823933\n",
      "Time for epoch[s]: 0.21025967597961426\n",
      "Epoch 68: validation data loss: 0.002001108235964492, training data loss: 0.0006580117358464629\n",
      "Time for epoch[s]: 0.1944112777709961\n",
      "Epoch 69: validation data loss: 0.0019882420698801675, training data loss: 0.0006514451547300434\n",
      "Time for epoch[s]: 0.18561506271362305\n",
      "Epoch 70: validation data loss: 0.001984412267327853, training data loss: 0.0006455070760152111\n",
      "Time for epoch[s]: 0.21544456481933594\n",
      "Epoch 71: validation data loss: 0.0019594219993782913, training data loss: 0.0006394878914367119\n",
      "Time for epoch[s]: 0.23057770729064941\n",
      "Epoch 72: validation data loss: 0.0019390236024987208, training data loss: 0.0006336321433385214\n",
      "Time for epoch[s]: 0.18617486953735352\n",
      "Epoch 73: validation data loss: 0.001918542194584189, training data loss: 0.0006280937423444774\n",
      "Time for epoch[s]: 0.19444727897644043\n",
      "Epoch 74: validation data loss: 0.0019097747323719879, training data loss: 0.0006227260583067594\n",
      "Time for epoch[s]: 0.20140504837036133\n",
      "Epoch 75: validation data loss: 0.0019010302682989809, training data loss: 0.0006173317018709226\n",
      "Time for epoch[s]: 0.21191740036010742\n",
      "Epoch 76: validation data loss: 0.001882956860816642, training data loss: 0.0006118731819875708\n",
      "Time for epoch[s]: 0.19654226303100586\n",
      "Epoch 77: validation data loss: 0.0018662481003155992, training data loss: 0.0006067249328578444\n",
      "Time for epoch[s]: 0.1904468536376953\n",
      "Epoch 78: validation data loss: 0.001849624800355467, training data loss: 0.000601562871236235\n",
      "Time for epoch[s]: 0.20402097702026367\n",
      "Epoch 79: validation data loss: 0.0018473339679578667, training data loss: 0.0005967149587526714\n",
      "Time for epoch[s]: 0.21359586715698242\n",
      "Epoch 80: validation data loss: 0.001832788665545041, training data loss: 0.000591636044249687\n",
      "Time for epoch[s]: 0.20728635787963867\n",
      "Epoch 81: validation data loss: 0.0018182063483756427, training data loss: 0.0005867392777307937\n",
      "Time for epoch[s]: 0.19933795928955078\n",
      "Epoch 82: validation data loss: 0.0018029886565796318, training data loss: 0.0005813547873605876\n",
      "Time for epoch[s]: 0.218489408493042\n",
      "Epoch 83: validation data loss: 0.0017936639317638798, training data loss: 0.0005790017647285984\n",
      "Time for epoch[s]: 0.3474445343017578\n",
      "Epoch 84: validation data loss: 0.001774907112121582, training data loss: 0.0005725948097498993\n",
      "Time for epoch[s]: 0.26874542236328125\n",
      "Epoch 85: validation data loss: 0.0017635698460008455, training data loss: 0.0005682903813989195\n",
      "Time for epoch[s]: 0.21292781829833984\n",
      "Epoch 86: validation data loss: 0.0017544610859596566, training data loss: 0.0005642483223518824\n",
      "Time for epoch[s]: 0.18965411186218262\n",
      "Epoch 87: validation data loss: 0.0017482299510746787, training data loss: 0.0005594187471420253\n",
      "Time for epoch[s]: 0.20148921012878418\n",
      "Epoch 88: validation data loss: 0.0017258954919092187, training data loss: 0.0005545219125813001\n",
      "Time for epoch[s]: 0.2210068702697754\n",
      "Epoch 89: validation data loss: 0.0017125542577543216, training data loss: 0.0005500075411578836\n",
      "Time for epoch[s]: 0.27589964866638184\n",
      "Epoch 90: validation data loss: 0.0017078280721080901, training data loss: 0.0005460358373650677\n",
      "Time for epoch[s]: 0.29162168502807617\n",
      "Epoch 91: validation data loss: 0.0016931411066011752, training data loss: 0.0005423002047081516\n",
      "Time for epoch[s]: 0.26723217964172363\n",
      "Epoch 92: validation data loss: 0.0016798443718043636, training data loss: 0.0005378630267430658\n",
      "Time for epoch[s]: 0.21213054656982422\n",
      "Epoch 93: validation data loss: 0.0016657573991714547, training data loss: 0.000534149133451453\n",
      "Time for epoch[s]: 0.19795775413513184\n",
      "Epoch 94: validation data loss: 0.001658821623074954, training data loss: 0.000529734443311822\n",
      "Time for epoch[s]: 0.1869351863861084\n",
      "Epoch 95: validation data loss: 0.0016519453155395647, training data loss: 0.0005259963611489562\n",
      "Time for epoch[s]: 0.22346067428588867\n",
      "Epoch 96: validation data loss: 0.001639398401730681, training data loss: 0.0005221056951779753\n",
      "Time for epoch[s]: 0.20868968963623047\n",
      "Epoch 97: validation data loss: 0.0016307640293417457, training data loss: 0.000518247110930752\n",
      "Time for epoch[s]: 0.19354820251464844\n",
      "Epoch 98: validation data loss: 0.0016198848208335982, training data loss: 0.0005144737830989438\n",
      "Time for epoch[s]: 0.1984562873840332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-10 18:18:18,201]\u001B[0m Trial 18 finished with value: 0.0016087545379656091 and parameters: {'learning_rate': 0.2, 'ff_neurons_layer_00': 10, 'ff_neurons_layer_01': 200, 'early_stopping_epochs': 8, 'layer_activation_00': 'Sigmoid'}. Best is trial 3 with value: 0.0011358471357659117.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: validation data loss: 0.0016087545379656091, training data loss: 0.0005104064124904267\n",
      "Time for epoch[s]: 0.20853209495544434\n",
      "Final validation data loss:  0.0016087545379656091\n",
      "Initial Guess - validation data loss:  0.12444681664035745\n",
      "Epoch 0: validation data loss: 0.0025094491706046883, training data loss: 0.010554606511712618\n",
      "Time for epoch[s]: 0.27470946311950684\n",
      "Epoch 1: validation data loss: 0.0017253474829948112, training data loss: 0.00034198600407604757\n",
      "Time for epoch[s]: 0.2689690589904785\n",
      "Epoch 2: validation data loss: 0.0016061313892608365, training data loss: 0.00022392296423650768\n",
      "Time for epoch[s]: 0.3351764678955078\n",
      "Epoch 3: validation data loss: 0.001399337837140854, training data loss: 0.00017810407090404807\n",
      "Time for epoch[s]: 0.33642077445983887\n",
      "Epoch 4: validation data loss: 0.0012269305856260535, training data loss: 0.0001491936408493617\n",
      "Time for epoch[s]: 0.25368618965148926\n",
      "Epoch 5: validation data loss: 0.0011764581072820377, training data loss: 0.00012724550619517286\n",
      "Time for epoch[s]: 0.26712918281555176\n",
      "Epoch 6: validation data loss: 0.0011647255181177565, training data loss: 0.00011044397065628609\n",
      "Time for epoch[s]: 0.2777431011199951\n",
      "Epoch 7: validation data loss: 0.001157008485706974, training data loss: 9.814191388485094e-05\n",
      "Time for epoch[s]: 0.3120856285095215\n",
      "Epoch 8: validation data loss: 0.0010705691630437495, training data loss: 9.098011983311884e-05\n",
      "Time for epoch[s]: 0.298248291015625\n",
      "Epoch 9: validation data loss: 0.001052861616491727, training data loss: 8.51106102744194e-05\n",
      "Time for epoch[s]: 0.303081750869751\n",
      "Epoch 10: validation data loss: 0.0010055777144758668, training data loss: 7.657784135102137e-05\n",
      "Time for epoch[s]: 0.23203730583190918\n",
      "Epoch 11: validation data loss: 0.0009664407331649571, training data loss: 7.214897299466067e-05\n",
      "Time for epoch[s]: 0.20293688774108887\n",
      "Epoch 12: validation data loss: 0.0009705225626627604, training data loss: 6.89233606468597e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.21746015548706055\n",
      "Epoch 13: validation data loss: 0.0009376623723060573, training data loss: 6.219576278777971e-05\n",
      "Time for epoch[s]: 0.19989967346191406\n",
      "Epoch 14: validation data loss: 0.0009357706732945899, training data loss: 6.024650038649502e-05\n",
      "Time for epoch[s]: 0.19321060180664062\n",
      "Epoch 15: validation data loss: 0.0009114424523697596, training data loss: 5.814910241185802e-05\n",
      "Time for epoch[s]: 0.24904441833496094\n",
      "Epoch 16: validation data loss: 0.0009086976448694865, training data loss: 5.593231653786141e-05\n",
      "Time for epoch[s]: 0.20130538940429688\n",
      "Epoch 17: validation data loss: 0.0008852447140706729, training data loss: 5.209504639449185e-05\n",
      "Time for epoch[s]: 0.19813179969787598\n",
      "Epoch 18: validation data loss: 0.0008852823412037332, training data loss: 5.2140579137900104e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.18138980865478516\n",
      "Epoch 19: validation data loss: 0.0008763251103222642, training data loss: 4.758639473757243e-05\n",
      "Time for epoch[s]: 0.1966264247894287\n",
      "Epoch 20: validation data loss: 0.0008599337649671998, training data loss: 4.624139908786234e-05\n",
      "Time for epoch[s]: 0.2550179958343506\n",
      "Epoch 21: validation data loss: 0.0008524152105801726, training data loss: 4.434494565338849e-05\n",
      "Time for epoch[s]: 0.3255891799926758\n",
      "Epoch 22: validation data loss: 0.0008470118726225204, training data loss: 4.2876781530031874e-05\n",
      "Time for epoch[s]: 0.40058374404907227\n",
      "Epoch 23: validation data loss: 0.0008314881967082959, training data loss: 4.085114217239972e-05\n",
      "Time for epoch[s]: 0.4262063503265381\n",
      "Epoch 24: validation data loss: 0.0008405388490250121, training data loss: 3.9751900861796726e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.3379495143890381\n",
      "Epoch 25: validation data loss: 0.0008271073233591367, training data loss: 3.91015400217004e-05\n",
      "Time for epoch[s]: 0.3157839775085449\n",
      "Epoch 26: validation data loss: 0.0008053013451023189, training data loss: 3.770194930709116e-05\n",
      "Time for epoch[s]: 0.2750093936920166\n",
      "Epoch 27: validation data loss: 0.0008036482687954489, training data loss: 3.564914424805881e-05\n",
      "Time for epoch[s]: 0.2299184799194336\n",
      "Epoch 28: validation data loss: 0.0008001558856877018, training data loss: 3.486478139888751e-05\n",
      "Time for epoch[s]: 0.2050464153289795\n",
      "Epoch 29: validation data loss: 0.0007951622276001325, training data loss: 3.43077420575978e-05\n",
      "Time for epoch[s]: 0.25072145462036133\n",
      "Epoch 30: validation data loss: 0.0007902358628843473, training data loss: 3.337464826904476e-05\n",
      "Time for epoch[s]: 0.2752072811126709\n",
      "Epoch 31: validation data loss: 0.0007891650205333483, training data loss: 3.294608679059978e-05\n",
      "Time for epoch[s]: 0.2875661849975586\n",
      "Epoch 32: validation data loss: 0.0007760328108861566, training data loss: 3.2671693219169635e-05\n",
      "Time for epoch[s]: 0.2199077606201172\n",
      "Epoch 33: validation data loss: 0.0007667263212813634, training data loss: 3.0746171038308645e-05\n",
      "Time for epoch[s]: 0.19871211051940918\n",
      "Epoch 34: validation data loss: 0.0007747844473956383, training data loss: 2.9470561046714652e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.20589065551757812\n",
      "Epoch 35: validation data loss: 0.0007614100768685886, training data loss: 2.9407499026354044e-05\n",
      "Time for epoch[s]: 0.2545924186706543\n",
      "Epoch 36: validation data loss: 0.0007509915796044754, training data loss: 2.9757961239580693e-05\n",
      "Time for epoch[s]: 0.2497410774230957\n",
      "Epoch 37: validation data loss: 0.0007532060010248124, training data loss: 2.8075973533059907e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.1947786808013916\n",
      "Epoch 38: validation data loss: 0.0007585627303275888, training data loss: 2.8096517913689898e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.1877732276916504\n",
      "Epoch 39: validation data loss: 0.0007395644302237524, training data loss: 2.665141359243763e-05\n",
      "Time for epoch[s]: 0.20860815048217773\n",
      "Epoch 40: validation data loss: 0.0007428793602338121, training data loss: 2.654076481586722e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.21035289764404297\n",
      "Epoch 41: validation data loss: 0.0007342596848805746, training data loss: 2.6656144626064387e-05\n",
      "Time for epoch[s]: 0.19678378105163574\n",
      "Epoch 42: validation data loss: 0.0007405273582293018, training data loss: 2.6304030272106058e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.1867067813873291\n",
      "Epoch 43: validation data loss: 0.0007289476590613797, training data loss: 2.5128329079172926e-05\n",
      "Time for epoch[s]: 0.21443462371826172\n",
      "Epoch 44: validation data loss: 0.000723479477237893, training data loss: 2.4533509510701107e-05\n",
      "Time for epoch[s]: 0.20824599266052246\n",
      "Epoch 45: validation data loss: 0.0007140222749753629, training data loss: 2.572952544308144e-05\n",
      "Time for epoch[s]: 0.1911149024963379\n",
      "Epoch 46: validation data loss: 0.0007203201588974696, training data loss: 2.406497770860859e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.19081377983093262\n",
      "Epoch 47: validation data loss: 0.0007190507704808831, training data loss: 2.451033914061986e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.19934773445129395\n",
      "Epoch 48: validation data loss: 0.0007071290930656537, training data loss: 2.2985068783504233e-05\n",
      "Time for epoch[s]: 0.22333860397338867\n",
      "Epoch 49: validation data loss: 0.0007155913876616247, training data loss: 2.2865257749002275e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.22786545753479004\n",
      "Epoch 50: validation data loss: 0.0007106427732667967, training data loss: 2.2901309288391783e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.21614527702331543\n",
      "Epoch 51: validation data loss: 0.0007096109188854966, training data loss: 2.2667847131485265e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.21244215965270996\n",
      "Epoch 52: validation data loss: 0.0007032541787787659, training data loss: 2.1996950415987946e-05\n",
      "Time for epoch[s]: 0.21023321151733398\n",
      "Epoch 53: validation data loss: 0.0007088757268914349, training data loss: 2.175740914665945e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.28025341033935547\n",
      "Epoch 54: validation data loss: 0.0007013776530958201, training data loss: 2.130770154263331e-05\n",
      "Time for epoch[s]: 0.2191781997680664\n",
      "Epoch 55: validation data loss: 0.0006949306214781112, training data loss: 2.084132581609025e-05\n",
      "Time for epoch[s]: 0.24529242515563965\n",
      "Epoch 56: validation data loss: 0.0006843434485126304, training data loss: 2.1353306580487997e-05\n",
      "Time for epoch[s]: 0.24671268463134766\n",
      "Epoch 57: validation data loss: 0.0006867415828791928, training data loss: 2.0562951797491883e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.26416993141174316\n",
      "Epoch 58: validation data loss: 0.0006869493145920915, training data loss: 2.045508635710908e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.24282073974609375\n",
      "Epoch 59: validation data loss: 0.0006853467933663495, training data loss: 1.992727098263562e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.22094345092773438\n",
      "Epoch 60: validation data loss: 0.0006810079018274943, training data loss: 1.9992608151751568e-05\n",
      "Time for epoch[s]: 0.1974043846130371\n",
      "Epoch 61: validation data loss: 0.000679116134774195, training data loss: 1.9689560463847635e-05\n",
      "Time for epoch[s]: 0.20583891868591309\n",
      "Epoch 62: validation data loss: 0.0006758851683847436, training data loss: 1.941294490134335e-05\n",
      "Time for epoch[s]: 0.27962350845336914\n",
      "Epoch 63: validation data loss: 0.0006722103651255778, training data loss: 1.9375054106184336e-05\n",
      "Time for epoch[s]: 0.37228822708129883\n",
      "Epoch 64: validation data loss: 0.0006766331522432092, training data loss: 1.9015350961657963e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.368257999420166\n",
      "Epoch 65: validation data loss: 0.0006737308279020057, training data loss: 1.8788459094147705e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.28928613662719727\n",
      "Epoch 66: validation data loss: 0.0006717803407477462, training data loss: 1.827460080044999e-05\n",
      "Time for epoch[s]: 0.22766757011413574\n",
      "Epoch 67: validation data loss: 0.0006629363314746178, training data loss: 1.8372285481729463e-05\n",
      "Time for epoch[s]: 0.22818660736083984\n",
      "Epoch 68: validation data loss: 0.0006605109123334492, training data loss: 1.857674693408078e-05\n",
      "Time for epoch[s]: 0.20699262619018555\n",
      "Epoch 69: validation data loss: 0.0006669856369767559, training data loss: 1.7954319392301176e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.21731185913085938\n",
      "Epoch 70: validation data loss: 0.0006588647762934367, training data loss: 1.7974738320803534e-05\n",
      "Time for epoch[s]: 0.18922829627990723\n",
      "Epoch 71: validation data loss: 0.0006622470677171123, training data loss: 1.749089179075744e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.21480393409729004\n",
      "Epoch 72: validation data loss: 0.0006614937085539238, training data loss: 1.769904451249125e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2877018451690674\n",
      "Epoch 73: validation data loss: 0.000652344803831893, training data loss: 1.7771965156407117e-05\n",
      "Time for epoch[s]: 0.2691762447357178\n",
      "Epoch 74: validation data loss: 0.0006601827465780249, training data loss: 1.734852701527615e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.23529767990112305\n",
      "Epoch 75: validation data loss: 0.0006584254982264618, training data loss: 1.698989765591001e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2391822338104248\n",
      "Epoch 76: validation data loss: 0.000653778921523595, training data loss: 1.7577974704098484e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2039165496826172\n",
      "Epoch 77: validation data loss: 0.0006557165487716187, training data loss: 1.6848870327687698e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.25040435791015625\n",
      "Epoch 78: validation data loss: 0.0006549216840909496, training data loss: 1.6408756621964446e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.20867013931274414\n",
      "Epoch 79: validation data loss: 0.0006568613525939314, training data loss: 1.7167950372241404e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2595863342285156\n",
      "Epoch 80: validation data loss: 0.0006504925690829482, training data loss: 1.7063645433345342e-05\n",
      "Time for epoch[s]: 0.24116230010986328\n",
      "Epoch 81: validation data loss: 0.0006529187366842679, training data loss: 1.665125452152126e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2795720100402832\n",
      "Epoch 82: validation data loss: 0.0006527249535469159, training data loss: 1.618377630467012e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2564280033111572\n",
      "Epoch 83: validation data loss: 0.0006482474607964084, training data loss: 1.6866487846390842e-05\n",
      "Time for epoch[s]: 0.248762845993042\n",
      "Epoch 84: validation data loss: 0.0006401540209713592, training data loss: 1.5945256725974278e-05\n",
      "Time for epoch[s]: 0.2317337989807129\n",
      "Epoch 85: validation data loss: 0.0006467081504325344, training data loss: 1.619686053632057e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.25136518478393555\n",
      "Epoch 86: validation data loss: 0.0006414965543572761, training data loss: 1.575175000856456e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.23560667037963867\n",
      "Epoch 87: validation data loss: 0.0006458564708221993, training data loss: 1.6080882170633094e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.23804616928100586\n",
      "Epoch 88: validation data loss: 0.0006457352202776904, training data loss: 1.497358747132837e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2493124008178711\n",
      "Epoch 89: validation data loss: 0.0006431688184607519, training data loss: 1.5128600585474272e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.21962451934814453\n",
      "Epoch 90: validation data loss: 0.0006407561231421554, training data loss: 1.5279521621321434e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.24950504302978516\n",
      "Epoch 91: validation data loss: 0.0006378161716678915, training data loss: 1.5568077462176753e-05\n",
      "Time for epoch[s]: 0.2428891658782959\n",
      "Epoch 92: validation data loss: 0.0006291192007935755, training data loss: 1.5319572744644396e-05\n",
      "Time for epoch[s]: 0.19663190841674805\n",
      "Epoch 93: validation data loss: 0.0006327810080628416, training data loss: 1.5191983678229323e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.21725106239318848\n",
      "Epoch 94: validation data loss: 0.0006387722954902475, training data loss: 1.4849652463918952e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2848367691040039\n",
      "Epoch 95: validation data loss: 0.0006314690254594637, training data loss: 1.500310061593034e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2288343906402588\n",
      "Epoch 96: validation data loss: 0.000633737812303517, training data loss: 1.4825834633280698e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2135176658630371\n",
      "Epoch 97: validation data loss: 0.0006336841953399519, training data loss: 1.4668743053799896e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2149217128753662\n",
      "Epoch 98: validation data loss: 0.0006299982332203486, training data loss: 1.4793369111064906e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.19606781005859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-10 18:18:42,686]\u001B[0m Trial 19 finished with value: 0.0006297527382907258 and parameters: {'learning_rate': 0.5, 'ff_neurons_layer_00': 10, 'ff_neurons_layer_01': 100, 'early_stopping_epochs': 8, 'layer_activation_00': 'LeakyReLU'}. Best is trial 19 with value: 0.0006297527382907258.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: validation data loss: 0.0006297527382907258, training data loss: 1.460756813112187e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.2291851043701172\n",
      "Final validation data loss:  0.0006297527382907258\n"
     ]
    }
   ],
   "source": [
    "hyperoptimizer.perform_study()\n",
    "\n",
    "# Will save the results directly to the parameters.\n",
    "hyperoptimizer.set_optimal_parameters()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's train the best network one final time and see where we stand. In my case, the network that Optuna suggest is bigger and deeper then the ones we have used before, but that may differ. Keep in mind we are performing a very limited search here in the interest of time."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the snapshots and your inputs for consistency.\n",
      "Checking descriptor file  Be_snapshot0.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot0.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking descriptor file  Be_snapshot1.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot1.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking descriptor file  Be_snapshot0.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot0.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking descriptor file  Be_snapshot1.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot1.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Consistency check successful.\n",
      "Initializing the data scalers.\n",
      "Input scaler parametrized.\n",
      "Output scaler parametrized.\n",
      "Data scalers initialized.\n",
      "Build datasets.\n",
      "Build dataset: Done.\n",
      "Initial Guess - validation data loss:  0.13545188729621505\n",
      "Epoch 0: validation data loss: 0.0038651301436228294, training data loss: 0.029042459514043103\n",
      "Time for epoch[s]: 0.5024847984313965\n",
      "Epoch 1: validation data loss: 0.003004203103993037, training data loss: 0.002098114114918121\n",
      "Time for epoch[s]: 0.7544422149658203\n",
      "Epoch 2: validation data loss: 0.0008395075389783677, training data loss: 0.0005485697151863412\n",
      "Time for epoch[s]: 0.46743059158325195\n",
      "Epoch 3: validation data loss: 0.0007244748611972756, training data loss: 9.638374696855675e-05\n",
      "Time for epoch[s]: 0.5240819454193115\n",
      "Epoch 4: validation data loss: 0.0006715785967160578, training data loss: 6.61242940382326e-05\n",
      "Time for epoch[s]: 0.42807722091674805\n",
      "Epoch 5: validation data loss: 0.0006754454820667772, training data loss: 5.146980574822317e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.47028398513793945\n",
      "Epoch 6: validation data loss: 0.0006128002519476904, training data loss: 4.2594157023244795e-05\n",
      "Time for epoch[s]: 0.44981908798217773\n",
      "Epoch 7: validation data loss: 0.0006013971893754724, training data loss: 3.705021912512714e-05\n",
      "Time for epoch[s]: 0.5294530391693115\n",
      "Epoch 8: validation data loss: 0.0006103945649377832, training data loss: 3.2613611010383796e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.5100221633911133\n",
      "Epoch 9: validation data loss: 0.0005710083463964941, training data loss: 2.936239792331713e-05\n",
      "Time for epoch[s]: 0.5898799896240234\n",
      "Epoch 10: validation data loss: 0.0005850808941610327, training data loss: 2.6443762679214347e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.9332432746887207\n",
      "Epoch 11: validation data loss: 0.0005776552849164293, training data loss: 2.4359003474859342e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.9428069591522217\n",
      "Epoch 12: validation data loss: 0.0005699284885027638, training data loss: 2.253932461618833e-05\n",
      "Time for epoch[s]: 0.997760534286499\n",
      "Epoch 13: validation data loss: 0.0005588459710007933, training data loss: 2.103240641693002e-05\n",
      "Time for epoch[s]: 1.2053978443145752\n",
      "Epoch 14: validation data loss: 0.0005551882122205273, training data loss: 1.9695741639017516e-05\n",
      "Time for epoch[s]: 1.0076572895050049\n",
      "Epoch 15: validation data loss: 0.0005594733507121534, training data loss: 1.8556985034518047e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 1.0942606925964355\n",
      "Epoch 16: validation data loss: 0.0005335829913888347, training data loss: 1.761036686871422e-05\n",
      "Time for epoch[s]: 1.1925292015075684\n",
      "Epoch 17: validation data loss: 0.0005488325948040235, training data loss: 1.681367143750463e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 1.0645012855529785\n",
      "Epoch 18: validation data loss: 0.000560541505410791, training data loss: 1.595485062427717e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.890986442565918\n",
      "Epoch 19: validation data loss: 0.0005443720444696679, training data loss: 1.5308168295719852e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.8985202312469482\n",
      "Epoch 20: validation data loss: 0.000545371136708891, training data loss: 1.4740949258480443e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 1.098909854888916\n",
      "Epoch 21: validation data loss: 0.0005459823905060825, training data loss: 1.4176331758158937e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 1.0051295757293701\n",
      "Epoch 22: validation data loss: 0.0005479566582806034, training data loss: 1.3680537507686441e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 1.1604077816009521\n",
      "Epoch 23: validation data loss: 0.0005239973601685267, training data loss: 1.3227342759773611e-05\n",
      "Time for epoch[s]: 1.0753092765808105\n",
      "Epoch 24: validation data loss: 0.0005354911565236305, training data loss: 1.2829019496702167e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.7261109352111816\n",
      "Epoch 25: validation data loss: 0.000533164704227012, training data loss: 1.2434462986920522e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.6867833137512207\n",
      "Epoch 26: validation data loss: 0.0005390155192923872, training data loss: 1.2091707574314178e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.6828603744506836\n",
      "Epoch 27: validation data loss: 0.0005105533828474071, training data loss: 1.1705644598834591e-05\n",
      "Time for epoch[s]: 0.6723544597625732\n",
      "Epoch 28: validation data loss: 0.0005314067414362136, training data loss: 1.1401461462860238e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.6569344997406006\n",
      "Epoch 29: validation data loss: 0.000522388272906003, training data loss: 1.1262512599972829e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.8265295028686523\n",
      "Epoch 30: validation data loss: 0.0005364879353405678, training data loss: 1.091616160181015e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.7083802223205566\n",
      "Epoch 31: validation data loss: 0.0005318504082013483, training data loss: 1.0671710689046067e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.6594080924987793\n",
      "Epoch 32: validation data loss: 0.0005287992423527861, training data loss: 1.044704187576357e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.649416446685791\n",
      "Epoch 33: validation data loss: 0.0005280141416749998, training data loss: 1.0198748725802387e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.582787036895752\n",
      "Epoch 34: validation data loss: 0.0005230105154590519, training data loss: 1.004356869653752e-05\n",
      "Validation accuracy has not improved enough.\n",
      "Time for epoch[s]: 0.8628475666046143\n",
      "Epoch 35: validation data loss: 0.0005231435712614016, training data loss: 9.752467665906366e-06\n",
      "Validation accuracy has not improved enough.\n",
      "Stopping the training, validation accuracy has not improved for 8 epochs.\n",
      "Final validation data loss:  0.0005231435712614016\n",
      "Checking the snapshots and your inputs for consistency.\n",
      "Checking descriptor file  Be_snapshot2.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot2.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking descriptor file  Be_snapshot3.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot3.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "DataHandler prepared for inference. No training possible with this setup. If this is not what you wanted, please revise the input script. Validation snapshots you may have entered willbe ignored.\n",
      "Consistency check successful.\n",
      "Build datasets.\n",
      "Build dataset: Done.\n",
      "Had to readjust batch size from 40 to 54\n",
      "{'band_energy': [-0.06057311374204488, -0.017166694512685154]}\n"
     ]
    }
   ],
   "source": [
    "parameters, data_handler, network = training(parameters)\n",
    "print(testing(parameters, data_handler, network))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The results may not necessarily be better with this limited search then what we had above. In my case they are very good, but again, this is a limited search compared to reasonable defaults. For actual production runs, one would set up a thorough, longer search before training production level models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Excurse: How to tune bispectrum hyperparameters (presentation only)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With all that talk about hyperparameters, you may have found yourself wondering how we determine the bispectrum component hyperparameters that we had used far above to calculate atomic density representations on the grid. That is a very valid question.\n",
    "\n",
    "Technically, in the same way you have just done - by testing different values and training models.\n",
    "However, physical intuition can be exploited quite a fair bit here. Namely, it is possible to correlate LDOS and bispectrum values at different points in space. And by doing so, it is enough to simply calculate descriptors and then calculate a simple metric. No training needed, yielding a speed-up of many orders of magnitude.\n",
    "\n",
    "Presume you have two separate points in space. If the LDOS of these points are very similar, so should the bispectrum descriptors. If not, then the model will never have a chance of learning from the data provided.\n",
    "\n",
    "MALA implements an analysis based on this concept called ACSD. Since you need the LAMMPS library to perform it, we will only look at it and not try it out ourselves.\n",
    "\n",
    "To show you - in a very very simple way - that it works, we will give it two values to choose from for the two hyperparameters.\n",
    "\n",
    "The first hyperparameter for bispectrum descriptors is the cutoff, i.e., how much information from surrounding grid points are incorporate into the components. The second one is called twojmax and dictates the dimensionality of the descriptor vectors. If it is too small, the information is underrepresented. If it is too large, we learn a lot of noise.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating descriptors from /home/fiedlerl/data/mala_data_repo/Be2/Be_snapshot1.out\n",
      "Descriptor calculation: had to enforce periodic boundary conditions on 0 atoms before calculation.\n",
      "Calculating descriptors from /home/fiedlerl/data/mala_data_repo/Be2/Be_snapshot1.out\n",
      "Descriptor calculation: had to enforce periodic boundary conditions on 0 atoms before calculation.\n",
      "Calculating descriptors from /home/fiedlerl/data/mala_data_repo/Be2/Be_snapshot1.out\n",
      "Descriptor calculation: had to enforce periodic boundary conditions on 0 atoms before calculation.\n",
      "Calculating descriptors from /home/fiedlerl/data/mala_data_repo/Be2/Be_snapshot1.out\n",
      "Descriptor calculation: had to enforce periodic boundary conditions on 0 atoms before calculation.\n",
      "Calculating descriptors from /home/fiedlerl/data/mala_data_repo/Be2/Be_snapshot2.out\n",
      "Descriptor calculation: had to enforce periodic boundary conditions on 0 atoms before calculation.\n",
      "Calculating descriptors from /home/fiedlerl/data/mala_data_repo/Be2/Be_snapshot2.out\n",
      "Descriptor calculation: had to enforce periodic boundary conditions on 0 atoms before calculation.\n",
      "Calculating descriptors from /home/fiedlerl/data/mala_data_repo/Be2/Be_snapshot2.out\n",
      "Descriptor calculation: had to enforce periodic boundary conditions on 0 atoms before calculation.\n",
      "Calculating descriptors from /home/fiedlerl/data/mala_data_repo/Be2/Be_snapshot2.out\n",
      "Descriptor calculation: had to enforce periodic boundary conditions on 0 atoms before calculation.\n",
      "ACSD analysis, optimal parameters: \n",
      "Bispectrum twojmax:  10\n",
      "Bispectrum cutoff:  4.67637\n"
     ]
    }
   ],
   "source": [
    "parameters.descriptors.descriptors_contain_xyz = True\n",
    "parameters.descriptors.acsd_points = 100\n",
    "\n",
    "hyperoptimizer = mala.ACSDAnalyzer(parameters)\n",
    "hyperoptimizer.add_hyperparameter(\"bispectrum_twojmax\", [2, 10])\n",
    "hyperoptimizer.add_hyperparameter(\"bispectrum_cutoff\", [0.5, 4.67637])\n",
    "\n",
    "# Add raw snapshots to the hyperoptimizer. For the targets, numpy files are\n",
    "# okay as well.\n",
    "hyperoptimizer.add_snapshot(\"espresso-out\", pj(data_path, \"Be_snapshot1.out\"),\n",
    "                            \"numpy\", pj(data_path, \"Be_snapshot1.in.npy\"),\n",
    "                            target_units=\"1/(Ry*Bohr^3)\")\n",
    "hyperoptimizer.add_snapshot(\"espresso-out\", pj(data_path, \"Be_snapshot2.out\"),\n",
    "                            \"numpy\", pj(data_path, \"Be_snapshot2.in.npy\"),\n",
    "                            target_units=\"1/(Ry*Bohr^3)\")\n",
    "\n",
    "# If you plan to plot the results (recommended for exploratory searches),\n",
    "# the optimizer can return the necessary quantities to plot.\n",
    "plotting = hyperoptimizer.perform_study()\n",
    "hyperoptimizer.set_optimal_parameters()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using trained models (hands-on/presentation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now understand how we can build optimal models with MALA. Once this is done, we would like to use these models to predict properties of interest. For this, let us first see how MALA can be used to save and load models.\n",
    "\n",
    "We train a final model - based on the parameters idenitified above.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the snapshots and your inputs for consistency.\n",
      "Checking descriptor file  Be_snapshot0.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot0.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking descriptor file  Be_snapshot1.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot1.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Consistency check successful.\n",
      "Initializing the data scalers.\n",
      "Input scaler parametrized.\n",
      "Output scaler parametrized.\n",
      "Data scalers initialized.\n",
      "Build datasets.\n",
      "Build dataset: Done.\n",
      "Initial Guess - validation data loss:  0.13316485322551963\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Validation accuracy has not improved enough.\n",
      "Final validation data loss:  0.000636495879770797\n"
     ]
    }
   ],
   "source": [
    "data_handler = mala.DataHandler(parameters)\n",
    "data_handler.clear_data()\n",
    "\n",
    "data_handler.add_snapshot(\"Be_snapshot0.in.npy\", data_path,\n",
    "                          \"Be_snapshot0.out.npy\", data_path, \"tr\")\n",
    "data_handler.add_snapshot(\"Be_snapshot1.in.npy\", data_path,\n",
    "                          \"Be_snapshot1.out.npy\", data_path, \"va\")\n",
    "\n",
    "# This already loads data into RAM!\n",
    "data_handler.prepare_data()\n",
    "parameters.network.layer_sizes.insert(0, data_handler.input_dimension)\n",
    "parameters.network.layer_sizes.append(data_handler.output_dimension)\n",
    "network = mala.Network(parameters)\n",
    "trainer = mala.Trainer(parameters, network, data_handler)\n",
    "trainer.train_network()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Saving such a model now becomes a one-liner. Models will be saved as `.zip` archives containing the network weights, parameter json file and coefficients for the data scaling."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "trainer.save_run(\"Be_model\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading is equally simple, but dependent on the task. Two possible task can be done: Testing (as we have done above, to verify whether a model is performing well) or predicting (using the model to predict the electronic structure of arbitrary atomic configurations).\n",
    "\n",
    "We will have a quick look at the testing interface again to plot the density of states, and see how MALA can directly give us access to the electronic structure of a material.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the snapshots and your inputs for consistency.\n",
      "Checking descriptor file  Be_snapshot2.in.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "Checking targets file  Be_snapshot2.out.npy at /home/fiedlerl/data/mala_data_repo/Be2\n",
      "DataHandler prepared for inference. No training possible with this setup. If this is not what you wanted, please revise the input script. Validation snapshots you may have entered willbe ignored.\n",
      "Consistency check successful.\n",
      "Build datasets.\n",
      "Build dataset: Done.\n"
     ]
    }
   ],
   "source": [
    "parameters, network, data_handler, tester = mala.Tester.load_run(\"Be_model\")\n",
    "data_handler.clear_data()\n",
    "data_handler.add_snapshot(\"Be_snapshot2.in.npy\", data_path,\n",
    "                          \"Be_snapshot2.out.npy\", data_path, \"te\",\n",
    "                          calculation_output_file=pj(data_path, \"Be_snapshot2.out\"))\n",
    "data_handler.prepare_data(reparametrize_scaler=False)\n",
    "\n",
    "actual_ldos, predicted_ldos = tester.predict_targets(0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can visualize the DOS via the internal LDOS calculator of the DataHandler object."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.legend.Legend at 0x7f864b881570>"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 720x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ldos_calculator = data_handler.target_calculator\n",
    "ldos_calculator.read_additional_calculation_data(data_handler.get_snapshot_calculation_output(0))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ldos_calculator.read_from_array(actual_ldos)\n",
    "ax.plot(ldos_calculator.energy_grid, ldos_calculator.density_of_states.copy(), label=\"Actual\")\n",
    "ldos_calculator.read_from_array(predicted_ldos)\n",
    "ax.plot(ldos_calculator.energy_grid, ldos_calculator.density_of_states.copy(), label=\"Predicted\")\n",
    "\n",
    "ax.set_xlabel(\"Energy [eV]\")\n",
    "ax.set_ylabel(\"Density of States [1/eV]\")\n",
    "ax.legend()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This finally leads us to the prediction part - the part, to which such efforts eventually should culminate too. Since this part again requires LAMMPS, it will not be hands-on.\n",
    "\n",
    "Instead I will demonstrate how MALA can be used to predict the electronic density and electronic structure of a system far larger the atomic configurations it was trained on. As we have demonstrated in a recently submitted research article, this is feasible if the training data is big enough to capture characteristic electronic effects. This is technically not the case with this reduced data set, but it is the case for production-level models and the syntax is the same regardless.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptor calculation: had to enforce periodic boundary conditions on 0 atoms before calculation.\n",
      "No number of electrons per atom provided, MALA cannot guess the number of electrons in the cell with this. Energy calculations may bewrong.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameters, network, data_handler, predictor = mala.Predictor.load_run(\"Be_model\")\n",
    "Be16 = read(\"Be16.vasp\")\n",
    "\n",
    "# We have to specify the grid on which to predict - a current limitation of MALA that we are working on fixing.\n",
    "parameters.running.inference_data_grid = [36, 36, 54]\n",
    "\n",
    "# We also read the additional calculation data before we predict.\n",
    "ldos_calculator = data_handler.target_calculator\n",
    "ldos_calculator.read_additional_calculation_data(data_handler.get_snapshot_calculation_output(0))\n",
    "\n",
    "predicted_ldos = predictor.predict_for_atoms(Be16)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have the LDOS we can calculate the total energy and electronic density. The latter can be visualized using e.g. VESTA."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [71]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m parameters\u001B[38;5;241m.\u001B[39mtargets\u001B[38;5;241m.\u001B[39mpseudopotential_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./data_generation/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      4\u001B[0m ldos_calculator\u001B[38;5;241m.\u001B[39mread_from_array(predicted_ldos)\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mldos_calculator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtotal_energy\u001B[49m)\n",
      "File \u001B[0;32m/usr/lib/python3.10/functools.py:981\u001B[0m, in \u001B[0;36mcached_property.__get__\u001B[0;34m(self, instance, owner)\u001B[0m\n\u001B[1;32m    979\u001B[0m val \u001B[38;5;241m=\u001B[39m cache\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattrname, _NOT_FOUND)\n\u001B[1;32m    980\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m val \u001B[38;5;129;01mis\u001B[39;00m _NOT_FOUND:\n\u001B[0;32m--> 981\u001B[0m     val \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43minstance\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    982\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    983\u001B[0m         cache[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattrname] \u001B[38;5;241m=\u001B[39m val\n",
      "File \u001B[0;32m~/codes/mala/mala/targets/ldos.py:267\u001B[0m, in \u001B[0;36mLDOS.total_energy\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;124;03m\"\"\"Total energy of the system, calculated via cached LDOS.\"\"\"\u001B[39;00m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlocal_density_of_states \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 267\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_total_energy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    269\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo cached LDOS available to calculate this \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    270\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproperty.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/codes/mala/mala/targets/ldos.py:698\u001B[0m, in \u001B[0;36mLDOS.get_total_energy\u001B[0;34m(self, ldos_data, dos_data, density_data, fermi_energy, temperature, voxel, grid_integration_method, energy_integration_method, atoms_Angstrom, qe_input_data, qe_pseudopotentials, create_qe_file, return_energy_contributions)\u001B[0m\n\u001B[1;32m    693\u001B[0m     e_entropy_contribution \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_density_of_states_calculator\u001B[38;5;241m.\u001B[39m\\\n\u001B[1;32m    694\u001B[0m         entropy_contribution\n\u001B[1;32m    696\u001B[0m     \u001B[38;5;66;03m# Density based energy contributions (via QE)\u001B[39;00m\n\u001B[1;32m    697\u001B[0m     density_contributions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_density_calculator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m\\\u001B[49m\n\u001B[0;32m--> 698\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_energy_contributions\u001B[49m\n\u001B[1;32m    700\u001B[0m e_total \u001B[38;5;241m=\u001B[39m e_band \u001B[38;5;241m+\u001B[39m density_contributions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124me_rho_times_v_hxc\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m \\\n\u001B[1;32m    701\u001B[0m     density_contributions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124me_hartree\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m \\\n\u001B[1;32m    702\u001B[0m     density_contributions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124me_xc\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m \\\n\u001B[1;32m    703\u001B[0m     density_contributions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124me_ewald\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m    704\u001B[0m     e_entropy_contribution\n\u001B[1;32m    705\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_energy_contributions:\n",
      "File \u001B[0;32m/usr/lib/python3.10/functools.py:981\u001B[0m, in \u001B[0;36mcached_property.__get__\u001B[0;34m(self, instance, owner)\u001B[0m\n\u001B[1;32m    979\u001B[0m val \u001B[38;5;241m=\u001B[39m cache\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattrname, _NOT_FOUND)\n\u001B[1;32m    980\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m val \u001B[38;5;129;01mis\u001B[39;00m _NOT_FOUND:\n\u001B[0;32m--> 981\u001B[0m     val \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43minstance\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    982\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    983\u001B[0m         cache[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattrname] \u001B[38;5;241m=\u001B[39m val\n",
      "File \u001B[0;32m~/codes/mala/mala/targets/density.py:303\u001B[0m, in \u001B[0;36mDensity.total_energy_contributions\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    297\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    298\u001B[0m \u001B[38;5;124;03mAll density based contributions to the total energy.\u001B[39;00m\n\u001B[1;32m    299\u001B[0m \n\u001B[1;32m    300\u001B[0m \u001B[38;5;124;03mCalculated via the cached density.\u001B[39;00m\n\u001B[1;32m    301\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    302\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdensity \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 303\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_energy_contributions\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    304\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    305\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo cached density available to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    306\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcalculate this property.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/codes/mala/mala/targets/density.py:720\u001B[0m, in \u001B[0;36mDensity.get_energy_contributions\u001B[0;34m(self, density_data, create_file, atoms_Angstrom, qe_input_data, qe_pseudopotentials)\u001B[0m\n\u001B[1;32m    718\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m atoms_Angstrom \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    719\u001B[0m     atoms_Angstrom \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39matoms\n\u001B[0;32m--> 720\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__setup_total_energy_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdensity_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43matoms_Angstrom\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    721\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mcreate_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcreate_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    722\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mqe_input_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mqe_input_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    723\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mqe_pseudopotentials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\n\u001B[1;32m    724\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mqe_pseudopotentials\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    726\u001B[0m \u001B[38;5;66;03m# Get and return the energies.\u001B[39;00m\n\u001B[1;32m    727\u001B[0m energies \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(te\u001B[38;5;241m.\u001B[39mget_energies())\u001B[38;5;241m*\u001B[39mRydberg\n",
      "File \u001B[0;32m~/codes/mala/mala/targets/density.py:846\u001B[0m, in \u001B[0;36mDensity.__setup_total_energy_module\u001B[0;34m(self, density_data, atoms_Angstrom, create_file, qe_input_data, qe_pseudopotentials)\u001B[0m\n\u001B[1;32m    843\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m qe_pseudopotentials \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    844\u001B[0m         qe_pseudopotentials \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mqe_pseudopotentials\n\u001B[0;32m--> 846\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_tem_input_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43matoms_Angstrom\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mqe_input_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    847\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mqe_pseudopotentials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    848\u001B[0m \u001B[43m                              \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrid_dimensions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    849\u001B[0m \u001B[43m                              \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkpoints\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    851\u001B[0m \u001B[38;5;66;03m# initialize the total energy module.\u001B[39;00m\n\u001B[1;32m    852\u001B[0m \u001B[38;5;66;03m# FIXME: So far, the total energy module can only be initialized once.\u001B[39;00m\n\u001B[1;32m    853\u001B[0m \u001B[38;5;66;03m# This is ok when the only thing changing\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    856\u001B[0m \u001B[38;5;66;03m# There should be some kind of de-initialization function that allows\u001B[39;00m\n\u001B[1;32m    857\u001B[0m \u001B[38;5;66;03m# for this.\u001B[39;00m\n\u001B[1;32m    859\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m Density\u001B[38;5;241m.\u001B[39mte_mutex \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n",
      "File \u001B[0;32m~/codes/mala/mala/targets/target.py:1133\u001B[0m, in \u001B[0;36mTarget.write_tem_input_file\u001B[0;34m(atoms_Angstrom, qe_input_data, qe_pseudopotentials, grid_dimensions, kpoints)\u001B[0m\n\u001B[1;32m   1126\u001B[0m     qe_input_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnr3s\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m grid_dimensions[\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;66;03m# Might be needed for test purposes, the Be2 test data\u001B[39;00m\n\u001B[1;32m   1129\u001B[0m \u001B[38;5;66;03m# for example has symmetry, even though it was deactivated for\u001B[39;00m\n\u001B[1;32m   1130\u001B[0m \u001B[38;5;66;03m# the DFT calculation. If symmetry is then on in here, that\u001B[39;00m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# leads to errors.\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m \u001B[38;5;66;03m# qe_input_data[\"nosym\"] = False\u001B[39;00m\n\u001B[0;32m-> 1133\u001B[0m \u001B[43mase\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmala.pw.scf.in\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43matoms_Angstrom\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mespresso-in\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1134\u001B[0m \u001B[43m             \u001B[49m\u001B[43minput_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mqe_input_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1135\u001B[0m \u001B[43m             \u001B[49m\u001B[43mpseudopotentials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mqe_pseudopotentials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1136\u001B[0m \u001B[43m             \u001B[49m\u001B[43mkpts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkpoints\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/ase/io/formats.py:628\u001B[0m, in \u001B[0;36mwrite\u001B[0;34m(filename, images, format, parallel, append, **kwargs)\u001B[0m\n\u001B[1;32m    624\u001B[0m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m'\u001B[39m  \u001B[38;5;66;03m# default is json\u001B[39;00m\n\u001B[1;32m    626\u001B[0m io \u001B[38;5;241m=\u001B[39m get_ioformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[0;32m--> 628\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_write\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[43m              \u001B[49m\u001B[43mparallel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparallel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mappend\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mappend\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/ase/parallel.py:244\u001B[0m, in \u001B[0;36mparallel_function.<locals>.new_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    238\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    239\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnew_func\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    240\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (world\u001B[38;5;241m.\u001B[39msize \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[1;32m    241\u001B[0m         args \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(args[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mserial\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[1;32m    242\u001B[0m         \u001B[38;5;129;01mnot\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparallel\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m)):\n\u001B[1;32m    243\u001B[0m         \u001B[38;5;66;03m# Disable:\u001B[39;00m\n\u001B[0;32m--> 244\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    246\u001B[0m     ex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    247\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/ase/io/formats.py:664\u001B[0m, in \u001B[0;36m_write\u001B[0;34m(filename, fd, format, io, images, parallel, append, **kwargs)\u001B[0m\n\u001B[1;32m    661\u001B[0m         fd \u001B[38;5;241m=\u001B[39m open_with_compression(filename, mode)\n\u001B[1;32m    662\u001B[0m         \u001B[38;5;66;03m# XXX remember to re-enable compressed open\u001B[39;00m\n\u001B[1;32m    663\u001B[0m         \u001B[38;5;66;03m# fd = io.open(filename, mode)\u001B[39;00m\n\u001B[0;32m--> 664\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    665\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    666\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m open_new \u001B[38;5;129;01mand\u001B[39;00m fd \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/ase/io/formats.py:191\u001B[0m, in \u001B[0;36mIOFormat._write_wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    189\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m function \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    190\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCannot write to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m-format\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 191\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/ase/io/espresso.py:1646\u001B[0m, in \u001B[0;36mwrite_espresso_in\u001B[0;34m(fd, atoms, input_data, pseudopotentials, kspacing, kpts, koffset, crystal_coordinates, **kwargs)\u001B[0m\n\u001B[1;32m   1642\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m species \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mset\u001B[39m(atoms\u001B[38;5;241m.\u001B[39mget_chemical_symbols()):\n\u001B[1;32m   1643\u001B[0m     \u001B[38;5;66;03m# Look in all possible locations for the pseudos and try to figure\u001B[39;00m\n\u001B[1;32m   1644\u001B[0m     \u001B[38;5;66;03m# out the number of valence electrons\u001B[39;00m\n\u001B[1;32m   1645\u001B[0m     pseudo \u001B[38;5;241m=\u001B[39m pseudopotentials\u001B[38;5;241m.\u001B[39mget(species, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m-> 1646\u001B[0m     valence \u001B[38;5;241m=\u001B[39m \u001B[43mget_valence_electrons\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspecies\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_parameters\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpseudo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1647\u001B[0m     species_info[species] \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpseudo\u001B[39m\u001B[38;5;124m'\u001B[39m: pseudo, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalence\u001B[39m\u001B[38;5;124m'\u001B[39m: valence}\n\u001B[1;32m   1649\u001B[0m \u001B[38;5;66;03m# Convert atoms into species.\u001B[39;00m\n\u001B[1;32m   1650\u001B[0m \u001B[38;5;66;03m# Each different magnetic moment needs to be a separate type even with\u001B[39;00m\n\u001B[1;32m   1651\u001B[0m \u001B[38;5;66;03m# the same pseudopotential (e.g. an up and a down for AFM).\u001B[39;00m\n\u001B[1;32m   1652\u001B[0m \u001B[38;5;66;03m# if any magmom are > 0 or nspin == 2 then use species labels.\u001B[39;00m\n\u001B[1;32m   1653\u001B[0m \u001B[38;5;66;03m# Rememeber: magnetisation uses 1 based indexes\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/ase/io/espresso.py:739\u001B[0m, in \u001B[0;36mget_valence_electrons\u001B[0;34m(symbol, data, pseudo)\u001B[0m\n\u001B[1;32m    737\u001B[0m     pseudo \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m_dummy.UPF\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(symbol)\n\u001B[1;32m    738\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m pseudo_dir \u001B[38;5;129;01min\u001B[39;00m get_pseudo_dirs(data):\n\u001B[0;32m--> 739\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m path\u001B[38;5;241m.\u001B[39mexists(\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpseudo_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpseudo\u001B[49m\u001B[43m)\u001B[49m):\n\u001B[1;32m    740\u001B[0m         valence \u001B[38;5;241m=\u001B[39m grep_valence(path\u001B[38;5;241m.\u001B[39mjoin(pseudo_dir, pseudo))\n\u001B[1;32m    741\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.10/posixpath.py:76\u001B[0m, in \u001B[0;36mjoin\u001B[0;34m(a, *p)\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mjoin\u001B[39m(a, \u001B[38;5;241m*\u001B[39mp):\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;124;03m\"\"\"Join two or more pathname components, inserting '/' as needed.\u001B[39;00m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;124;03m    If any component is an absolute path, all previous path components\u001B[39;00m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;124;03m    will be discarded.  An empty last part will result in a path that\u001B[39;00m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;124;03m    ends with a separator.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 76\u001B[0m     a \u001B[38;5;241m=\u001B[39m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfspath\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m     sep \u001B[38;5;241m=\u001B[39m _get_sep(a)\n\u001B[1;32m     78\u001B[0m     path \u001B[38;5;241m=\u001B[39m a\n",
      "\u001B[0;31mTypeError\u001B[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "# For the total energy calculation we need to tell MALA where to find a pseudopotential.\n",
    "\n",
    "parameters.targets.pseudopotential_path = \"./data_generation/\"\n",
    "ldos_calculator.read_from_array(predicted_ldos)\n",
    "print(ldos_calculator.total_energy)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}